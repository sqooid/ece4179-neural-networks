{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://www.dropbox.com/s/vold2f3fm57qp7g/ECE4179_5179_6179_banner.png?dl=1\" alt=\"ECE4179/5179/6179 Banner\" style=\"max-width: 60%;\"/>\n",
    "</div>\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "# Let There Be Light!\n",
    "\n",
    "</div>\n",
    "\n",
    "Welcome to **ECE4179/5179/6179 Week 1**! In this notebook, we will study some aspects of data processing.\n",
    "\n",
    "By the end of this notebook, you will have accomplished the following learning outcomes:\n",
    "\n",
    "- Understanding vector algebra, including operations such as addition, subtraction, norm, inner product, and cosine similarity.\n",
    "- Applying these vector operations in a practical contexts such as audio and text processing.\n",
    "\n",
    "So, let's **get started**!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import librosa\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "RND_SEED = 42\n",
    "np.random.seed(RND_SEED)  # For reproducibility\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Algebra\n",
    "\n",
    "Recall from your previous math units tthat in $\\mathbb{R}^2$ (2-dimensional real plane), vectors can be represented as directed line segments. We briefly discuss basic vector operations such as addition, subtraction, norm, inner product, and cosine similarity.\n",
    "\n",
    "### Vector Addition\n",
    "Vector addition involves adding corresponding components of two vectors to form a new vector. Geometrically, this can be visualized by placing the tail of vector $\\mathbf{b}$ at the head of vector $\\mathbf{a}$. The resultant vector $\\mathbf{a} + \\mathbf{b}$ is the diagonal of the parallelogram formed by $\\mathbf{a}$ and $\\mathbf{b}$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Vector Subtraction\n",
    "Vector subtraction involves subtracting corresponding components of one vector from another. Geometrically, this can be visualized by placing the tail of vector $\\mathbf{b}$ at the tail of vector $\\mathbf{a}$. The resultant vector $\\mathbf{a} - \\mathbf{b}$ points from the head of $\\mathbf{b}$ to the head of $\\mathbf{a}$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Norm (Magnitude)\n",
    "The norm or magnitude of a vector is a measure of its length. For a vector $\\mathbf{v} = \\begin{pmatrix} x \\\\ y \\end{pmatrix}$, the norm is given by:\n",
    "\n",
    "\\begin{align}\n",
    "\\|\\mathbf{v}\\| = \\sqrt{x^2 + y^2}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\n",
    "### Inner Product (Dot Product)\n",
    "The inner product (or dot product) of two vectors $\\mathbf{a}$ and $\\mathbf{b}$ is a scalar representing the product of their magnitudes and the cosine of the angle between them. It is calculated as:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{a} \\cdot \\mathbf{b} = a_1b_1 + a_2b_2\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\n",
    "### Cosine Similarity\n",
    "Cosine similarity measures the cosine of the angle between two vectors, indicating their directional similarity. It is given by the formula:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{cosine\\_similarity} = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\|\\mathbf{a}\\| \\|\\mathbf{b}\\|}\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #3352FF; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "\n",
    "\n",
    "### <span style=\"color: pink;\">Task #1.</span>  Practice with Vector Operations\n",
    "\n",
    "Consider the following two vectors in $\\mathbb{R}^2$:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{a} = \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}, \\quad \\mathbf{b} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Obtain the following:\n",
    "- $\\mathbf{a} + \\mathbf{b}$\n",
    "- $\\mathbf{a} - \\mathbf{b}$\n",
    "- $\\|\\mathbf{a}\\|$ and $\\|\\mathbf{b}\\|$\n",
    "- $\\mathbf{a} \\cdot \\mathbf{b}$\n",
    "- cosine similarity between $\\mathbf{a}$ and $\\mathbf{b}$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #3352FF; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "\n",
    "\n",
    "### <span style=\"color: pink;\">Task #2.</span>  Use NumPy for Vector Operations\n",
    "\n",
    "Define the vectors $\\mathbf{a} = \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}$ and $\\mathbf{b} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$ using NumPy. Then, compute the following:\n",
    "\n",
    "- $\\mathbf{a} + \\mathbf{b}$\n",
    "- $\\mathbf{a} - \\mathbf{b}$\n",
    "- $\\|\\mathbf{a}\\|$ and $\\|\\mathbf{b}\\|$\n",
    "- $\\mathbf{a} \\cdot \\mathbf{b}$\n",
    "- cosine similarity between $\\mathbf{a}$ and $\\mathbf{b}$\n",
    "\n",
    "You can use the `numpy.linalg.norm()`, and `numpy.dot()` functions to compute the norm, and dot product, respectively.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define two vectors in R2\n",
    "a = np.array([3, 4])\n",
    "b = np.array([1, 2])\n",
    "\n",
    "# Vector Addition\n",
    "a_plus_b = # TODO <--- write your code here\n",
    "\n",
    "# Vector Subtraction\n",
    "a_minus_b = # TODO <--- write your code here\n",
    "\n",
    "# Norm (Magnitude) of vectors\n",
    "norm_a = # TODO <--- write your code here\n",
    "norm_b = # TODO <--- write your code here\n",
    "\n",
    "# Inner Product (Dot Product)\n",
    "inner_product = # TODO <--- write your code here\n",
    "\n",
    "# Cosine Similarity\n",
    "cosine_similarity = # TODO <--- write your code here\n",
    "\n",
    "# Print results\n",
    "print(f\"Vector a: {a}\")\n",
    "print(f\"Vector b: {b}\")\n",
    "print(f\"Vector Addition (a + b): {a_plus_b}\")\n",
    "print(f\"Vector Subtraction (a - b): {a_minus_b}\")\n",
    "print(f\"Norm of a: {norm_a}\")\n",
    "print(f\"Norm of b: {norm_b}\")\n",
    "print(f\"Inner Product (a Â· b): {inner_product}\")\n",
    "print(f\"Cosine Similarity: {cosine_similarity}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with audio\n",
    "\n",
    "An audio file is a sequence of values recorded over time. In technical terms, it is a time-series data. If you were to open an MP3 file with a text editor, you would see a series of characters. These characters correspond to the raw audio samples that constitute the time-series data. To properly read and play an audio file, we need to understand some formatting details, but let's not worry about that for now. The crucial parameter we need is the sample rate (SR), which denotes the number of samples per second. For example, if the SR is 44100, then there are 44100 samples per second. Higher SR values generally indicate better audio quality. \n",
    "\n",
    "The [Librosa](https://librosa.org/doc/latest/index.html) library offers a diverse set of functionalities for reading, analyzing, and manipulating audio data. Utilizing Librosa, we can load audio files in various formats, including MP3 or WAV.\n",
    "\n",
    "First, install Librosa and then study the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install librosa\n",
    "\n",
    "\n",
    "# Librosa is a library for audio analysis\n",
    "# https://librosa.org/doc/latest/index.html\n",
    "# with this library we can load audio files, extract features, and perform analysis\n",
    "import librosa\n",
    "from IPython.display import Audio\n",
    "\n",
    "# Path to the MP3 file\n",
    "audio_file = \"data/Warm-Memories.mp3\"\n",
    "\n",
    "# Load the audio file\n",
    "audio, sr = librosa.load(audio_file, sr=None)\n",
    "\n",
    "\n",
    "# Display the audio player\n",
    "Audio(audio_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #3352FF; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "\n",
    "\n",
    "### <span style=\"color: pink;\">Task #3.</span>  Be a conductor!\n",
    "Use the code below to read piano.mp3 and drums.mp3 files. Your task is now \n",
    " \n",
    "1. play them simultaneously \n",
    "2. play the drum and after 10 seconds play the piano\n",
    "3. can you make the piano louder than the drums?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the first audio file\n",
    "y1, sr1 = librosa.load('data/piano.mp3', sr=None)\n",
    "\n",
    "# Load the second audio file\n",
    "y2, sr2 = librosa.load('data/drum.mp3', sr=None)\n",
    "# Ensure both audio files have the same length\n",
    "min_len = min(len(y1), len(y2))\n",
    "y1 = y1[:min_len]\n",
    "y2 = y2[:min_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. playing the audios simultaneously\n",
    "y_superimposed1 = # TODO <--- write your code here\n",
    "y_superimposed1 = y_superimposed1 / np.max(np.abs(y_superimposed1)) # Normalize the audio\n",
    "\n",
    "# Display the audio player\n",
    "Audio(y_superimposed1, rate=sr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. playing the piano into drums after 10 seconds\n",
    "y_superimposed2 = # TODO <--- write your code here\n",
    "y_superimposed2 = y_superimposed2 / np.max(np.abs(y_superimposed2)) # Normalize the audio\n",
    "# Display the audio player\n",
    "Audio(y_superimposed2, rate=sr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make piano louder than drums\n",
    "\n",
    "y_superimposed3 = # TODO <--- write your code here\n",
    "y_superimposed3 = y_superimposed3 / np.max(np.abs(y_superimposed3))\n",
    "\n",
    "# Display the audio player\n",
    "Audio(y_superimposed3, rate=sr1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Text data\n",
    "\n",
    "We learn about how text data can be represented as vectors using a Language Model (LM), known as ![BERT](https://arxiv.org/abs/1810.04805). A language model is a statistical model that assigns probabilities to sequences of words. We will learn about LMs in depth later in the course but for now, our focus will be on how text data can be represented as vectors.\n",
    "\n",
    "### Tokenization\n",
    "Tokenization is the process of splitting text into smaller units called tokens. These tokens can be words, subwords, or characters. In LLMs and during training, a vocabulary is created that contains words and subwords more frequently seen in the training data. Each token in the vocabulary is assigned a unique integer ID. During tokenization, each token in the text is replaced with its corresponding ID.\n",
    "\n",
    "\n",
    "BERT-base is a 12-layer neural network with 110M parameters model, developed by Google in 2019. It is trained on English text data and can be fine-tuned on specific tasks such as text classification, question-answering, and named entity recognition.\n",
    "\n",
    "You need to install the transformers library to use the BERT model. You can install it using the following command:\n",
    "\n",
    "```!pip install transformers```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "# Function to get BERT embeddings for a word\n",
    "def get_tokens_and_embeddings(text, model, tokenizer):\n",
    "    tokens = tokenizer(text, return_tensors='pt')\n",
    "    token_ids = tokens['input_ids']\n",
    "    with torch.inference_mode():\n",
    "        token_embs = model.embeddings.word_embeddings(token_ids)\n",
    "    # Pick the first token embedding for each word\n",
    "    token_embs = token_embs[:, 1, :]\n",
    "    return token_ids.squeeze().detach().numpy(), token_embs.squeeze().detach().numpy()\n",
    "\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "LM_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello my friend\"\n",
    "tokens = tokenizer(text, return_tensors='pt')['input_ids'].squeeze().detach().numpy()\n",
    "\n",
    "print(f\"Tokens: {tokens}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the tokenizer has some special tokens such as `[CLS]` and `[SEP]` that are used to mark the beginning and end of a sentence, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #3352FF; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "\n",
    "\n",
    "### <span style=\"color: pink;\">Task #4.</span>  Use BERT to embed \n",
    "\n",
    "Consider the following words: \"france\", \"germany\", \"italy\", \"apple\", \"orange\", \"banana\", \"truck\", \"car\", \"bus\". Use the BERT model to embed these words into vectors. Use your previous code to create a similarity matrix between these words.\n",
    "\n",
    "\n",
    "You can use the following two functions ```cosine_similarity``` and ```plot_similarity_matrix``` to compute the cosine similarity and plot the similarity matrix, respectively.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b=None):\n",
    "    \"\"\"\n",
    "    Calculates the cosine similarity between two arrays.\n",
    "\n",
    "    Parameters:\n",
    "    a (ndarray): A 2D array of shape (m, n) where each row represents a vector.\n",
    "    b (ndarray, optional): A 2D array of shape (k, n) where each row represents a vector. If None, defaults to `a`.\n",
    "\n",
    "    Returns:\n",
    "    ndarray: A 2D array of shape (m, k) containing the cosine similarity scores.\n",
    "\n",
    "    This function computes the cosine similarity between each pair of vectors in `a` and `b`. \n",
    "    If `b` is not provided, it computes the cosine similarity between each pair of vectors in `a`.\n",
    "\n",
    "    The cosine similarity is calculated as the dot product of the vectors divided by the product of their magnitudes.\n",
    "\n",
    "    # Example usage:\n",
    "    a = np.random.rand(5, 4)  # Random array of shape (5, 4)\n",
    "    b = np.random.rand(3, 4)  # Random array of shape (3, 4)\n",
    "    similarity_matrix = cosine_similarity(a, b)\n",
    "    print(\"Cosine Similarity Matrix:\\n\", similarity_matrix)\n",
    "    \"\"\"\n",
    "\n",
    "    if b is None:\n",
    "        b = a\n",
    "    \n",
    "    # Compute the dot product between each pair of vectors in `a` and `b`\n",
    "    a_times_b = np.dot(a, b.T)  # Shape will be (m, k)\n",
    "    \n",
    "    # Compute the L2 norm (magnitude) of each vector in `a` and `b`\n",
    "    norm_a = np.linalg.norm(a, axis=1, keepdims=True)  # Shape will be (m, 1)\n",
    "    norm_b = np.linalg.norm(b, axis=1, keepdims=True)  # Shape will be (k, 1)\n",
    "    \n",
    "    # Compute the cosine similarity\n",
    "    cos_sim = a_times_b / (norm_a * norm_b.T)  # Shape will be (m, k)\n",
    "    \n",
    "    return cos_sim\n",
    "\n",
    "\n",
    "\n",
    "def plot_similarity_matrix(words, similarity_matrix):\n",
    "    \"\"\"\n",
    "    Plots a normalized similarity matrix using matplotlib.\n",
    "\n",
    "    Parameters:\n",
    "    words (list of str): List of words corresponding to the similarity matrix.\n",
    "    similarity_matrix (ndarray): A square matrix of similarity scores between words.\n",
    "\n",
    "    This function normalizes each row of the similarity matrix, plots it using a heatmap, \n",
    "    and annotates each cell with the original similarity value up to three decimal places.\n",
    "\n",
    "    # Example usage\n",
    "    words = [\"word1\", \"word2\", \"word3\", \"word4\", \"word5\"]\n",
    "    similarity_matrix = cosine_similarity(np.random.rand(5, 4))\n",
    "    plot_similarity_matrix(words, similarity_matrix)\n",
    "    \"\"\"\n",
    "\n",
    "    # Normalize each row of the similarity matrix\n",
    "    row_max = similarity_matrix.max(axis=1, keepdims=True)\n",
    "    row_min = similarity_matrix.min(axis=1, keepdims=True)\n",
    "    normalized_similarity_matrix = (similarity_matrix - row_min) / (row_max - row_min)\n",
    "\n",
    "    # Plotting the normalized similarity matrix using matplotlib\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    cax = ax.matshow(normalized_similarity_matrix, cmap='plasma')\n",
    "\n",
    "    # Add color bar to the plot\n",
    "    plt.colorbar(cax)\n",
    "\n",
    "    # Set up the axes with words\n",
    "    ax.set_xticks(np.arange(len(words)))\n",
    "    ax.set_yticks(np.arange(len(words)))\n",
    "\n",
    "    # Label the axes with the words\n",
    "    ax.set_xticklabels(words)\n",
    "    ax.set_yticklabels(words)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment for better readability\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"left\", rotation_mode=\"anchor\")\n",
    "\n",
    "    # Annotate each cell with the original similarity value up to three decimal places\n",
    "    for i in range(len(words)):\n",
    "        for j in range(len(words)):\n",
    "            ax.text(j, i, f\"{similarity_matrix[i, j]:.3f}\", ha=\"center\", va=\"center\", color=\"black\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = # TODO <--- write your code here\n",
    "tokens, token_embs = get_tokens_and_embeddings(words, LM_model, tokenizer)\n",
    "\n",
    "# Calculate cosine similarity matrix\n",
    "similarity_matrix = # TODO <--- write your code here\n",
    "\n",
    "\n",
    "plot_similarity_matrix(words, similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #3352FF; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "\n",
    "\n",
    "### <span style=\"color: pink;\">Task #5.</span>  Vector Arithmetic \n",
    "\n",
    "Consider this analogy: \"The relation of man to woman is as king to ?\".\n",
    "\n",
    "As a human, you can easily identify that the answer is \"queen\".\n",
    "\n",
    "Your task is to demonstrate this relationship using vector algebra with pre-trained BERT embeddings. Follow these steps:\n",
    "\n",
    "1. Embed the words \"man\", \"woman\", \"king\", \"queen\", \"france\", \"germany\", \"italy\", \"apple\", \"orange\", \"banana\", \"truck\", \"car\", and \"bus\" using BERT.\n",
    "2. Perform a vector operation that uses the embeddings of \"man\", \"woman\", and \"king\" to find the missing word in the analogy.\n",
    "3. Calculate the cosine similarity between the resulting vector and the embeddings of \"queen\", \"france\", \"germany\", \"italy\", \"apple\", \"orange\", \"banana\", \"truck\", \"car\", and \"bus\".\n",
    "4. Print out their similarities.\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words for the analogy and additional words for comparison\n",
    "words = [\"man\", \"woman\", \"king\", \"queen\", \n",
    "         \"france\", \"germany\", \"italy\", \n",
    "         \"apple\", \"orange\", \"banana\", \n",
    "         \"truck\", \"car\", \"bus\"]\n",
    "\n",
    "\n",
    "# TODO <--- write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook, we have explored some concepts in vector algebra and their applications in data processing. Here are the key takeaways:\n",
    "\n",
    "- **Vector Algbera**: We reviewed how to perform basic vector operations, discussed vector norms and cosine similarity.\n",
    "\n",
    "- **Continuous/Discrete Data** We learned how to work with audio data and text data. The former is an example of continuous data, while the latter is an example of discrete data.\n",
    "\n",
    "We will see you next week with more exciting topics. Until then, happy learning!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BloodMNIST",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
