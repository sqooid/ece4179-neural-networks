{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "  <img src=\"https://www.dropbox.com/s/vold2f3fm57qp7g/ECE4179_5179_6179_banner.png?dl=1\" alt=\"ECE4179/5179/6179 Banner\" style=\"max-width: 60%;\"/>\n",
    "</div>\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "\n",
    "# A Journey Through Optimization\n",
    "\n",
    "</div>\n",
    "\n",
    "Welcome to **ECE4179/5179/6179 Week 4**! This week, we will focus on the Gradient Descent (GD) algorithm. GD is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. The method of GD can be dated back to the work of the French mathematician Augustin-Louis Cauchy in 19th century (Compte Rendu `a l’Acad´emie des Sciences, 1847). As you can guess, adaptation of GD into machine learning and artificial intelligence has been influential. Despite its sheer simplicity, GD has paved the way to what some may call \"the silicon intelligence\" today. As humans, we are \"carbon-based life forms\", hence \"carbon-intelligence\" might describe our intelligence. Some argue that silicon-intelligence is superior to carbon-intelligence. Compare the number of synapses (parameters), silicon-intelligence has vs. several mammals below to see why.   \n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"data/compare_GPT4_mammal_brain.jpeg\" alt=\"Synapsis comparison\" style=\"max-width: 60%;\"/>\n",
    "\n",
    "  [Source](https://twitter.com/hodlipson/status/1608887008436555777?s=20)\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "With this short introduction, roll up your sleeves and let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as always, we start by importing the necessary packages\n",
    "\n",
    "# Enable autoreload extension\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "# Set random seed\n",
    "RND_SEED = 4179\n",
    "np.random.seed(RND_SEED)\n",
    "\n",
    "from ECE4179_utils_ws4 import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Descent (GD)**\n",
    "\n",
    "---\n",
    "\n",
    "Gradient Descent is a first-order iterative optimization algorithm used to find the minimum of a function. Considering minimizing a differentiable function \n",
    "\n",
    "\n",
    "\\begin{equation} \n",
    "    f(\\mathbf{x}): \\mathbb{R}^n \\rightarrow \\mathbb{R} \n",
    "\\end{equation}\n",
    "\n",
    "The gradient descent method updates the variable $\\mathbf{x}$ iteratively using the following update rule:\n",
    "\n",
    "\\begin{equation} \n",
    "    \\mathbf{x}_{\\text{new}} = \\mathbf{x}_{\\text{old}} - \\eta \\nabla f(\\mathbf{x}_{\\text{old}}) \n",
    "\\end{equation}\n",
    "\n",
    "Here:\n",
    "- $\\mathbf{x}_{\\text{old}}$ and $\\mathbf{x}_{\\text{new}}$ are the old and new values of the variable $\\mathbf{x}$, respectively.\n",
    "\n",
    "- $\\eta$ is the learning rate, a hyperparameter that determines the step size in the direction of the steepest descent.\n",
    "\n",
    "- $\\nabla f(\\mathbf{x}_{\\text{old}})$ is the gradient of the function $f(\\mathbf{x})$ at $\\mathbf{x}_{\\text{old}}$, pointing in the direction of the steepest ascent.\n",
    "\n",
    "The algorithm starts with an initial guess for $\\mathbf{x}$ and iteratively moves in the direction opposite to the gradient until it reaches a local minimum. Let's see how this works in practice. \n",
    "\n",
    "\n",
    "<div style=\"background-color: #3352FF; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "\n",
    "\n",
    "### <span style=\"color: pink;\">Task #1.</span>  GD basics\n",
    "Your task is to compute the gradient of the following function. Remember for univariate functions, the gradient is the derivative. \n",
    "\n",
    "\\begin{equation} \n",
    "    f(x) = x^2 + \\cos(x) + 2\n",
    "\\end{equation}\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that was quite easy. However, what does this mean exactly? For example, the gradient of $f$ at $x=1$ is \n",
    "$2-\\sin(1) \\approx 1.59$. What does this geometrically mean? Let's visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(x):\n",
    "    fx = TODO <--- write your code here\n",
    "    return  fx\n",
    "\n",
    "def df1_dx(x):\n",
    "    df_dx = TODO <--- write your code here\n",
    "    return df_dx\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot the function and its gradient at two points\n",
    "# x=-1.5 and x=1\n",
    "\n",
    "\n",
    "x = np.linspace(-2, 2, 100)  # Define the x-values for the plot\n",
    "\n",
    "x_points = [1, -1.5]  # Define the points where the tangents will be plotted\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(x, f1(x), c='b', label='f(x)')  # Plot the function\n",
    "# Plot the tangents at the specified points\n",
    "for x0 in x_points:\n",
    "    plot_tangent(f1, df1_dx, x0)\n",
    "plt.legend()  # Display the legend\n",
    "plt.show()  # Show the plot\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient in Univariate Functions**\n",
    "\n",
    "For univariate functions, the gradient (our old good friend, aka, the derivative) is a \n",
    "scalar value that represents the slope of the tangent line to the graph of the function at a specific point. \n",
    "\n",
    "However, that is not the only information that the gradient provides. \n",
    "The gradient also tells us the direction in which the function increases the most. For example, in our case above,\n",
    "the gradient at $x=-1.5$ is approximately -4. This means that if you want to increase the value of the function,\n",
    "you should go to the left (why). If you want to decrease the value of the function, then you should go in the opposite direction of the gradient, in our case to the right. \n",
    "\n",
    "\n",
    "<div style=\"background-color: #3352FF; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "### <span style=\"color: pink;\">Task #2.</span>  Gradient and univariate functions\n",
    "\n",
    "Now, think about $x=1$ and the gradient at that point. What is the direction that you should go to increase the value of the function? What is the direction that you should go to decrease the value of the function?\n",
    "</div>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementing the Gradient Descent Algorithm**\n",
    "\n",
    "You can implement a vanilla GD using a for loop. Let's review the main steps of the algorithm:\n",
    "\n",
    "1. **Initialization**: Choose an initial guess for $\\mathbf{x}$, set hyperparameters of the algorithm \n",
    "(the learning rate $\\eta$ and maximum number of iterations of the algorithm (`maxiter`).\n",
    "\n",
    "2. **Iteration**: For each iteration up to `maxiter`, do the following:\n",
    "   a. **Compute the Gradient**: Calculate the gradient of the function $f(\\mathbf{x})$ at the current $\\mathbf{x}$, denoted as $\\nabla f(\\mathbf{x})$.\n",
    "\n",
    "   b. **Update the Variable**: Update $\\mathbf{x}$ using the update rule:\n",
    "   \n",
    "    \\begin{equation}\n",
    "        \\mathbf{x}_{\\text{new}} = \\mathbf{x}_{\\text{old}} - \\eta \\nabla f(\\mathbf{x}_{\\text{old}})\n",
    "    \\end{equation}\n",
    "\n",
    "\n",
    "We do not bother ourselves with the convergence criteria in our code but if you simply opt for a more elaborate implementation, you can add a convergence criterion to your code. For example, you can check if the difference between the current and previous values of $\\mathbf{x}$ is less than a threshold value, say $10^{-6}$, and if so, stop.\n",
    "\n",
    "Study the code snippet below and get ready to see the GD in action!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GD(func, grad, x0, eta=1e-3, maxiter=100):\n",
    "    ''' \n",
    "    Gradient descent algorithm\n",
    "    func: objective function\n",
    "    grad: gradient of the objective function\n",
    "    x0: initial point\n",
    "    eta: step size (aka learning rate)\n",
    "    maxiter: maximum number of iterations\n",
    "\n",
    "    return: x, the solution after maxiter iterations\n",
    "            and a dictionary containing the history of the algorithm\n",
    "            hist_f: the history of the objective function values\n",
    "            hist_x: the history of the solution x\n",
    "            hist_g: the history of the gradient at the solution x\n",
    "    '''\n",
    "    hist_f = []\n",
    "    hist_x = []\n",
    "    hist_g = []\n",
    "    x = x0\n",
    "    for i in range(maxiter):\n",
    "        # keep track of the history\n",
    "        hist_f.append(func(x))\n",
    "        hist_x.append(x)\n",
    "        hist_g.append(grad(x))\n",
    "\n",
    "        # update x according to x <- x - eta * grad(x)\n",
    "        x = x - eta * grad(x)\n",
    "\n",
    "    hist_dict = {'hist_f': hist_f, 'hist_x': hist_x, 'hist_g': hist_g}\n",
    "    return x, hist_dict\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Six-hump Camelback function\n",
    "\n",
    "A modified version of the six-hump camelback function can be defined as:\n",
    "\\begin{align}\n",
    "    f(x) = 4x^2 - 2.1x^4 + \\frac{1}{3}x^6 + x\\cos(x)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://www.nicepng.com/png/detail/245-2450208_camels-clipart-2-hump-cartoon-camel-two-humps.png\" alt=\"cute camel\" style=\"max-width: 40%;\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #3352FF; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "### <span style=\"color: pink;\">Task #3.</span>  Implement the six-hump camelback function and its gradient\n",
    "\n",
    "Implement the six-hump camelback function and its gradient below so we can study the GD algorithm on this function.\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uni_6_hump_camelback(x):\n",
    "    f_x = TODO <--- write your code here\n",
    "    return f_x\n",
    "\n",
    "def grad_uni_6_hump_camelback(x):\n",
    "    df_dx = TODO <--- write your code here\n",
    "    return df_dx\n",
    "\n",
    "\n",
    "# plotting the function\n",
    "\n",
    "x = np.linspace(-2.2, 2.2, 100)\n",
    "y = uni_6_hump_camelback(x)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, y, color='blue', label='f(x)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('A modified version of 6-hump camelback function')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #3352FF; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "### <span style=\"color: pink;\">Task #4.</span>  Apply GD\n",
    "\n",
    "Imagine you are at $x=-2$. Apply GD function to the camelback function with $\\eta=0.05$ for five iterations. What is the value of the function at the end of the 15 iterations? Have you arrived at the minimum? If not, what should you do to arrive at the minimum?\n",
    "\n",
    "\n",
    "\n",
    "**Hint:** You can use the helper function plot_history to plot the path that you take. This function takes the history of the GD function as an input and plots the path that you take. Here is an example of how you can use this function.\n",
    "\n",
    "```python\n",
    "    x_star1, gd_hist1 = GD(uni_6_hump_camelback, grad_uni_6_hump_camelback, x0, lr, max_iter)\n",
    "    plot_history(x,uni_6_hump_camelback, x_star1,gd_hist1)\n",
    "```\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = -2\n",
    "lr = 0.05\n",
    "max_iter = 5 \n",
    "x_star1, gd_hist1 = GD(uni_6_hump_camelback, grad_uni_6_hump_camelback, x0, lr, max_iter)\n",
    "print(f'x* = {x_star1: .2f}, f(x*) = {uni_6_hump_camelback(x_star1): .2f}')\n",
    "plot_history(x,uni_6_hump_camelback, x_star1,gd_hist1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From 1D to 2D\n",
    "\n",
    "The Ackley's function is a well-known multimodal function that is often used to test optimization algorithms. It is defined as follows:\n",
    "\\begin{equation}\n",
    "    f(x,y) = -20 \\exp \\left( -0.2 \\sqrt{0.5(x^2 + y^2)} \\right) - \\exp \\left( 0.5 \\left( \\cos(2\\pi x) + \\cos(2\\pi y) \\right) \\right) + e + 20\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"background-color: #3352FF; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "\n",
    "\n",
    "### <span style=\"color: pink;\">Task #5.</span>  Obtain the gradient of the Ackley's function\n",
    "Your task is to compute the gradient of the Ackley's function. Remember, the gradient is a vector of partial derivatives. \n",
    "That is $\\nabla f = \\left[ \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y} \\right]$.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But worry none, we have computational in the title of our workshop so let's just implement the Ackley function and its derivative and see how GD may work there. Study the code below and try to understand what is going on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ackley_func(v):\n",
    "    \"\"\"\n",
    "    The Ackley function, a commonly used benchmark function in optimization.\n",
    "    \n",
    "    :param v: A two-dimensional array where each row represents a point (x, y) in the input space.\n",
    "    :return: The value of the Ackley function at each point in v.\n",
    "    \"\"\"\n",
    "    pi = np.pi\n",
    "    x = v[:,0]  # Extract the x coordinates\n",
    "    y = v[:,1]  # Extract the y coordinates\n",
    "    x_sq = x**2\n",
    "    y_sq = y**2\n",
    "\n",
    "    # Calculate the exponential term\n",
    "    exp_term = np.exp(-0.2 * np.sqrt(0.5 * (x_sq + y_sq)))\n",
    "    # Calculate the cosine term\n",
    "    cos_term = np.cos(2*pi*x) + np.cos(2*pi*y)\n",
    "\n",
    "    # Compute the Ackley function value\n",
    "    f_xy = -20 * exp_term - np.exp(0.5 * cos_term) + np.e + 20\n",
    "    return f_xy \n",
    "\n",
    "\n",
    "\n",
    "def ackley_grad(v):\n",
    "    \"\"\"\n",
    "    gradient of the Ackley function.\n",
    "    \n",
    "    :param v: A two-dimensional array where each row represents a point (x, y) in the input space.\n",
    "    :return: A two-dimensional array where each row represents the gradient at the corresponding point in v.\n",
    "    \"\"\"\n",
    "    pi = np.pi\n",
    "    x = v[:, 0]  # Extract the x coordinates\n",
    "    y = v[:, 1]  # Extract the y coordinates\n",
    "\n",
    "    x_sq = x**2\n",
    "    y_sq = y**2\n",
    "\n",
    "    # Calculate the exponential term and its derivative\n",
    "    sqrt_term = np.sqrt(x_sq + y_sq)\n",
    "    exp_term = np.exp(-0.1 * np.sqrt(2) * sqrt_term)\n",
    "    d_exp_dx = (2 * np.sqrt(2) * x / sqrt_term) * exp_term\n",
    "    d_exp_dy = (2 * np.sqrt(2) * y / sqrt_term) * exp_term\n",
    "    \n",
    "    # Calculate the cosine term and its derivative\n",
    "    cos_term = np.cos(2 * pi * x) + np.cos(2 * pi * y)\n",
    "    exp_cos_term = np.exp(0.5 * cos_term)\n",
    "    d_cos_dx = pi * np.sin(2 * pi * x) * exp_cos_term\n",
    "    d_cos_dy = pi * np.sin(2 * pi * y) * exp_cos_term\n",
    "    \n",
    "    # Compute the gradient\n",
    "    df_dx = (d_exp_dx + d_cos_dx)\n",
    "    df_dy = (d_exp_dy + d_cos_dy)\n",
    "\n",
    "    # Combine the partial derivatives into a gradient array\n",
    "    grad = np.array([df_dx, df_dy]).T\n",
    "    return grad\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid setup\n",
    "num_pnts = 100\n",
    "x = np.linspace(-5, 5, num_pnts)\n",
    "y = np.linspace(-5, 5, num_pnts)\n",
    "X_grid, Y_grid = np.meshgrid(x, y)\n",
    "\n",
    "v = np.array([X_grid, Y_grid]).reshape(2, -1).T\n",
    "Z = ackley_func(v).reshape(num_pnts, num_pnts)\n",
    "\n",
    "# Figure setup\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "# Subplot 1: 3D surface plot\n",
    "ax1 = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "ax1.plot_surface(X_grid, Y_grid, Z, cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "ax1.set_zlabel('z')\n",
    "ax1.set_title('Ackley Function')\n",
    "\n",
    "# Subplot 2: Contour plot\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "contour_plot = ax2.contourf(X_grid, Y_grid, Z, levels=50, cmap=cm.coolwarm)\n",
    "plt.colorbar(contour_plot, ax=ax2, shrink=0.75)\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y')\n",
    "ax2.set_title('Contour Plot of Ackley Function')\n",
    "ax2.set_aspect('equal')  # Make the contour plot square\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you have noticed, the Ackley function has many local minima. This makes it a good test function for optimization algorithms. Let's have some fun. Start from point $x_0 = (4, 3)$ and use gradient descent to find the minimum of the Ackley function. Pick learning rate yourself and run the algorithm for 15 iterations. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_start, y_start = TODO <--- write your code here\n",
    "\n",
    "# Learning rate\n",
    "lr = TODO <--- write your code here\n",
    "\n",
    "# Number of iterations\n",
    "max_iter = TODO <--- write your code here\n",
    "\n",
    "x0 = np.array([[x_start, y_start]])\n",
    "x_ast, hist_ackley = GD(ackley_func, ackley_grad, x0, lr, max_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to understand what has happened inside GD! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_graph(i, ax):\n",
    "    if i > 0:\n",
    "        ax.annotate('', xy=(x_history[i], y_history[i]), xytext=(x_history[i - 1], y_history[i - 1]),\n",
    "                    arrowprops=dict(facecolor='black', edgecolor='black', arrowstyle='->', lw=2))\n",
    "    ax.set_title('Gradient Descent on Ackley Function, iteration={}'.format(i))\n",
    "\n",
    "# Extract x and y history from hist_ackley\n",
    "hist_x = hist_ackley['hist_x']\n",
    "x_history = [coord[0][0] for coord in hist_x]\n",
    "y_history = [coord[0][1] for coord in hist_x]\n",
    "\n",
    "\n",
    "# plot the initial state\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "contour_plot = ax.contourf(X_grid, Y_grid, Z, levels=50, cmap=cm.coolwarm)\n",
    "\n",
    "ax.set_aspect('equal')  # Make the contour plot square\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title('Gradient Descent on Ackley Function')\n",
    "\n",
    "# animate the graph\n",
    "ani = FuncAnimation(fig, update_graph, frames=len(x_history), fargs=(ax,), interval=300, blit=False)\n",
    "plt.colorbar(contour_plot, ax=ax, shrink=0.75)\n",
    "plt.close()\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #0855cf; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "\n",
    "\n",
    "### <span style=\"color: yellow;\">Conclusions</span>  \n",
    "\n",
    "In this notebook, we studied the fundamentals of the GD algorithm. We explored the mathematical foundation of the gradient and its geometric interpretation, focusing on both univariate and multivariate functions. Have it at the back of your mind, silicon-intelligence emerges from the mathematical foundations of the GD we use!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"data/tintin_meme.jpg\" alt=\"Breast Cancer dataset\" style=\"width: 60%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "### In Differentiable Programming We Trust!\n",
    "\n",
    "Computing the gradient of a function manually can be cumbersome, especially for complex functions. This is where automatic differentiation comes into play. It allows us to compute gradients automatically, simplifying the process and reducing the likelihood of errors. Libraries like PyTorch and TensorFlow use automatic differentiation to compute gradients for training neural networks.\n",
    "\n",
    "Let’s take a quick look at how PyTorch can make our lives easier. \n",
    "\n",
    "In PyTorch, all you need to do is:\n",
    "1. Define your function.\n",
    "2. Identify the variables for which you want to compute the gradient.\n",
    "3. Call the `backward()` function.\n",
    "\n",
    "Let's see how this works in practice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "# Define the 6-hump camelback function using PyTorch\n",
    "def uni_6_hump_camelback_torch(x):\n",
    "    return (4 - 2.1*(x**2) + (x**4)/3)*(x**2) + x*torch.cos(x)\n",
    "\n",
    "\n",
    "# Define a function to calculate the derivative using autograd\n",
    "def grad_uni_6_hump_camelback_autograd(x):\n",
    "    x_torch = torch.tensor(x, requires_grad=True)\n",
    "    f_x = uni_6_hump_camelback_torch(x_torch)\n",
    "    f_x.backward()\n",
    "    return x_torch.grad.item()\n",
    "\n",
    "# Example usage\n",
    "x = 3.0\n",
    "exact_grad = grad_uni_6_hump_camelback(x)\n",
    "autograd_grad = grad_uni_6_hump_camelback_autograd(x)\n",
    "\n",
    "print(f\"   Exact derivative at x={x:6.3f}: {exact_grad:6.3f}\")\n",
    "print(f\"Autograd derivative at x={x:6.3f}: {autograd_grad:6.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the code to compute the gradient of the Ackley function using PyTorch. Compare the results with the ones you obtained above. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Ackley function using PyTorch\n",
    "def ackley_func_torch(v):\n",
    "    pi = torch.pi\n",
    "    x = v[:, 0]  # Extract the x coordinates\n",
    "    y = v[:, 1]  # Extract the y coordinates\n",
    "    x_sq = x**2\n",
    "    y_sq = y**2\n",
    "\n",
    "    # Calculate the exponential term\n",
    "    exp_term = torch.exp(-0.2 * torch.sqrt(0.5 * (x_sq + y_sq)))\n",
    "    # Calculate the cosine term\n",
    "    cos_term = torch.cos(2 * pi * x) + torch.cos(2 * pi * y)\n",
    "\n",
    "    # Compute the Ackley function value\n",
    "    f_xy = -20 * exp_term - torch.exp(0.5 * cos_term) + torch.exp(torch.tensor(1.0)) + 20\n",
    "    return f_xy\n",
    "\n",
    "# Define a function to calculate the gradient using autograd\n",
    "def grad_ackley_func_autograd(v):\n",
    "    v_torch = torch.tensor(v, requires_grad=True)\n",
    "    f_xy = ackley_func_torch(v_torch)\n",
    "\n",
    "    # Ensure the gradient is zeroed out before the backward pass\n",
    "    # if v_torch.grad is not None:\n",
    "    #     v_torch.grad.zero_()\n",
    "\n",
    "    f_xy.backward()\n",
    "    return v_torch.grad.numpy()\n",
    "\n",
    "# Example usage\n",
    "v = np.array([[1.3, 0.5]])\n",
    "exact_grad = ackley_grad(v)\n",
    "autograd_grad = grad_ackley_func_autograd(v)\n",
    "\n",
    "print(f\"   Exact gradient: at ({v[0][0]:6.3f}, {v[0][1]:6.3f}) = ({exact_grad[0][0]:6.3f}, {exact_grad[0][1]:6.3f})\")\n",
    "print(f\"Autograd gradient: at ({v[0][0]:6.3f}, {v[0][1]:6.3f}) = ({autograd_grad[0][0]:6.3f}, {autograd_grad[0][1]:6.3f})\") \n",
    "      \n",
    "      \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
