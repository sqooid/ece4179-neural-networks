{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "# <p style=\"text-align: center;\">Lab 3 (Weeks 5,6): Multi-Layer Perceptrons (MLP) </p>\n",
    "\n",
    "<img src=\"https://viso.ai/wp-content/uploads/2021/04/multilayer-perceptrons-MLP-concept-1.jpg\" width=\"400\" height=\"200\" />\n",
    "\n",
    "<!-- ![linear-vs-logistic-regression--medium](https://miro.medium.com/max/1400/1*dm6ZaX5fuSmuVvM4Ds-vcg.jpeg) -->\n",
    "\n",
    "Welcome to your third lab of ECE4179! Labs in this unit will run as a help desk and they are not mandatory to attend. As a reminder, this lab can be completed in pairs but you and your partner need to register via the google forms on Moodle under week 5.\n",
    "\n",
    "The two notebooks provided contain all the code and comments that you need to submit. Feel free to add in your own markdown for additional comments. After completion, You need to submit both Jupyter Notebooks (.ipynb file) to Moodle. Make sure you run the Notebooks before submitting and all outputs are visible.\n",
    "\n",
    "In this Lab, you will find three tasks distributed across two notebooks: Sinusoidal and CoverType. These tasks will guide you through to learn using a deep learning framework (Pytorch lightning) for MLP based problems. These knowledge and skills will be essential for lab 4 and assignment, and in general, critical to get you prepared to enter the deep learning world.\n",
    "\n",
    "- <b>Task 1:</b> Shallow MLP to fit MLP into a sinusoidal function\n",
    "- <b>Task 2:</b> Shallow MLP for multiclass classification of the Covertype dataset\n",
    "- <b>Task 3:</b> Deep MLP for multiclass classification of the Covertype dataset\n",
    "\n",
    "Each task will contain code to complete, and worded questions, so ensure you complete both before submitting.\n",
    "\n",
    "Good luck with the Lab!\n",
    "\n",
    "__Submission details:__\n",
    "- __Make sure you have run all your cells from top to bottom (you can click _Kernel_ and _Restart Kernel and Run All Cells_).__ </br>\n",
    "- __Submit the Jupyter Notebooks (Lab3\\_1\\_Sinusoidal.ipynb) and (Lab3\\_2\\_FashionMNIST.ipynb).__\n",
    "- __Outputs must be visible upon submission. We will also be re-running your code__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Enter your student details below</b>\n",
    "\n",
    "- <b>Student Name:</b> Firstname Lastname\n",
    "- <b>Student ID:</b> 123456789\n",
    "\n",
    "If you have a partner:\n",
    "- <b>Second Student Name:</b> Firstname Lastname\n",
    "- <b>Second Student ID:</b> 123456789"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "* [Task 2 - Shallow MLP for CoverType dataset](#task2)\n",
    "    * [2.1 Loading data points and Create Tensor Dataset](#2_1)\n",
    "    * [2.2 Design Shallow MLP](#2_2)\n",
    "    * [2.3 Train and evaluate model's performance](#2_3)\n",
    "    * [2.3 Train and evaluate model's performance](#2_4)\n",
    "* [Task 3 Deep MLP](#task3)\n",
    "    * [3.1 Design Deep MLP](#3_1)\n",
    "    * [3.2 Train and evaluate model's performance](#3_2)\n",
    "    * [3.3 Visualise the results for the network.](#3_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "# Before you begin\n",
    "\n",
    "We have provided some numerical answers for you to aim for. To replicate these results, do not change any of the codes that are labelled \"Do not change\".\n",
    "\n",
    "Throughout this lab, there will be code and written answers that you need to fill in / complete. Please read the instructions carefully. The comments in the code snippet and markdown text will guide you on what you need to do.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have not installed pytorch-lightning already, then please follow the instructions below\n",
    "\n",
    "# # If you run on Jupyter Lab uncomment below comment\n",
    "# ! pip install --quiet \"matplotlib\" \"pytorch-lightning\" \"pandas\" \"torchmetrics\" \"torchvision\"\n",
    "\n",
    "# # If you run on google colab uncomment below comment\n",
    "# ! pip install pytorch-lightning -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Libraries, you do not need to import any additional libraries for this lab\n",
    "\n",
    "import numpy as np ## Numpy is the fundamental building block of understanding tensor (matrices) within Python\n",
    "import matplotlib.pyplot as plt ## Matplotlib.pyplot is the graphing library that we will be using throughout the semester\n",
    "import random ## Useful for sampling\n",
    "# import sys ## Useful to retrieve some system information\n",
    "\n",
    "import os ## Useful for running command line within python\n",
    "import pandas as pd ## Useful for data manipulation\n",
    "from IPython.display import Image ## For markdown purposes\n",
    "from IPython.display import clear_output\n",
    "import PIL\n",
    "\n",
    "import torch ## Pytorch is the deep learning library that we will be using\n",
    "import torch.nn as nn ## Neural network module\n",
    "import torch.nn.functional as F ## Functional module\n",
    "from torch import optim ## Optimizer module\n",
    "from torch.utils.data import DataLoader, random_split ## Under torchvision datasets you can find popular datasets that are frequently used for machine learning/deep learning tasks (eg., MNIST, SVHN, CIFAR10, CIFAR100 etc).\n",
    "from torchmetrics import Accuracy ## Torchmetrics is a library that contains metrics for evaluating models\n",
    "import torchvision ## Torchvision is a library that contains popular datasets, model architectures, and image transformations for computer vision tasks\n",
    "from torchvision import transforms\n",
    "\n",
    "import pytorch_lightning as pl ## Pytorch lightning is a wrapper for pytorch that makes it easier to train models\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import Callback, ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
    "\n",
    "\n",
    "\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "## Setting seeds for reproducibility. Do NOT change these!\n",
    "seed_everything(4179)\n",
    "random.seed(4179)\n",
    "np.random.seed(4179)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This code snippet does not need to be edited\n",
    "\n",
    "# from python_environment_check import check_packages\n",
    "# from python_environment_check import set_background\n",
    "\n",
    "# ## Colour schemes for setting background colour\n",
    "# white_bgd = 'rgba(0,0,0,0)'\n",
    "# red_bgd = 'rgba(255,0,0,0.2)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "# Task 2 & 3 : Compare Shallow MLP and Deep MLP\n",
    "\n",
    "In sections 2 and 3 you will be training a Shallow MLPs and Deep MLPs for classifying a CoverType dataset which is collected from 7 types of forest covers. 54 features are captured in each sample of the dataset. Then we will train a Shallow (Section 2) and Deep (Section 3) MLP for classifying this dataset that contains input features and labels (The input features are 1x54 and class labels are from 1-7).\n",
    "\n",
    "<img src=\"figures/shallow_deep_mlp.png\" width=\"1200\" align=\"center\">\n",
    "    \n",
    "In the above figure A is a shallow neural network and B is a deep neural network.\n",
    "\n",
    "#### Details of Covertype dataset:\n",
    "\n",
    "- This dataset is available form UCI Machine Learning Repository, you should use [```fetch_openml()```](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_openml.html) function to load the data (Note: version 3 of data must be loaded)\n",
    "\n",
    "- The dataset consists of 54 input features, some of the entries are shown as below:\n",
    "\n",
    "| **Feature**                | **Variable Type**     | **Description**                                                      |\n",
    "|----------------------------|-------------------|----------------------------------------------------------------------|\n",
    "| Elevation                          | Numerical   | Measured in meters                                                                    |\n",
    "| Aspect                             | Numerical   | Aspect in degrees azimuth                                                    |\n",
    "| Slope                              | Numerical   | Measured in degrees                                                                         |\n",
    "| Horizontal_Distance_To_Hydrology   | Numerical   | Horizontal distance to the nearest surface water features (in mters)\n",
    "| Vertical_Distance_To_Hydrology     | Numerical   | Vertical distance to the nearest surface water features (in meters)                                               |\n",
    "| Horizontal_Distance_To_Roadways    | Numerical   | Horizontal distance to the nearest roadway (in meters)                                       |\n",
    "| Hillshade_9am                      | Numerical   | Hillshade index at 9am, which measures the shadow and sunlight exposure on the landscape (0 to 255)             |\n",
    "| Hillshade_Noon                     | Numerical   | Hillshade index at Noon                                                                       |\n",
    "| Hillshade_3pm                      | Numerical   | Hillshade index at 3pm                                                                                     |\n",
    "| Horizontal_Distance_To_Fire_Points | Numerical   | Horizontal distance to the nearest wildfire ignition points (in meters)                                |\n",
    "| Wilderness_Area                    | Categorical | One-hot encoded as 4 binary columns representing the wilderness area designation (Rawah, Neota, Comanche Peak, Cache la Poudre) |\n",
    "| Soil_Type                          | Categorical | One-hot encoded as 40 binary columns representing different soil types according to US Forest Service classification   |\n",
    "\n",
    "\n",
    "- The target variable in the Covertype dataset represents the forest cover type, and there are 7 distinct classes:\n",
    "\n",
    "| **Cover Type**          | **Code** |\n",
    "|-------------------------|----------|\n",
    "| Spruce/Fir              | 1        |\n",
    "| Lodgepole Pine          | 2        |\n",
    "| Ponderosa Pine          | 3        |\n",
    "| Cottonwood/Willow       | 4        |\n",
    "| Aspen                   | 5        |\n",
    "| Douglas-fir             | 6        |\n",
    "| Krummholz               | 7        |\n",
    "\n",
    "    \n",
    "#### Modules that you will need in this task:\n",
    "- sklearn's datasets and preprocessing modules to load the dataset and apply proper transforms\n",
    "- sklearn's model_selection module to split the dataset into train(60%), test(20%), and validation(20%)\n",
    "- torch TensorDataset and DataLoader to create the train, test, and validation datasets and data loaders\n",
    "- pytorch_lightning module class to construct a MLP in order to perform training and testing on the given datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "# Task 2 - Shallow MLP for CoverType dataset <a class=\"anchor\" id=\"task2\"></a>\n",
    "\n",
    "For this task you will work on the following points:\n",
    " 1. Create training, validation and testing dataloaders.\n",
    " 2. investigate the training datasets (in terms of shape of the data).\n",
    " 3. Design shallow neural network model.\n",
    " 4. Perform training the model and evaluation for different settings. Report test accuracies.\n",
    " 5. Visualize experimental results for training losses.\n",
    " 6. Visualize experimental results for validation accuracies.\n",
    " 7. Visualize predictions.\n",
    " 8. Optimize Shallow network's performance.\n",
    "\n",
    "    \n",
    "Note:  In all the parts below, you should only use the training data and their labels to train your model. You may use the **validation set** to pick a trained model. For example, during training, you can test the accuracy of your model using the validation set every epoch and pick the model that achieves the highest validation accuracy. You should then report your results on the test set once you choose your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## Learning Objective\n",
    "\n",
    "This task aims to understand and implement a Deep MLP model to train neural networks on the CoverType dataset. By the end of this task, you will be able to:\n",
    "\n",
    "1. Load the dataset from sci-kit learn package, and prepare the dataset and dataloader on the CoverType dataset.\n",
    "\n",
    "2. Implement a Multi-Layer Perceptron (MLP) model using PyTorch Lightning's framework.\n",
    "\n",
    "3. Apply stochastic gradient descent (SGD) optimization method to train the MLP model on the CoverType dataset.\n",
    "\n",
    "4. Visualize and analyze the performance of the MLP model on the given samples to understand how well it fits the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## 2.1 Loading data points and Create Tensor Dataset <a class=\"anchor\" id=\"2_1\"></a>\n",
    "\n",
    "Let's load the dataset and inspect the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code rovided in the following cell includes:\n",
    "- The implementation of `fetch_openml` to fetch the Covertype dataset from the OpenML repository. It ensures that the data is numeric and encodes the target labels as integers.\n",
    "- The implementation of the `fit_transform` method in the `LabelEncoder` to encode target labels with values between 0 and the number of classes minus 1.\n",
    "- The `unique` function is also included, which returns unique values of the targets after encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DO NOT CHANGE THIS SECTION ####\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Fetch the Covertype dataset\n",
    "covertype = fetch_openml(name='Covertype', version=3)\n",
    "\n",
    "# Ensure all data is numeric\n",
    "X = covertype.data.astype(np.float32)\n",
    "\n",
    "# Encode the target labels as integers\n",
    "label_encoder = LabelEncoder()\n",
    "Y = label_encoder.fit_transform(covertype.target)\n",
    "unique_labels = np.unique(Y)\n",
    "print(\"Classes:\", np.unique(Y))\n",
    "#### DO NOT CHANGE THIS SECTION ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Inspect the data\n",
    "\n",
    "Now you have input features in 'X' and the corresponding labels in 'Y'. Inspect the data by doing the following:\n",
    "\n",
    "- Print out the number of features \n",
    "- Print out the number of samples\n",
    "- Print out the shape of Y\n",
    "- Print the first 3 samples of input features 'X' and their corresponding labels in 'Y'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the input data. You should be able to infer the number of samples and features from the shape\n",
    "????\n",
    "# Print the shape of the target label\n",
    "????\n",
    "\n",
    "# Print the first samples of the input features and their labels\n",
    "????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Split the dataset\n",
    "\n",
    "Different from Task 1, to train a network for classification, a key process is to split the dataset into train, validatio and test sets.\n",
    "\n",
    "Let's split the data into 60% train, 20% validation, and 20% test.\n",
    "\n",
    "- You will need to use [```train_test_split```](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function \n",
    "- Make sure after splitting the number of samples per class is balanced among all classes. You can achive it by using the 'startify' option in the 'train_test_split' function\n",
    "- Print out the shape of input features and labels for splitted data.\n",
    "- Remember that you need to this for both the features (X) and labels (Y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = ????\n",
    "X_val, X_test, Y_val, Y_test = ????\n",
    "\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_val.shape, Y_val.shape)\n",
    "print(X_test.shape, Y_test.shape)\n",
    "print(type(X_train), type(Y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Preprocessing of input features:\n",
    "\n",
    "Note that the PyTorch network only recognise tensors to be passed in. If you check the ```type()``` of the data imported so far, you will find out that they are not tensors yet. And there are also some common preprocess as well.\n",
    "\n",
    "For this task, we need to:\n",
    "1. Scaling the Numerical Features: It's often a good idea to *scale* or *normalize* the numerical features so that they have similar ranges. This can help the model converge faster and avoid issues related to features with different scales.\n",
    "    - You can use [```StandardScaler```](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) from sklearn.preprocessing to scale the input features in train, test, and val data\n",
    "    - Then check the datatype of the transformed variables\n",
    "2. Convert the data from numpy array into torch tensors:\n",
    "   1. Convert input features into 32-bit floating-point tensors.\n",
    "   2. Convert labels into 64-bit integer tensors.\n",
    "   3. You can check if the convertion is done correctly by use ```dtype``` to check the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = ????\n",
    "X_val = ????\n",
    "X_test = ????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X_train), type(Y_train))\n",
    "print(type(X_val), type(Y_val))\n",
    "print(type(X_test), type(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data to torch tensors\n",
    "X_train_tensor = ????\n",
    "Y_train_tensor = ????\n",
    "\n",
    "X_val_tensor = ????\n",
    "Y_val_tensor = ????\n",
    "\n",
    "X_test_tensor = ????\n",
    "Y_test_tensor = ????\n",
    "\n",
    "print(X_train_tensor.dtype, Y_train_tensor.dtype)\n",
    "print(X_val_tensor.dtype, Y_val_tensor.dtype)\n",
    "print(X_test_tensor.dtype, Y_test_tensor.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) Create the datasets and dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the dataset**\n",
    "\n",
    "Now we have the tensors, let's create three datasets for train, validation, and test. It is a similar process to Task 1 but with a different torch module ```TensorDataset``` from ```torch.utils.data```. You can check out the shape of Dataset.tensors[0] (input feature tensor) and Dataset.tensors[1] (target tensor) when you create Dataset from train, test, and validation tensors to make sure the dataset is created correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import TensorDataset\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = ????\n",
    "val_dataset = ????\n",
    "test_dataset = ????\n",
    "\n",
    "print(train_dataset.tensors[0].shape, train_dataset.tensors[1].shape)\n",
    "print(val_dataset.tensors[0].shape, val_dataset.tensors[1].shape)\n",
    "print(test_dataset.tensors[0].shape, test_dataset.tensors[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the dataloader**\n",
    "\n",
    "You can create the dataloader in the same as Task 1, with the batch size of 256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "train_loader = ????\n",
    "val_loader = ????\n",
    "test_loader = ????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## 2.2 Design Shallow MLP <a class=\"anchor\" id=\"2_2\"></a>\n",
    "\n",
    "Design a Shallow MLP. This will be similar to in task 1. \n",
    " - You need to instantiate Shallow_MLP class from LightningModule\n",
    " - Shallow_MLP must take these inputs:\n",
    "   1. the number of input features\n",
    "   2. the number of classes\n",
    "   3. the number of nodes in hidden layer\n",
    "   4. the learning_rate\n",
    "   5. the train loader\n",
    "   6. the test loader \n",
    "   7. the validation loader\n",
    "  \n",
    " - In the Shallow_MLP class you should define metrics to evaluate model performance:\n",
    "   1. train_accuracy\n",
    "   2. test_accuracy\n",
    "   3. val_accuracy\n",
    "   - To define these metrics you should use [```Accuracy()```](https://lightning.ai/docs/torchmetrics/stable/classification/accuracy.html ) class from pytorch-lightning. Make sure you choose the correct type of evaluation - multiclass.\n",
    "\n",
    " - Shallow_MLP must have two Linear layer: Linear(# input features $\\times$ # hidden nodes) $\\rightarrow$ ReLU $\\rightarrow$ Linear( # hidden nodes $\\times$  # classes)\n",
    "   1. The first Linear layer (input layer) is followed by a **Relu** activation function.\n",
    "   2. The second Linear layer (hidden layer) maps the hidden nodes into class scores (logits)\n",
    "\n",
    "- In this task, use the  [```CrossEntropyLoss```](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Shallow_MLP(LightningModule):\n",
    "    def __init__(self, input_features, hidden_node, classes, train_loader, test_loader, val_loader, learning_rate=1e-1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss_fun = ????\n",
    "        \n",
    "        self.linear1 =  ????\n",
    "        self.linear2 =  ????\n",
    "        \n",
    "        self.train_accuracy = ????\n",
    "        self.val_accuracy = ????\n",
    "        self.test_accuracy = ????\n",
    "            \n",
    "    def forward(self, x):\n",
    "        # Pass input through conv layers\n",
    "        # Don't forget the activation function\n",
    "        out1 = ????\n",
    "        out2 = ????\n",
    "    \n",
    "        return out2\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        # This should be similar to Task 1\n",
    "        \n",
    "        # get the x and y values of the data point\n",
    "        # pass through the network\n",
    "        # Get the loss\n",
    "        ????\n",
    "        ????\n",
    "        ????\n",
    "        \n",
    "        preds = logits.argmax(1)\n",
    "        self.train_accuracy.update(preds, y)\n",
    "        \n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log(\"train_acc\", self.train_accuracy, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # get the x and y values of the data point\n",
    "        # pass through the network\n",
    "        # Get the loss\n",
    "        ????\n",
    "        ????\n",
    "        ????\n",
    "        \n",
    "        preds = logits.argmax(1)\n",
    "        self.val_accuracy.update(preds, y)\n",
    "\n",
    "        # Calling self.log will surface up scalars for you in TensorBoard\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log(\"val_acc\", self.val_accuracy, prog_bar=True, on_step=False, on_epoch=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # get the x and y values of the data point\n",
    "        # pass through the network\n",
    "        # Get the loss\n",
    "        ????\n",
    "        ????\n",
    "        ????\n",
    "        \n",
    "        preds = logits.argmax(1)\n",
    "        self.test_accuracy.update(preds, y)\n",
    "\n",
    "        # Calling self.log will surface up scalars for you in TensorBoard\n",
    "        self.log(\"test_loss\", loss, prog_bar=True)\n",
    "        self.log(\"test_acc\", self.test_accuracy, prog_bar=True)\n",
    "        \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        # get the x and y values of the data point\n",
    "        # pass through the network\n",
    "        # Get the loss\n",
    "        ????\n",
    "        ????\n",
    "        ????\n",
    "        return (pred, y, x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # define the SGD optimizer\n",
    "        ????\n",
    "        return optimizer\n",
    "\n",
    "    ####################\n",
    "    # DATA RELATED HOOKS\n",
    "    ####################\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return train_loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return val_loader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## 2.3 Train and evaluate model's performance <a class=\"anchor\" id=\"2_3\"></a>\n",
    "\n",
    "Let's train the model with three different scenarios for **30 epochs**\n",
    "\n",
    "a. model_shallow_a : # hidden nodes = 8\n",
    "\n",
    "b. model_shallow_b : # hidden nodes = 128\n",
    "\n",
    "c. model_shallow_c : # hidden nodes = 1024\n",
    "\n",
    "In all the parts above, you should only use the training input and their labels to train your model. You may use the validation set to pick a trained model.\n",
    "\n",
    "For example, during training, you can test the accuracy of your model using the validation set every epoch and pick the model that achieves the highest validation accuracy. You should then report your results on the test set once you choose your model. More details can be found in the documentation of [```ModelCheckpoint```](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.callbacks.ModelCheckpoint.html).\n",
    "\n",
    "Note: Make sure to have different log file directories and checkpoint folders (eg: logs_task_2a, logs_task_2b and logs_task_2c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the input features and number of classes\n",
    "input_features = ????\n",
    "classes = ????\n",
    "\n",
    "# define the training epoch and learning rate\n",
    "max_epochs = 30\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario a: Initialize Shallow MLP model with hidden node = 8\n",
    "hidden_node = ????\n",
    "model_shallow_a = ????\n",
    "\n",
    "# Define checkpoint callback function to save best model\n",
    "checkpoint_callback_2a = ModelCheckpoint(\n",
    "        monitor=????, # the value to watch out on checking the model performance\n",
    "        dirpath=????, # path to save the checkpoint\n",
    "        save_top_k=?, # save the best model\n",
    "        mode=\"max\",\n",
    "        every_n_epochs=1\n",
    "    )\n",
    "\n",
    "# Train and test the model\n",
    "trainer_2a = Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    devices = 1 if not torch.cuda.is_available() else torch.cuda.device_count(),  \n",
    "    max_epochs=????,\n",
    "    callbacks=????,\n",
    "    logger=????,\n",
    ")\n",
    "\n",
    "# Start the training loop\n",
    "????\n",
    "\n",
    "# Report the test performance\n",
    "????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should expect some output like :\n",
    "\n",
    "[{'test_loss': 0.6990464925765991, 'test_acc': 0.7178385853767395}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario b: Initialize Shallow MLP model with hidden node = 128\n",
    "hidden_node = 128\n",
    "model_shallow_b = ????\n",
    "\n",
    "# Define checkpoint callback function to save best model\n",
    "checkpoint_callback_2b = ModelCheckpoint(\n",
    "        ????\n",
    "    )\n",
    "\n",
    "# Train and test the model\n",
    "trainer_2b = Trainer(\n",
    "    ????\n",
    ")\n",
    "\n",
    "# Start the training loop\n",
    "????\n",
    "\n",
    "# Report the test performance\n",
    "????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should expect some output like :\n",
    "\n",
    "[{'test_loss': 0.6637445688247681, 'test_acc': 0.7278985977172852}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario c: Initialize Shallow MLP model with hidden node = 1024\n",
    "hidden_node = 1024\n",
    "model_shallow_c = ????\n",
    "\n",
    "# Define checkpoint callback function to save best model\n",
    "checkpoint_callback_2c = ModelCheckpoint(\n",
    "        ????\n",
    "    )\n",
    "\n",
    "# Train and test the model\n",
    "trainer_2c = Trainer(\n",
    "    ????\n",
    ")\n",
    "\n",
    "# Start the training loop\n",
    "????\n",
    "\n",
    "# Report the test performance\n",
    "????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should expect some output like :\n",
    "\n",
    "[{'test_loss': 0.6303123235702515, 'test_acc': 0.7377262115478516}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## 2.3 Train and evaluate model's performance <a class=\"anchor\" id=\"2_4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Plot Training losses for the different shallow networks\n",
    "\n",
    "Now let's read the ```csv``` files that are stored during training and plot the losses for all different training setups.\n",
    "\n",
    "Make sure you include the following:\n",
    "- Make the plot on the same figure\n",
    "- Properly labelled axes\n",
    "- Proper title\n",
    "- Legends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read logs for 2a\n",
    "metrics_task_2a = ????\n",
    "????\n",
    "????\n",
    "\n",
    "# read logs for 2b\n",
    "metrics_task_2b = ????\n",
    "????\n",
    "????\n",
    "\n",
    "\n",
    "# read logs for 2c\n",
    "metrics_task_2c = ????\n",
    "????\n",
    "????\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot using matplotlib\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.plot(metrics_task_2a[\"train_loss\"][:-1])\n",
    "plt.plot(metrics_task_2b[\"train_loss\"][:-1])\n",
    "plt.plot(metrics_task_2c[\"train_loss\"][:-1])\n",
    "plt.title(\"Training Loss Shallow MLP\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "_ = plt.legend([\"Train Loss 8\",\"Train Loss 128\", \"Train Loss 1024\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Plot Validation accuracies for the different shallow networks\n",
    "\n",
    "Make sure you include the following for the accuracy plots too:\n",
    "- Make the plot on the same figure\n",
    "- Properly labelled axes\n",
    "- Proper title\n",
    "- Legends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot using matplotlib\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "### Discussion\n",
    "\n",
    "Please enter your answers below\n",
    "\n",
    "**Question 1: In task 2.1(b) Why have a balanced split across classes is necessary?**\n",
    "\n",
    "Answer:\n",
    "\n",
    "**Question 2: In the code provided below, select one model you have trained, then print out 5 predictions of test set along with groundtruth labels. How do you think the performance of the model?**\n",
    "\n",
    "Answer:\n",
    "\n",
    "**Question 3: Now please provide at least 2-3 suggestions to improve the performance of the shallow MLP, you should support your idea with reasoning or providing results of implementing the idea.**\n",
    "\n",
    "Answer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions using predict function\n",
    "outputs = ????\n",
    "\n",
    "predictions_2b, labels_2b, inputs_2b = ????\n",
    "\n",
    "# printout predictions along with ground truths \n",
    "n = 5  \n",
    "labels_2b = labels_2b.cpu().numpy()\n",
    "predictions_2b = ????\n",
    "\n",
    "for i in range(n):\n",
    "    print(f\"Prediction: {label_encoder.inverse_transform([predictions_2b[i]])[0]}\")\n",
    "    print(f\"Ground Truth: {label_encoder.inverse_transform([labels_2b[i]])[0]}\")\n",
    "    print(\"----------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "# Task 3 Deep MLP <a class=\"anchor\" id=\"task3\"></a>\n",
    "\n",
    "How about to build a different network for this dataset? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## Learning Objective\n",
    "\n",
    "This task aims to understand and implement a Deep MLP model to train neural networks on the CoverType dataset. By the end of this task, you will be able to:\n",
    "\n",
    "1. Implement a Deep Multi-Layer Perceptron (MLP) model using PyTorch Lightning's framework.\n",
    "\n",
    "2. Apply stochastic gradient descent (SGD) optimization method to train the MLP model on the CoverType dataset.\n",
    "\n",
    "3. Visualize and analyze the performance of the MLP model on the given samples to understand how well it fits the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## 3.1 Design Deep MLP <a class=\"anchor\" id=\"3_1\"></a>\n",
    "\n",
    "Design a Deep MLP:\n",
    " - You need to instantiate Deep_MLP class from LightningModule\n",
    " - Deep_MLP must take these inputs:\n",
    "   1. the number of input features\n",
    "   2. the number of classes\n",
    "   3. A list showing number of nodes in hidden layers (e.g hidden_layers = [n_hidden1 n_hidden2 n_hidden3 n_hidden4])\n",
    "   4. the learning_rate\n",
    "   5. the train loader\n",
    "   6. the test loader \n",
    "   7. the validation loader\n",
    "\n",
    " - And it should have metrics to evaluate model performance:\n",
    "   1. train_accuracy\n",
    "   2. test_accuracy\n",
    "   3. val_accuracy\n",
    "   - To define these metrics you should use Accuracy() class from pytorch-lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's define a deep MLP with 5 hidden layers, all with **ReLU** as activation functions afterwards: \n",
    "  - fc1: Input Layer: Linear(#input features $\\times$ n_hidden1) $\\rightarrow$ ReLU $\\rightarrow$\n",
    "  - fc2: Hidden Layer: Linear(n_hidden1 $\\times$ n_hidden2) $\\rightarrow$ ReLU $\\rightarrow$\n",
    "  - fc3: Hidden Layer: Linear(n_hidden2 $\\times$ n_hidden3) $\\rightarrow$ ReLU $\\rightarrow$\n",
    "  - fc4: Hidden Layer: Linear(n_hidden3 $\\times$ n_hidden4) $\\rightarrow$ ReLU $\\rightarrow$\n",
    "  - fc5: Output Layer: Linear(n_hidden4 $\\times$ classes) : Output layer maps the hidden nodes into class scores (logits)\n",
    "\n",
    "The Class should take a list of numbers during initialisation and define the network from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Deep_MLP(LightningModule):\n",
    "    def __init__(self, input_features, classes, hidden_layers, train_loader, val_loader, test_loader, learning_rate=1e-1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss_fun = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Define the 5 fc layers\n",
    "        self.linear1 = ????\n",
    "        ????\n",
    "        self.linear5 = ????\n",
    "        \n",
    "        self.train_accuracy = ????\n",
    "        self.val_accuracy = ????\n",
    "        self.test_accuracy = ????\n",
    "            \n",
    "    def forward(self, x):\n",
    "\n",
    "        # Pass input through layers\n",
    "        # Don't forget the activation function\n",
    "        \n",
    "        out1 = ????\n",
    "        ????\n",
    "        out5 = ????\n",
    "      \n",
    "        return out5\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # get the x and y values of the data point\n",
    "        # pass through the network\n",
    "        # Get the loss\n",
    "        ????\n",
    "        ????\n",
    "        ????\n",
    "        \n",
    "        preds = logits.argmax(1)\n",
    "        self.train_accuracy.update(preds, y)\n",
    "        \n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log(\"train_acc\", self.train_accuracy, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # get the x and y values of the data point\n",
    "        # pass through the network\n",
    "        # Get the loss\n",
    "        ????\n",
    "        ????\n",
    "        ????\n",
    "        \n",
    "        preds = logits.argmax(1)\n",
    "        self.val_accuracy.update(preds, y)\n",
    "\n",
    "        # Calling self.log will surface up scalars for you in TensorBoard\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log(\"val_acc\", self.val_accuracy, prog_bar=True, on_step=False, on_epoch=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # get the x and y values of the data point\n",
    "        # pass through the network\n",
    "        # Get the loss\n",
    "        ????\n",
    "        ????\n",
    "        ????\n",
    "        \n",
    "        preds = logits.argmax(1)\n",
    "        self.test_accuracy.update(preds, y)\n",
    "\n",
    "        # Calling self.log will surface up scalars for you in TensorBoard\n",
    "        self.log(\"test_loss\", loss, prog_bar=True)\n",
    "        self.log(\"test_acc\", self.test_accuracy, prog_bar=True)\n",
    "        \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "       # get the x and y values of the data point\n",
    "        # pass through the network\n",
    "        # Get the loss\n",
    "        ????\n",
    "        ????\n",
    "        ????\n",
    "        return pred, y, x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # define the SGD optimizer\n",
    "        ????\n",
    "        return optimizer\n",
    "\n",
    "    ####################\n",
    "    # DATA RELATED HOOKS\n",
    "    ####################\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return train_loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return val_loader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## 3.2 Train and evaluate model's performance <a class=\"anchor\" id=\"3_2\"></a>\n",
    "\n",
    "Now, create an instance of the model and with the following hidden layers: [128, 64, 32, 16] \n",
    "\n",
    "\n",
    "The forward propagation of model should follow this:\n",
    "- Hidden layers: [n_hidden1 , n_hidden2 , n_hidden3, n_hidden4]\n",
    "- Linear(#input features $\\times$ n_hidden1) $\\rightarrow$ ReLU $\\rightarrow$ Linear(n_hidden1 $\\times$ n_hidden2) $\\rightarrow$ ReLU $\\rightarrow$  Linear(n_hidden2 $\\times$ n_hidden3) $\\rightarrow$ Relu $\\rightarrow$ Linear(n_hidden3 $\\times$ n_hidden4)  $\\rightarrow$ Relu $\\rightarrow$ Linear(n_hidden4 $\\times$ classes)\n",
    "- Note: Make sure to set name for log file directories (eg: logs_task_3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the epochs and learning rate\n",
    "max_epochs = 30\n",
    "lr = 1e-3\n",
    "\n",
    "# define the input features and output classes\n",
    "input_features = ????\n",
    "classes = ????\n",
    "\n",
    "# Initialize Deep MLP model\n",
    "hidden_layers = ????\n",
    "model_deep_a = ????\n",
    "\n",
    "# Define checkpoint callback function to save best model\n",
    "checkpoint_callback_3 = ModelCheckpoint(\n",
    "        ????\n",
    "    )\n",
    "\n",
    "# Train and Test Model\n",
    "trainer_3 = Trainer(\n",
    "    ????\n",
    ")\n",
    "\n",
    "# Start the training loop\n",
    "????\n",
    "\n",
    "# Report the test performance\n",
    "????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should expect an output like :\n",
    "[{'test_loss': 0.7383896708488464, 'test_acc': 0.7318916320800781}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## 3.3 Visualise the results for the network. <a class=\"anchor\" id=\"3_3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Plot the training loss and validation loss on the same figure. Make sure you have the axes and title properly labelled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read logs for 3\n",
    "metrics_task_3 =????\n",
    "????\n",
    "????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot using matplotlib\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Plot the training accuracy and validation accuracy on the same figure. Make sure you have the axes and title properly labelled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot using matplotlib\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "### Discussion:\n",
    "\n",
    "**Question: Did you observe a strange behaviour for the deep MLPs? If yes, what is that? Discuss what might be the source of that.**\n",
    "\n",
    "Answer:\n",
    "\n",
    "**Question: How does the deep MLP compare with the shallow MLP for this case?**\n",
    "\n",
    "Answer:\n",
    "\n",
    "**Question: Compare the Deep MLP performance trained in Task 3 with the Shallow MLPs trained in Task 2. How can we improve Deep MLP performance more? Try other network setups for number of hidden nodes or other settings in the Deep MLP and Explain why or why not the performance changed after your modifications?**\n",
    "\n",
    "Answer:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "# <center> Well done! This is the end of Lab 3\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
