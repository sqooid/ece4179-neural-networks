{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "  <img src=\"https://www.dropbox.com/s/vold2f3fm57qp7g/ECE4179_5179_6179_banner.png?dl=1\" alt=\"ECE4179/5179/6179 Banner\" style=\"max-width: 60%;\"/>\n",
    "</div>\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "# Sentiment Analysis with Logistic Regression\n",
    "\n",
    "</div>\n",
    "\n",
    "Welcome to **ECE4179/5179/6179 Week 3**! Our primary objective in this notebook is to introduce the concept of **logistic regression** and its application in **binary classification**. We will use our knowledge to determine the sentiment of a given text. Sentiment analysis is to determine the attitude of a speaker or a writer with respect to some topic or the overall contextual polarity of a document. The attitude may be his or her judgment or evaluation (see [Wikipedia](https://en.wikipedia.org/wiki/Sentiment_analysis)). Sentiment analysis is widely applied to reviews and social media for a variety of applications, ranging from marketing to customer service. We will show how we can build a sentiment analysis model using a simple linear classifier and a Large Language Model (LLM). How exciting!\n",
    "\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"data/sent_classifier.png\" alt=\"drawing\" width=\"600\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Let's get started! In this notebook, we will use the following Python packages:\n",
    "\n",
    "- [torch](https://pytorch.org/) is the PyTorch package for deep learning implementation.\n",
    "- [transformers](https://huggingface.co/transformers/) is a state-of-the-art library for Natural Language Processing (NLP) tasks.\n",
    "- [datasets](https://huggingface.co/docs/datasets/) is a library for easily accessing and processing datasets.\n",
    "- [NumPy](https://numpy.org/) is a package for scientific computing with Python.\n",
    "- [Matplotlib](https://matplotlib.org/) is a comprehensive library for creating visualizations in Python.\n",
    "- [Pandas](https://pandas.pydata.org/) is a package for data analysis.\n",
    "- [Scikit-learn](https://scikit-learn.org/stable/) is a comprehensive package for machine learning in Python.\n",
    "- [ipywidgets](https://ipywidgets.readthedocs.io/en/latest/) is a Python library that provides interactive HTML widgets for Jupyter notebooks.\n",
    "- [tqdm](https://tqdm.github.io/) is a package that allows for visualizing the progress of tasks in Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable autoreload extension\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "RND_SEED = 42\n",
    "np.random.seed(RND_SEED)  # For reproducibility\n",
    "torch.manual_seed(RND_SEED)  # PyTorch random seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========================================================================================================\n",
    "# plot the decision boundary\n",
    "def plot_decision_boundary(X, y, model):\n",
    "\n",
    "    # let's first compute the accuracy of the logistic regression model\n",
    "    y_pred_data = model.predict(X) # predictions on the original data\n",
    "\n",
    "    accuracy = accuracy_score(y, y_pred_data)\n",
    "    # precision = precision_score(y, y_pred_data)\n",
    "    # recall = recall_score(y, y_pred_data)\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 # get the min and max of the first feature\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 # get the min and max of the second feature\n",
    "\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), # create a meshgrid\n",
    "                         np.arange(y_min, y_max, 0.02))\n",
    "\n",
    "    x_grid = np.c_[xx.ravel(), yy.ravel()] # create a grid of points\n",
    "\n",
    "    y_pred = model.predict(x_grid) # predict the class of each point in the grid\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    # Create mask for positive and negative classes\n",
    "    pos_idx = y == 1\n",
    "    neg_idx = y == 0\n",
    "\n",
    "    # Plot positive and negative points\n",
    "    plt.scatter(X[pos_idx, 0], X[pos_idx, 1], c='r', alpha=0.6, edgecolors='k', marker='o', s=50, label='Positive class')\n",
    "    plt.scatter(X[neg_idx, 0], X[neg_idx, 1], c='b', alpha=0.6, edgecolors='k', marker='s', s=50, label='Negative class')\n",
    "\n",
    "\n",
    "\n",
    "    # Plot the decision boundary\n",
    "    plt.contourf(xx, yy, y_pred.reshape(xx.shape), cmap=plt.cm.coolwarm, alpha=0.2)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    # plt.title(f'Logistic Regression\\nAccuracy: {accuracy:.2f} Precision: {precision:.2f} Recall: {recall:.2f}')\n",
    "    plt.title(f'Logistic Regression\\nAccuracy: {accuracy:.2f}')\n",
    "\n",
    "    # Display the legend\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Logistic Regression\n",
    "\n",
    "Logistic regression is a linear classification method for binary problems. To better understand its workings, we start by applying the algorithm to a toy dataset. The steps of the algorithm are as follows:\n",
    "\n",
    "1. **Initialize the weights and bias**:\n",
    "   - Initialize the weights $\\mathbf{w}$ to small random values or zeros.\n",
    "\n",
    "2. **Compute the prediction**:\n",
    "   - For each data point $\\color{pink}{\\big(\\mathbf{x}_i, y_i\\big)}$, compute the logistic regression's prediction, denoted by $\\color{pink}{\\hat{y}}$, which is determined by the expression \n",
    "   \\begin{align} \n",
    "   \\hat{y}_i = \\sigma\\big(\\mathbf{w}^\\top\\mathbf{x}_i\\big)\\;,\n",
    "   \\end{align}\n",
    "   where $\\sigma$ is the sigmoid function defined as:\n",
    "   \\begin{align} \n",
    "   \\sigma(x) = \\frac{1}{1 + \\exp{(-x)}}\\;.\n",
    "   \\end{align}\n",
    "\n",
    "3. **Compute the loss**:\n",
    "   - The loss function for logistic regression is the binary cross-entropy loss, given by:\n",
    "   \\begin{align} \n",
    "   \\mathcal{L} = -\\frac{1}{m}\\sum_{i=1}^m \\Big[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\Big]\\;,\n",
    "   \\end{align}\n",
    "   where $m$ is the number of data points.\n",
    "\n",
    "4. **Update the weights**:\n",
    "   - Underneath, weights are updated using gradient descent. The update rules are:\n",
    "   \\begin{align} \n",
    "   \\mathbf{w} &\\leftarrow \\mathbf{w} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{w}}\\;,\n",
    "   \\end{align}\n",
    "   where $\\eta$ is the learning rate, and the gradients are computed as:\n",
    "   \\begin{align} \n",
    "   \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{w}} &= \\frac{1}{m}\\sum_{i=1}^m (\\hat{y}_i - y_i) \\mathbf{x}_i\\;.\n",
    "   \\end{align}\n",
    "\n",
    "5. **Repeat steps 2 to 4**:\n",
    "   - Continue iterating through steps 2 to 4 until convergence is achieved. This occurs when the change in the loss function is below a certain threshold or a specified number of iterations is reached.\n",
    "\n",
    "### Toy dataset\n",
    "We will use a toy dataset that consists of two classes, each with two features. The data points are plotted below and as you can see is linearly separable. Our first goal is to design and train a perceptron to classify the data points into their respective classes. Note that the red class is labeled as $+1$ and the blue class is labeled as $-1$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U scikit-learn\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "# Create a 2D toy dataset from two Gaussian distributions\n",
    "# with 100 samples each, 2 features each, and 2 classes\n",
    "# with 1 cluster per class\n",
    "\n",
    "\n",
    "X, y = make_blobs(n_samples=100, n_features=2, \n",
    "                  cluster_std=1.0, centers=2, center_box=(-5.0, 5.0),\n",
    "                  random_state=RND_SEED)\n",
    "\n",
    "# X is a 2D array of shape (100, 2), each row is a sample\n",
    "# y is a 1D array of shape (100,), each element is a class label (0 or 1)\n",
    "pos_idx = y == 1\n",
    "neg_idx = y == 0\n",
    "\n",
    "# Plot the dataset\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[pos_idx, 0], X[pos_idx, 1], c='r', alpha=0.6, edgecolors='k', marker='o', s=50, label='Positive class')\n",
    "plt.scatter(X[neg_idx, 0], X[neg_idx, 1], c='b', alpha=0.6, edgecolors='k', marker='s', s=50, label='Negative class')\n",
    "plt.grid(True)\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #3352FF; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "### <span style=\"color: pink;\">Task #1.</span>  Working with Logistic Regression\n",
    "Your task is to use the  `LogisticRegression` class from the `sklearn.linear_model` module. This method is efficient and widely used in practice. The good thing about the sklearn implementation is that it takes care of the optimization process for you. In other words, you don't have to worry the learning rate. \n",
    "</div>\n",
    "\n",
    "<details>\n",
    "<summary style=\"color: yellow; font-weight: bold;\">Show Answer</summary>\n",
    "    \n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# Initialize the LogisticRegression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Fit the model to the data\n",
    "model.fit(X, y)\n",
    "\n",
    "# Retrieve the weights (coefficients) and bias (intercept)\n",
    "weights = model.coef_[0]\n",
    "bias = model.intercept_[0]\n",
    "\n",
    "print(\"Weights:\", weights)\n",
    "print(\"Bias:\", bias)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Use the LogisticRegression class from scikit-learn and fit method to train the model\n",
    "# TODO <--- write your code here\n",
    "\n",
    "\n",
    "# Print the weights and bias of the trained model\n",
    "weights = # TODO <--- write your code here\n",
    "bias = # TODO <--- write your code here\n",
    "\n",
    "print(f'Weights: {weights}')\n",
    "print(f'Bias: {bias}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #3352FF; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "\n",
    "\n",
    "### <span style=\"color: pink;\">Task #2.</span>  Study the trained model\n",
    "Use plot_decision_boundary to plot the decision boundary for the weights and biases of your model. The plot_decision_boundary also reports some numbers in its title, can you figure out what they are?  \n",
    "</div>\n",
    "\n",
    "<details>\n",
    "<summary style=\"color: yellow; font-weight: bold;\">Show Answer</summary>\n",
    "\n",
    "\n",
    "For your convenience, we have provided a function to plot the decision boundary of the logistic regression model. Recall that the decision boundary is the line that separates the two classes. The function is called `plot_decision_boundary` and can be called as shown below.\n",
    "\n",
    "```python\n",
    "plot_decision_boundary(X, y, model)\n",
    "# X: data (each row is a sample)\n",
    "# y: labels (each row is a label)\n",
    "# model: trained logistic regression model\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the plot_decision_boundary function to plot the decision boundary\n",
    "\n",
    "# TODO <--- write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #3352FF; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "### <span style=\"color: pink;\">Task #3.</span>  A more difficult problem\n",
    "Patterns are not always linearly separable. In this task, we will work with a dataset that is not linearly separable. The data points are plotted below. Your task is to train a logistic regression model on this dataset and plot the decision boundary.\n",
    "</div>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "X_circ, y_circ = make_circles(n_samples=1_000, factor=0.3, noise=0.05, random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "# X is a 2D array of shape (100, 2), each row is a sample\n",
    "# y is a 1D array of shape (100,), each element is a class label (0 or 1)\n",
    "pos_idx = y_circ == 1\n",
    "neg_idx = y_circ == 0\n",
    "\n",
    "# Plot the dataset\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_circ[pos_idx, 0], X_circ[pos_idx, 1], c='r', alpha=0.6, edgecolors='k', marker='o', s=50, label='Positive class')\n",
    "plt.scatter(X_circ[neg_idx, 0], X_circ[neg_idx, 1], c='b', alpha=0.6, edgecolors='k', marker='s', s=50, label='Negative class')\n",
    "plt.grid(True)\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, use the LogisticRegression class from scikit-learn and fit method to train the model on the circular dataset\n",
    "# TODO <--- write your code here\n",
    "\n",
    "\n",
    "# Print the weights and bias of the trained model on the circular dataset\n",
    "# TODO <--- write your code here\n",
    "\n",
    "\n",
    "# use the plot_decision_boundary function to plot the decision boundary on the circular dataset\n",
    "# TODO <--- write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #3352FF; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "### <span style=\"color: pink;\">Task #4.</span>  Can you improve the model?\n",
    "You have learned about polynomial features in the previous weeks. Can you improve the model by adding polynomial features to the data?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Use the PolynomialFeatures class from scikit-learn to create a polynomial feature transformer\n",
    "# and then use the LogisticRegression class from scikit-learn to train the model\n",
    "# opt for choosing a polynomial with minimum degree that can separate the two classes\n",
    "\n",
    "\n",
    "# TODO <--- write your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-2\n",
    "\n",
    "\n",
    "GPT-2 is a next-word prediction model developed by OpenAI and is the ancestor of chatGPT.\n",
    " It predicts the most probable word based on the context provided by all the words fed into it. In this sense, we can say that the GPT-2 is basically the next word prediction feature of a keyboard app, but one that is much larger and more sophisticated than what your phone has. \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Let's looks how GPT-2 works!\n",
    "<br>\n",
    "<p align=\"center\">\n",
    "<img src=\"data/gpt-2-output.gif\" alt=\"drawing\" width=\"600\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task we are going to use `transformers` library from [Hugging FaceðŸ¤—](https://huggingface.co/docs/transformers/index). Hugging Face is an open-source framework which provides an easy and cost-free way to work with a wide variety of open-source models including vision,language and audio models.\n",
    "\n",
    "We will use the [IMDB](https://huggingface.co/datasets/imdb) dataset from Hugging Face. This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We are going to use  a set of 1,000 highly polar movie reviews for training, and 1,000 for testing.\n",
    "\n",
    "In this task, we are going to get the representation of movie reviews, and use `Logistic rRegression` from scikit-learn library to classify these representations as $\\textcolor{green}{\\mathbf{Positive}}$ or $\\textcolor{red}{\\mathbf{Negative}}$ .\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"data/model_sent.png\" alt=\"drawing\" width=\"600\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you have not installed the transformers and datasets library, you can install it using the following command\n",
    "\n",
    "!pip install transformers -q\n",
    "!pip install datasets -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/imdb_tiny_train.csv')\n",
    "test_df = pd.read_csv('data/imdb_tiny_test.csv')\n",
    "\n",
    "# Printing the first 5 rows of the training data\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "However, language models can not directly understand natural language texts. They can only process numbers, so we need to find a way to convert the raw text to numbers. We use a process called `tokenization` tokenization to find the most meaningful representation â€” that is, the one that makes the most sense to the model â€” and, if possible, the smallest representation.\n",
    "\n",
    "Tokenization process does two main functions.\n",
    "1. Split the raw text into words, sub-words or characters, which we call as **tokens**.\n",
    "2. Represent each token to a unique ID.\n",
    "\n",
    "\n",
    "The model uses these IDs to identify each word.\n",
    "\n",
    "<!-- ![tokenization](tokenization.png) -->\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"data/tokenization.png\" alt=\"drawing\" width=\"600\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the GPT-2 model and its tokenizer\n",
    "\n",
    "We will use the `GPT2Tokenizer` from the `transformers` library to tokenize the text. I have to add that you can find SOTA LLMs such as LLAMA3 (Meta), Phi-3 (Microsoft), Gemma2 (Google), and many more in the Hugging Face model hub. Interestingly, many models on the [arena leaderboard](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard) are available for you to use, and if you have a good machine, you can use a bigger and more powerful model. I leave exploring these models to you as an exercise, and use GPT-2 in the workshop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel \n",
    "\n",
    "\n",
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(torch_device)\n",
    "\n",
    "\n",
    "gpt_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "gpt_model = AutoModel.from_pretrained(\"gpt2\", pad_token_id=gpt_tokenizer.eos_token_id).to(torch_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the tokenization process using GPT-2.\n",
    "\n",
    "A language model process the text in two stages. First, it uses a model called Tokenizer to convert the  text into a list of numerical values. Then, these numerical values are processed by a neural network  to generate the output.\n",
    "\n",
    "\n",
    "If a tokenizer does not know a word, either it will split it into subwords or it will replace sub-word tokens\n",
    "or it will replace the word with the unknown token $(<\\text{unk}>)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello from the ECE4179 workshop\"\n",
    "tokens = gpt_tokenizer.encode(text, return_tensors='np').squeeze()\n",
    "print(tokens)\n",
    "print('*'*50)\n",
    "# Decode the tokens\n",
    "for token in tokens:\n",
    "    decoded_token = gpt_tokenizer.decode([token])\n",
    "    print(f\"{token} --> {[decoded_token]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's feed the model with a new sentence and see its output.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"data/pred.png\" alt=\"drawing\" width=\"750\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the input text\n",
    "inputs = gpt_tokenizer.encode(text, return_tensors='pt').to(torch_device)\n",
    "\n",
    "# Feed the input to the model\n",
    "outputs = gpt_model(inputs)\n",
    "\n",
    "z = outputs[\"last_hidden_state\"].detach().cpu().numpy().squeeze()\n",
    "\n",
    "print(f'Input shape: {inputs.shape}')\n",
    "print(f'Output shape: {z.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see, the output is 8x768, which means that we have 8 tokens in the sentence and each token is represented by a 768-dimensional vector. For simplicity, we will use the average of all the tokens as the sentence representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.mean(z, axis=0) # Average the embeddings of all tokens\n",
    "print(f'Output Average shape: {z.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use two helper functions to process the text in batches and get the review representation. The first function is `process_batch` which takes a list of reviews and returns the average of each review as above. The second function is `process_dataset` which takes the dataset and split the dataset to batches and process each batch using the `process_batch` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(text_batch, tokenizer, model, device='cpu'):\n",
    "    \"\"\"Preprocess a batch of text for sentiment analysis.\n",
    "\n",
    "    Args:\n",
    "        text_batch (list): A list of strings to be processed.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of processed strings.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    # Tokenize the text. Note that the encode method does not work for batches\n",
    "    inputs = tokenizer(text_batch, padding=True, truncation=True, max_length=1024, return_tensors='pt').to(device)\n",
    " \n",
    "\n",
    "    if model.device != device:\n",
    "        model.to(device)  # Move model to GPU if available\n",
    "    with torch.inference_mode():  # No need to calculate gradients\n",
    "        # Feed the input to the model. These are not ids anymore and ** is used to unpack the dictionary\n",
    "        outputs = model(**inputs) \n",
    "\n",
    "    z = outputs.last_hidden_state.cpu().numpy()  # Move outputs back to CPU\n",
    "    z = z.mean(axis=1)  # Average over sequence length\n",
    "\n",
    "    return z\n",
    "\n",
    "\n",
    "\n",
    "# Function to transform a dataset in batches\n",
    "def process_dataset(dataset, tokenizer, model, batch_size=8, device='cpu'):\n",
    "    X = []\n",
    "    for i in tqdm(range(0, len(dataset), batch_size)):\n",
    "        batch_texts = dataset[i:i+batch_size]\n",
    "        batch_outputs = process_batch(batch_texts, tokenizer, model, device=device)\n",
    "        X.extend(batch_outputs)\n",
    "    return np.array(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the training and testing datasets\n",
    "\n",
    "We will process the training and testing datasets using the `process_dataset` function. The function will return the representations of the reviews in the training and testing datasets. We will use these representations to train and evaluate the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = process_dataset(train_df['text'].tolist(), gpt_tokenizer, gpt_model, device=torch_device)\n",
    "X_test = process_dataset(test_df['text'].tolist(), gpt_tokenizer, gpt_model, device=torch_device)\n",
    "\n",
    "y_train = train_df['label'].values\n",
    "y_test = test_df['label'].values\n",
    "\n",
    "\n",
    "# # # if you could not run GPT2 on your laptop, you can just load the data via the following code\n",
    "# X_train = np.load('data/ECE4179_GPT2_data.npz')['X_train']\n",
    "# X_test = np.load('data/ECE4179_GPT2_data.npz')['X_test']\n",
    "# y_train = np.load('data/ECE4179_GPT2_data.npz')['y_train']\n",
    "# y_test = np.load('data/ECE4179_GPT2_data.npz')['y_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color: #3352FF; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "### <span style=\"color: pink;\">Task #5.</span>  Use a logistic regression model to perform sentiment analysis on the IMDB dataset.\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "In this task, you will use the [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) from sklearn to train a logistic regression model. Evaluate your model on the test data and discuss your findings.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"data/final_pred.png\" alt=\"drawing\" width=\"800\"/>\n",
    "</p>\n",
    "\n",
    "<details>\n",
    "<summary style=\"color: yellow; font-weight: bold;\">Show Answer</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Create a Logistic Regression model\n",
    "model = LogisticRegression(random_state=RND_SEED, \n",
    "                           max_iter=1000,\n",
    "                           solver='saga')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy, precision, recall, and F1 score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test,y_pred)\n",
    "\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy))\n",
    "print(\"Precision: {:.2f}\".format(precision))\n",
    "print(\"Recall: {:.2f}\".format(recall))\n",
    "print(\"F1 Score: {:.2f}\".format(f1))\n",
    "''' \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that you have processed the text using GPT2, you can use the processed data to train a logistic regression model\n",
    "# train a logistic regression model on the processed data and evaluate the model using the test data\n",
    "\n",
    "\n",
    "# TODO <--- write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your model \n",
    "\n",
    "How about we test our model with some recent movie reviews? Let's see how our model performs on these reviews. I have provided a function called `predict_sentiment` that takes a review and returns the sentiment of it. I also checked the reviews for `Deadpool & Wolverine' and here are two reviews for you to test your model.\n",
    "\n",
    "Review 1: \"The Bloodiest Mayhem Of The Century! We've waited so long for this moment, and it was beyond fun, wholesome, full of surprises, emotional and epic! Ryan Reynolds, Hugh Jackman and Shawn Levy pour out their hearts for this movie to be what is it.\"\n",
    "\n",
    "Review 2: \"This isnt a movie, its a checklist.See, i need a little thing called motivation. Its why i could not just sit back and enjoy the movie. I need a real story and I need to figure out why the character is doing what they are doing and this whole movie just felt like stuff happening for no real reason. Does this movie have a story....eh...not really. Its just stuff happening. Just Deadpool and Wolverine fighting again, and again. Lets try stabbing Deadpool some more. That didnt work the first 100 times, but maybe it will work now. No slicing, no dicing, just stabbing, which is easier on the sfx department. Sure the fight scenes were cool I guess. But not amazing.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text, language_model, language_tokenizer, clf):\n",
    "    # Encode the input text\n",
    "    inputs = language_tokenizer.encode(text, return_tensors='pt').to(torch_device)\n",
    "\n",
    "    # Feed the input to the model\n",
    "    outputs = language_model(inputs)\n",
    "\n",
    "    # Extract and average the hidden states\n",
    "    z = outputs.last_hidden_state.cpu().detach().numpy()\n",
    "    z = z.mean(axis=1)\n",
    "\n",
    "    # Predict sentiment using the classifier\n",
    "    y_pred = clf.predict(z)\n",
    "\n",
    "    print(f\"Sentiment Polarity: {'Positive' if y_pred[0] else 'Negative'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"The Bloodiest Mayhem Of The Century! We've waited so long for this moment, and it was beyond fun, wholesome, full of surprises, emotional and epic! Ryan Reynolds, Hugh Jackman and Shawn Levy pour out their hearts for this movie to be what is it.\"\n",
    "text2 =\"This isnt a movie, its a checklist.See, i need a little thing called motivation. Its why i could not just sit back and enjoy the movie. I need a real story and I need to figure out why the character is doing what they are doing and this whole movie just felt like stuff happening for no real reason. Does this movie have a story....eh...not really. Its just stuff happening. Just Deadpool and Wolverine fighting again, and again. Lets try stabbing Deadpool some more. That didnt work the first 100 times, but maybe it will work now. No slicing, no dicing, just stabbing, which is easier on the sfx department. Sure the fight scenes were cool I guess. But not amazing.\"\n",
    "\n",
    "\n",
    "predict_sentiment(text1,gpt_model,gpt_tokenizer,model)\n",
    "predict_sentiment(text2,gpt_model,gpt_tokenizer,model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "In this notebook, we explored binary classification tasks. Here are the key takeaways:\n",
    "\n",
    "### Logistic Regression and Decision Boundaries:\n",
    "\n",
    "We used logistic regression to classify data points and visualized decision boundaries. We learned that a logistic regression model can only classify linearly separable data. Using polynomial features, we can employ logistic regression to classify non-linear data.\n",
    "\n",
    "### Sentiment Analysis with GPT-2\n",
    "\n",
    "We used GPT-2, a language model developed by OpenAI along with a logistic regression model to perform sentiment analysis on the IMDB dataset. For each review, we tokenized the text data and used the average output of GPT2 as text representation. We trained a logistic regression model on the resulting representations and evaluated the model on the test data and some reviews from the latest movie `Deadpool & Wolverine'.\n",
    "\n",
    "\n",
    "### Want to do more?\n",
    "\n",
    "- Try using a different language model such as BERT, LLAMA, or Phi-3 for sentiment analysis.\n",
    "\n",
    "- If you use an instruct model, you can also prepare a response to the negative reviews. Certain models in the transformer library have generate method that can be used to generate text. Check this [page](https://huggingface.co/openai-community/gpt2) to learn about generating text with GPT2."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
