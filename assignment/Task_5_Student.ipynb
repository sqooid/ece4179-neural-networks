{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "# Assignment\n",
    "\n",
    "Welcome to the task 5 of the assignment! This notebook contains all the code and comments that you need to submit. The places where you need to edit are highlighted in red. Feel free to add in your own markdown for additional comments.\n",
    "\n",
    "__Submission details: make sure you all your outputs have been generated. Submit this Jupyter Notebook__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Enter you student details below</b>\n",
    "\n",
    "- <b>Student Name:</b> Lucas Liu\n",
    "- <b>Student ID:</b> 31445179"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "# Speech Recognition Task\n",
    "\n",
    "Speech recognition is the task of converting spoken language into written text. It involves processing audio signals and applying machine learning algorithms to recognize and transcribe spoken words or phrases.\n",
    "\n",
    "- Speech recognition has a wide range of applications, including Voice-Controlled Systems; Speech recognition is used in voice-controlled systems for controlling smart home devices, navigation systems, and automotive applications.\n",
    "\n",
    "- To perform speech recognition, various techniques and models are used, including Hidden Markov Models (HMMs), Deep Neural Networks (DNNs), and Transformer-based models like Wave2Vec2. These models are trained on large datasets of labeled speech data to learn the patterns and features of spoken language.\n",
    "\n",
    "- In this task, we are exploring the task of speech recognition using the Wave2Vec2 model. We preprocess audio data, extract features using the pretrained Wave2Vec2 model, and train a simple MLP model to predict the labels. The dataset used in this notebook is the Speech Commands dataset, which consists of audio recordings of various command words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## Kaggle Competition: ECSE 4179/5179/6179 - Assignment 1\n",
    "\n",
    "Welcome to the Kaggle competition for ECSE 4179/5179/6179 - Assignment 1! This competition is part of the course assignments and is designed to test your skills in speech recognition using the Wave2Vec2 model.\n",
    "\n",
    "### Competition Details\n",
    "\n",
    "- Competition Link: [ECSE 4179/5179/6179 - Assignment 1](https://www.kaggle.com/competitions/ecse-4179-5179-6179-2024-s-2-assignment-1)\n",
    "- Registration: You will have been registered with your student email addresses. Please check the above link to see if you can access the assignment.\n",
    "- Access to Competition Dataset: The competition dataset is available in the Data tab of the Kaggle competition page.\n",
    "- Competition Rules: The competition rules can be found in the Rules tab of the Kaggle competition page.\n",
    "- Competition Description: The competition description and task details can be found in the Description tab of the Kaggle competition page.\n",
    "\n",
    "### Competition Task\n",
    "\n",
    "The task of this competition is speech recognition, which involves converting spoken language into written text. Participants are required to process audio signals and apply machine learning algorithms to recognize and transcribe spoken words or phrases.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "The competition dataset consists of audio recordings of various command words. The dataset is divided into three folders: Train, Validation, and Test. The Train folder contains audio files for training the models, the Validation folder contains audio files for hyperparameter tuning, and the Test folder contains audio files for final predictions.\n",
    "\n",
    "### Evaluation\n",
    "\n",
    "The evaluation metric for this competition is accuracy, which measures the percentage of correctly predicted labels.\n",
    "\n",
    "### Getting Started\n",
    "\n",
    "To get started with the competition, please follow these steps:\n",
    "\n",
    "1. Access the competition dataset from the Data tab.\n",
    "2. Read the competition rules and guidelines from the Rules tab.\n",
    "3. Understand the competition task and description from the Description tab.\n",
    "4. Download the dataset and explore the audio files.\n",
    "5. Preprocess the audio data, extract features using the Wave2Vec2 model, and train your models.\n",
    "6. Tune the hyperparameters using the validation dataset.\n",
    "7. Make predictions on the test dataset and submit your results.\n",
    "8. Monitor your leaderboard position and improve your models if necessary.\n",
    "9. Submit your final predictions before the assignment deadline.\n",
    "\n",
    "Good luck and have fun participating in the Kaggle competition!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "# Libraries\n",
    "\n",
    "Libraries are important as it saves us time from writing our own functions all the time such as graphing, or creating matrices. Brief library descriptions have been added for every library that we import. You may also check the official documentations for more details.\n",
    "\n",
    "The required libraries are specified in this notebook for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# import all Python packages needed here\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# PyTorch or Pytorch Lightning(depend on your choice)\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# TorchMetrics\n",
    "from torchmetrics.classification import Accuracy\n",
    "\n",
    "# Hugging Face\n",
    "from datasets import load_dataset\n",
    "from transformers import Wav2Vec2Model, Wav2Vec2Processor  # depend on your choice\n",
    "\n",
    "# Audio\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "\n",
    "RND_SEED = 42\n",
    "np.random.seed(RND_SEED)\n",
    "torch.manual_seed(RND_SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# torch.set_default_device(device)\n",
    "\n",
    "# Import more libraries if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### Dataset\n",
    "To work with the audio data in this project, you will need to download the zip file **speech_wav.zip** from the Data tab in the Kaggle Competeition Page. Once you have downloaded the zip file, extract its contents, and move **speech_wav** folder to the same directory as your Python notebook. In **speech_wav** folder you have three folders: Train, Validation, and Test.\n",
    "\n",
    "- The **Train** folder contains 18,538 audio files, named trn_00000.wav to trn_18537.wav. You will use these audio files to train your model.\n",
    "\n",
    "- The **Validation** folder contains 2,577 audio files, named val_00000.wav to val_02576.wav. You will use these audio files to find the best hyperparameters for your model.\n",
    "\n",
    "- The **Test** folder contains 2,567 audio files, named tst_00000.wav to tst_02566.wav. You will save your model's predictions using these audio files for the final submission.\n",
    "\n",
    "In addition to the audio files, you will also need the corresponding labels for the training and validation phases. The labels are provided in two CSV files: train.csv and validation.csv.\n",
    "\n",
    "- The **train.csv** file lists the file names under the **ID** column and their corresponding labels under the **Label** column. For example, an entry in the validation.csv file would look like this:\n",
    "\n",
    "    ID, Label\n",
    "    \n",
    "    val_00001.wav, 3\n",
    "\n",
    "    This indicates that the file val_00001.wav is labeled as category \"3,\" which corresponds to the command \"Down.\"\n",
    "\n",
    "The ten command labels are mapped as follows:\n",
    "\n",
    "- 0: Yes\n",
    "- 1: No\n",
    "- 2: Up\n",
    "- 3: Down\n",
    "- 4: Left\n",
    "- 5: Right\n",
    "- 6: On\n",
    "- 7: Off\n",
    "- 8: Stop\n",
    "- 9: Go\n",
    "\n",
    "To get started, please download the datasets from the Data tab on the Kaggle page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping of prefixes to the ten command labels\n",
    "label_mapping = {\n",
    "    \"0_\": \"Yes\",\n",
    "    \"1_\": \"No\",\n",
    "    \"2_\": \"Up\",\n",
    "    \"3_\": \"Down\",\n",
    "    \"4_\": \"Left\",\n",
    "    \"5_\": \"Right\",\n",
    "    \"6_\": \"On\",\n",
    "    \"7_\": \"Off\",\n",
    "    \"8_\": \"Stop\",\n",
    "    \"9_\": \"Go\",\n",
    "}\n",
    "\n",
    "# Define the path to speech_wav folder\n",
    "dataset_path = \"./data/speech\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "Before you begin to play with the data, you will need to pre-process the audio data. Preprocessing audio data is an essential step in speech recognition tasks. It involves extracting meaningful features from the raw audio signals that can be used as input to machine learning models. There are various models available for audio feature extraction, such as Waveform-based models like Wave2Vec2.\n",
    "\n",
    "You can choose a model that best suits their needs and expertise to preprocess the audio files. For example, you can use the pretrained Wave2Vec2 model provided by Hugging Face to extract feature embeddings from the audio files. This model has been trained on a large amount of labeled speech data and can capture the patterns and features of spoken language effectively.\n",
    "\n",
    "To preprocess the audio files using the Wave2Vec2 model, follow these steps:\n",
    "\n",
    "1. Load the audio files using a suitable audio processing library like librosa.\n",
    "2. Pass the audio signals through the pretrained Wave2Vec2 model to obtain the feature embeddings.\n",
    "\n",
    "It is important to note that the choice of the preprocessing model may depend on factors such as the size and complexity of the dataset, the computational resources available, and the specific requirements of the task. By preprocessing the audio files into feature embeddings, you can effectively represent the audio data in a format that can be easily understood and processed by the MLP model. This allows the model to learn and make accurate predictions based on the extracted features.\n",
    "\n",
    "Some options for the pre-processing are:\n",
    "- Word2Vec2\n",
    "- Whisper\n",
    "- HuBERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucas/code/ece4179-neural-networks/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Wav2Vec2Model(\n",
       "  (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "    (conv_layers): ModuleList(\n",
       "      (0): Wav2Vec2GroupNormConvLayer(\n",
       "        (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "        (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "      )\n",
       "      (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (feature_projection): Wav2Vec2FeatureProjection(\n",
       "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): Wav2Vec2Encoder(\n",
       "    (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "      (conv): ParametrizedConv1d(\n",
       "        768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "        (parametrizations): ModuleDict(\n",
       "          (weight): ParametrizationList(\n",
       "            (0): _WeightNorm()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (padding): Wav2Vec2SamePadLayer()\n",
       "      (activation): GELUActivation()\n",
       "    )\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x Wav2Vec2EncoderLayer(\n",
       "        (attention): Wav2Vec2Attention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): Wav2Vec2FeedForward(\n",
       "          (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Wav2Vec2 model and processor\n",
    "wav_model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "wav_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "# put the model on the device\n",
    "wav_model.to(device)  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### Feature extraction from audio files\n",
    "The `process_split` function is responsible for processing the audio files and their corresponding labels for a given split (train, validation, or test) of the dataset. It takes two inputs: the path to the CSV file containing the file names and labels, and the path to the folder containing the audio files. The function returns two outputs: the processed feature vectors (pooled tensors) and their corresponding labels.\n",
    "\n",
    "Here is a high-level explanation of how to write the `process_split` function:\n",
    "\n",
    "1. Read the CSV file using a suitable library (e.g., pandas) to obtain the file names and labels.\n",
    "2. Initialize empty lists to store the processed feature vectors and labels.\n",
    "3. Iterate over each row in the CSV file.\n",
    "4. Construct the full path to the audio file by concatenating the audio folder path and the file name.\n",
    "5. Load the audio file using an audio processing library (e.g., librosa).\n",
    "6. Process the audio input using the desired audio processing model (e.g., Wave2Vec2) to obtain the feature embeddings.\n",
    "7. Store the processed feature vector (pooled tensor) and its corresponding label in the respective lists.\n",
    "8. Convert the lists to numpy arrays.\n",
    "9. Return the processed feature vectors and labels as the output of the function. Use the output of the function to save it to a .npz file format.\n",
    "\n",
    "By following these steps, the `process_split` function will preprocess the audio files and extract meaningful features that can be used as input to machine learning models for speech recognition tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process files based on the CSV file and return pooled tensors and labels\n",
    "def process_split(csv_file, audio_folder):\n",
    "    pooled_tensors = []\n",
    "    labels = []\n",
    "\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "    bar = tqdm(total=len(df))\n",
    "\n",
    "    # Process each row in the CSV file\n",
    "    for _, row in df.iterrows():\n",
    "\n",
    "        file_name = row[\"FileName\"]\n",
    "        label = row[\"Label\"]\n",
    "\n",
    "        # Construct the full path to the audio file\n",
    "        path = os.path.join(audio_folder, file_name)\n",
    "        # Load the audio file using librosa.load\n",
    "        audio = librosa.load(path, sr=16000)[0]\n",
    "        # Process the audio input using the pre-processor that you have defined\n",
    "        audio = wav_processor(\n",
    "            audio, sampling_rate=16000, return_tensors=\"pt\", padding=True\n",
    "        )\n",
    "\n",
    "        # Get the model's output tensor. If you are using PyTorch for the model - you need to make sure you use the \"with torch.no_grad(): \". See below\n",
    "        # This will ensure gradients are not kept in your RAM\n",
    "        with torch.no_grad():\n",
    "            output = wav_model(input_values=audio[\"input_values\"].to(device))\n",
    "\n",
    "        # You can simplify the sequence dimension by applying average pooling across the sequence dimension (dim=1).\n",
    "        # This will simplify result in a tensor of size 1x768\n",
    "        output = torch.mean(output.last_hidden_state, dim=1)\n",
    "\n",
    "        # Store the pooled tensor and its corresponding label\n",
    "        # Flattening to remove the extra dimension (1x768 -> 768)\n",
    "        # Append the pooled_tensor and label\n",
    "        pooled_tensors.append(output.flatten().cpu().numpy())\n",
    "        labels.append(label)\n",
    "        bar.update()\n",
    "\n",
    "    # Convert lists to numpy arrays for pooled_tensor\n",
    "    pooled_tensors = np.array(pooled_tensors)\n",
    "    labels = np.array(labels)\n",
    "    bar.close()\n",
    "\n",
    "    # Return the pooled tensor and labels\n",
    "    return pooled_tensors, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the extracted features and corresponding labels for the train, validation, and test splits, you can apply the `process_split` function on each split individually. Here are the steps to follow:\n",
    "\n",
    "1. Call the `process_split` function for the train split by providing the path to the train CSV file and the audio folder. This will return the processed feature vectors and labels for the train split.\n",
    "\n",
    "2. Repeat the same step for the validation split and test split, providing the respective CSV file and audio folder paths. However, note that for the test split, you won't have the labels available.\n",
    "\n",
    "\n",
    "Please note that in the test split, you won't have the labels available as they are not provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/speech\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18538/18538 [02:07<00:00, 145.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train data to ./data/speech/train_data.npz. Shape: (18538, 768), Labels: (18538,)\n",
      "./data/speech\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2577/2577 [00:17<00:00, 144.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved validation data to ./data/speech/validation_data.npz. Shape: (2577, 768), Labels: (2577,)\n",
      "./data/speech\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2137/2137 [00:14<00:00, 143.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved test data to ./data/speech/test_data.npz. Shape: (2137, 768), Labels: (2137,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Process each split and save the results\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    print(dataset_path)\n",
    "    csv_file = os.path.join(dataset_path, f\"{split}.csv\")\n",
    "    audio_folder = os.path.join(dataset_path, split)\n",
    "\n",
    "    # Process the split folder\n",
    "    X, y = process_split(csv_file, audio_folder)\n",
    "\n",
    "    # Save the pooled tensors and labels into an npz file\n",
    "    npz_file_path = os.path.join(dataset_path, f\"{split}_data.npz\")\n",
    "    np.savez(npz_file_path, X=X, y=y)\n",
    "\n",
    "    print(f\"Saved {split} data to {npz_file_path}. Shape: {X.shape}, Labels: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should look similar to the one below: \n",
    "\n",
    "```\n",
    "Saved train data to train_data.npz. Shape: (18538, 768), Labels: (18538,)\n",
    "Saved validation data to validation_data.npz. Shape: (2577, 768), Labels: (2577,)\n",
    "Saved test data to test_data.npz. Shape: (2567, 768), Labels: (2567,)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### Dataset Class\n",
    "\n",
    "To create the train-validation and test sets, we can define a custom Dataset class (SpeechCommandsDataset) that takes the split name (\"train\", \"validation\", or \"test\") and the extracted features and labels as input. This class will handle the creation of the train-validation and test sets based on the split name.\n",
    "\n",
    "By using this SpeechCommandsDataset class, we can easily create the train-validation and test sets by passing the split name (\"train\", \"validation\", or \"test\") and the extracted features and labels. For the test set, the labels will be None as we don't have labels for the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dataset Class is provided here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechCommandsDataset(Dataset):\n",
    "    def __init__(self, parent_folder=\"\", split=\"train\"):\n",
    "        \"\"\"\n",
    "        Initializes the dataset by loading data from the corresponding npz file.\n",
    "\n",
    "        Args:\n",
    "        - parent_folder (str): Path to the parent folder containing the split npz files.\n",
    "        - split (str): The split of the dataset to load ('train', 'validation', or 'test').\n",
    "        \"\"\"\n",
    "        # Build the path to the corresponding npz file\n",
    "        npz_file = os.path.join(parent_folder, f\"{split}_data.npz\")\n",
    "\n",
    "        # Load the data from the .npz file\n",
    "        data = np.load(npz_file)\n",
    "        self.X = data[\"X\"]  # Feature vectors (e.g., of shape [num_samples, 768])\n",
    "        self.y = data[\"y\"]  # Corresponding labels (e.g., of shape [num_samples])\n",
    "\n",
    "        # Optionally, convert labels to integers if they're strings\n",
    "        unique_labels = sorted(set(self.y))\n",
    "        if split == \"test\":  # This is to take care of empty labels in the test set\n",
    "            self.label_to_index = {}\n",
    "            self.y = np.empty(len(self.X), dtype=np.int64)\n",
    "\n",
    "        else:\n",
    "            self.label_to_index = {\n",
    "                label: idx for idx, label in enumerate(unique_labels)\n",
    "            }\n",
    "            self.y = np.array(\n",
    "                [self.label_to_index[label] for label in self.y], dtype=np.int64\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the sample at index `idx`.\n",
    "\n",
    "        Args:\n",
    "        - idx (int): Index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "        - A tuple (features, label) where:\n",
    "          - features is a tensor of shape [768] representing the pooled feature vector.\n",
    "          - label is an integer representing the corresponding class.\n",
    "        \"\"\"\n",
    "        # Convert features to float tensor\n",
    "        features = torch.tensor(self.X[idx], dtype=torch.float32)\n",
    "\n",
    "        # Convert label to a long tensor (int)\n",
    "        label = torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "        return features, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the MLP classifier on the data, we need to create the train, validation, and test datasets, as well as the corresponding dataloaders. Here are the steps to create and prepare the datasets and dataloaders:\n",
    "\n",
    "1. Create instances of the `SpeechCommandsDataset` class for the train, validation, and test splits. Pass the parent folder path and the split name (\"train\", \"validation\", or \"test\") as arguments.\n",
    "\n",
    "2. Create instances of the `DataLoader` class for the train, validation, and test datasets. Specify the batch size and set the `shuffle` parameter to `True` for the train loader and `False` for the validation and test loaders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "npz_dataset_path = \"data/speech\"\n",
    "\n",
    "# Create dataset instances by specifying the parent folder and split\n",
    "train_dataset = SpeechCommandsDataset(parent_folder=npz_dataset_path, split=\"train\")\n",
    "validation_dataset = SpeechCommandsDataset(\n",
    "    parent_folder=npz_dataset_path, split=\"validation\"\n",
    ")\n",
    "test_dataset = SpeechCommandsDataset(parent_folder=npz_dataset_path, split=\"test\")\n",
    "\n",
    "# Create DataLoader instances\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### Classification Model\n",
    "\n",
    "To classify the extracted features and predict the corresponding command category, we can define a classification model using PyTorch or PyTorch Lightning. \n",
    "\n",
    "You can customize the model architecture according to your specific requirements and experiment with different architectures to improve the classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in your code here to define your model\n",
    "from typing import Sequence\n",
    "\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import pytorch_lightning.loggers as pl_loggers\n",
    "import typing\n",
    "\n",
    "Activations = typing.Literal[\"relu\", \"leaky_relu\", \"gelu\"]\n",
    "activations = typing.get_args(Activations)\n",
    "\n",
    "\n",
    "class SpeechConvBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels: int,\n",
    "        kernel_size: int,\n",
    "        stride: int,\n",
    "        activation: nn.Module,\n",
    "        in_channels=None,\n",
    "    ):\n",
    "        super(SpeechConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv1d(\n",
    "            channels if in_channels is None else in_channels,\n",
    "            channels,\n",
    "            kernel_size,\n",
    "            stride,\n",
    "            (kernel_size - 1) // 2,\n",
    "        )\n",
    "        # self.norm = nn.LayerNorm(channels)\n",
    "        self.activation = activation\n",
    "        self.pool = nn.MaxPool1d(2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SpeechFCBlock(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, activation: nn.Module):\n",
    "        super(SpeechFCBlock, self).__init__()\n",
    "        self.fc = nn.Linear(in_features, out_features)\n",
    "        self.activation = activation\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SpeechModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        num_classes,\n",
    "        lr=1e-3,\n",
    "        activation: Activations = \"relu\",\n",
    "    ):\n",
    "        super(SpeechModel, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        if activation == \"relu\":\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == \"leaky_relu\":\n",
    "            self.activation = nn.LeakyReLU()\n",
    "        elif activation == \"gelu\":\n",
    "            self.activation = nn.GELU()\n",
    "\n",
    "        self.block1 = SpeechConvBlock(32, 7, 1, self.activation, 1)\n",
    "        self.block2 = SpeechConvBlock(64, 5, 1, self.activation, 32)\n",
    "        self.block3 = SpeechConvBlock(64, 3, 1, self.activation)\n",
    "        self.block4 = SpeechConvBlock(128, 3, 1, self.activation, 64)\n",
    "        self.block5 = SpeechConvBlock(256, 3, 1, self.activation, 128)\n",
    "        self.fc1 = SpeechFCBlock(6144, 1024, self.activation)\n",
    "        self.fc2 = SpeechFCBlock(1024, num_classes, self.activation)\n",
    "\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.train_acc = Accuracy(\n",
    "            task=\"multiclass\", average=\"micro\", num_classes=num_classes\n",
    "        )\n",
    "        self.val_acc = Accuracy(\n",
    "            task=\"multiclass\", average=\"micro\", num_classes=num_classes\n",
    "        )\n",
    "        self.test_acc = Accuracy(\n",
    "            task=\"multiclass\", average=\"micro\", num_classes=num_classes\n",
    "        )\n",
    "\n",
    "    def on_train_start(self) -> None:\n",
    "        for l in self.loggers:\n",
    "            if isinstance(l, pl_loggers.TensorBoardLogger):\n",
    "                l.log_hyperparams(\n",
    "                    self.hparams_initial,\n",
    "                    {\n",
    "                        \"train_acc\": 0,\n",
    "                        \"val_acc\": 0,\n",
    "                        \"test_acc\": 0,\n",
    "                        \"train_loss\": 0,\n",
    "                        \"val_loss\": 0,\n",
    "                        \"test_loss\": 0,\n",
    "                    },\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define the forward pass\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Define the training step\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "\n",
    "        log = {\n",
    "            \"train_loss\": loss,\n",
    "            \"train_acc\": self.train_acc(y_hat, y),\n",
    "        }\n",
    "\n",
    "        self.log_dict(log, prog_bar=True, on_epoch=True, on_step=False)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Define the training step\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "\n",
    "        log = {\n",
    "            \"val_loss\": loss,\n",
    "            \"val_acc\": self.val_acc(y_hat, y),\n",
    "        }\n",
    "\n",
    "        self.log_dict(log, prog_bar=True, on_epoch=True, on_step=False)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Define the training step\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "\n",
    "        log = {\n",
    "            \"test_loss\": loss,\n",
    "            \"test_acc\": self.test_acc(y_hat, y),\n",
    "        }\n",
    "\n",
    "        self.log_dict(log, prog_bar=True, on_epoch=True, on_step=False)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), self.hparams_initial[\"lr\"])\n",
    "\n",
    "\n",
    "# chuck this here for fast retraining\n",
    "# from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# from pytorch_lightning.loggers import TensorBoardLogger, CSVLogger\n",
    "\n",
    "\n",
    "# def train_model(epochs, lr):\n",
    "#     model = SpeechModel(input_dim=768, num_classes=10, lr=lr)\n",
    "#     name = f\"model-{lr}\"\n",
    "#     checkpoint_callback = ModelCheckpoint(\n",
    "#         dirpath=\"checkpoints_task5\",\n",
    "#         filename=name,\n",
    "#         save_top_k=1,\n",
    "#         monitor=\"val_loss\",\n",
    "#         mode=\"min\",\n",
    "#     )\n",
    "#     early_stop = EarlyStopping(monitor=\"val_loss\", patience=10, mode=\"min\")\n",
    "#     tensorboard = TensorBoardLogger(\n",
    "#         save_dir=\"logs_task5\",\n",
    "#         name=name,\n",
    "#         default_hp_metric=False,\n",
    "#     )\n",
    "#     csv = CSVLogger(save_dir=\"logs_task5\", name=name)\n",
    "#     trainer = pl.Trainer(\n",
    "#         accelerator=\"auto\",\n",
    "#         max_epochs=epochs,\n",
    "#         callbacks=[checkpoint_callback, early_stop],\n",
    "#         logger=[tensorboard, csv],\n",
    "#     )\n",
    "#     trainer.fit(model, train_loader, validation_loader)\n",
    "#     trainer.test(model, test_loader)\n",
    "\n",
    "\n",
    "# train_model(100, lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### Training and evaluating the classifier\n",
    "\n",
    "Train and evaluate your classifier on several hyperparameters. Experiment with different hyperparameter combinations and model architectures to find the best performing model. Use ModelCheckpoint from the torch library and you can use the torch.load() function to load in the best model. Set appropriate parameters and good luck with the competition!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code here to train and evaluate the classifier\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger, CSVLogger\n",
    "\n",
    "\n",
    "def train_model(epochs, lr, activation):\n",
    "    model = SpeechModel(input_dim=768, num_classes=10, lr=lr, activation=activation)\n",
    "    name = f\"model-{lr}-{activation}\"\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=\"checkpoints_task5\",\n",
    "        filename=name,\n",
    "        save_top_k=1,\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "    )\n",
    "    early_stop = EarlyStopping(monitor=\"val_loss\", patience=10, mode=\"min\")\n",
    "    tensorboard = TensorBoardLogger(\n",
    "        save_dir=\"logs_task5\",\n",
    "        name=name,\n",
    "        default_hp_metric=False,\n",
    "    )\n",
    "    csv = CSVLogger(save_dir=\"logs_task5\", name=name)\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator=\"auto\",\n",
    "        max_epochs=epochs,\n",
    "        callbacks=[checkpoint_callback, early_stop],\n",
    "        logger=[tensorboard, csv],\n",
    "    )\n",
    "    trainer.fit(model, train_loader, validation_loader)\n",
    "    trainer.test(\n",
    "        model, validation_loader\n",
    "    )  # use validation_loader for testing (for hparam sweeping) since test loader doesn't have labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%tensorboard --logdir logs_task5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with progress bars\n",
    "for lr in [1e-4, 5e-4, 1e-5]:\n",
    "    for a in activations:\n",
    "        train_model(200, lr, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/lucas/code/ece4179-neural-networks/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9996016c390c41c18ec76d05d289024c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.9329957365989685\n",
      "        test_loss           0.23113633692264557\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.23113633692264557, 'test_acc': 0.9329957365989685}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "model = SpeechModel.load_from_checkpoint(\"checkpoints_task5/model-0.0001-v1.ckpt\")\n",
    "trainer = pl.Trainer()\n",
    "trainer.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_acc</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.126065</td>\n",
       "      <td>2.296311</td>\n",
       "      <td>0.305782</td>\n",
       "      <td>2.263978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385209</td>\n",
       "      <td>1.971353</td>\n",
       "      <td>0.667443</td>\n",
       "      <td>1.289849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.727425</td>\n",
       "      <td>0.948118</td>\n",
       "      <td>0.854094</td>\n",
       "      <td>0.540409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.804402</td>\n",
       "      <td>0.656160</td>\n",
       "      <td>0.873496</td>\n",
       "      <td>0.450356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.820854</td>\n",
       "      <td>0.602251</td>\n",
       "      <td>0.883586</td>\n",
       "      <td>0.419870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.909375</td>\n",
       "      <td>0.379432</td>\n",
       "      <td>0.925495</td>\n",
       "      <td>0.281088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.907973</td>\n",
       "      <td>0.379851</td>\n",
       "      <td>0.924331</td>\n",
       "      <td>0.282366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.910346</td>\n",
       "      <td>0.380351</td>\n",
       "      <td>0.920450</td>\n",
       "      <td>0.282325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.913691</td>\n",
       "      <td>0.364692</td>\n",
       "      <td>0.922390</td>\n",
       "      <td>0.282093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.095053</td>\n",
       "      <td>8.946985</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       test_acc  test_loss  train_acc  train_loss   val_acc  val_loss\n",
       "epoch                                                                \n",
       "0      0.000000   0.000000   0.126065    2.296311  0.305782  2.263978\n",
       "1      0.000000   0.000000   0.385209    1.971353  0.667443  1.289849\n",
       "2      0.000000   0.000000   0.727425    0.948118  0.854094  0.540409\n",
       "3      0.000000   0.000000   0.804402    0.656160  0.873496  0.450356\n",
       "4      0.000000   0.000000   0.820854    0.602251  0.883586  0.419870\n",
       "...         ...        ...        ...         ...       ...       ...\n",
       "58     0.000000   0.000000   0.909375    0.379432  0.925495  0.281088\n",
       "59     0.000000   0.000000   0.907973    0.379851  0.924331  0.282366\n",
       "60     0.000000   0.000000   0.910346    0.380351  0.920450  0.282325\n",
       "61     0.000000   0.000000   0.913691    0.364692  0.922390  0.282093\n",
       "62     0.095053   8.946985   0.000000    0.000000  0.000000  0.000000\n",
       "\n",
       "[63 rows x 6 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"logs_task5/model-0.0001/version_7/metrics.csv\")\n",
    "data = data.drop(columns=[\"step\"]).groupby(\"epoch\").sum()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "### Task 5: Discussion Questions <a class=\"anchor\" id=\"t5_5\"></a>\n",
    "\n",
    "### Questions\n",
    "1. What is the approach you took to solve the problem?\n",
    "\n",
    "2. Describe your chosen model and hyper-parameter tuning procedure.\n",
    "\n",
    "3. Observations and plots from the evaluation on the test set and your own recordings. Have you managed to improve your model over time?\n",
    "    \n",
    "### Answers\n",
    "    \n",
    "1.\n",
    "\n",
    "3. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### Final Submission to Kaggle \n",
    "To evaluate the performance of your best model on the test set and generate predictions for submission to the Kaggle competition, you can use the `evaluate_model_and_save_prediction` function provided below. You can only submit one time per day for the Kaggle competition and we only take your final CSV submission\n",
    "\n",
    "The function takes three arguments: the trained model (your best model), the dataloader for the test set, and the path to the CSV file containing the test filenames. \n",
    "\n",
    "\n",
    "After running this code, the function will evaluate the model on the test set, generate predictions, and save them to a CSV file. You can then submit the CSV file to the Kaggle competition for evaluation. \n",
    "\n",
    "Please note that the `evaluate_model_and_save_prediction` function assumes that the model has already been trained and is ready for evaluation. It also assumes that the test dataset has been preprocessed and is ready for evaluation. \n",
    "\n",
    "Good luck with your submission to the Kaggle competition!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = SpeechModel.load_from_checkpoint(\n",
    "    \"checkpoints_task5/model-0.0005-leaky_relu.ckpt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(test_loader))\n",
    "x.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DO NOT change any code below, other than your name and ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update your name, and ID here\n",
    "First_name = \"Lucas\"\n",
    "Surname = \"Liu\"\n",
    "Student_ID = \"31445179\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE!\n",
    "def evaluate_model_and_save_predictions(model, loader, device, csv_file):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the provided DataLoader, calculates accuracy using torchmetrics,\n",
    "    and saves predictions to a CSV file.\n",
    "\n",
    "    Args:\n",
    "    - model: The trained model.\n",
    "    - loader: DataLoader for the dataset (test).\n",
    "\n",
    "    - device: Device (CPU or GPU) for evaluation.\n",
    "    - csv_file: Path to the CSV file to save predictions.\n",
    "\n",
    "    Returns:\n",
    "    - Accuracy as a percentage.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, _ in loader:\n",
    "            features = features.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(features)\n",
    "\n",
    "            # Get predicted labels\n",
    "            _, predicted_labels = torch.max(outputs, 1)\n",
    "            predictions.extend(predicted_labels.cpu().numpy())\n",
    "\n",
    "    # Save predictions to CSV\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df[\"Label\"] = predictions\n",
    "    df.to_csv(csv_file, index=False)\n",
    "\n",
    "    # Return accuracy as a percentage\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE!\n",
    "\n",
    "predictions = evaluate_model_and_save_predictions(\n",
    "    mlp_model, test_loader, device, f\"{First_name}_{Surname}_{str(Student_ID)}.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    \n",
    "## Final remarks\n",
    "\n",
    "Well done on making it all the way to the end. We hope you have enjoyed applying deep learning concepts to further your understanding and to new applications ! All the best :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
