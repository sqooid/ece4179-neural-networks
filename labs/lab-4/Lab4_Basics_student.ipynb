{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40a1LtlE1AA4"
   },
   "source": [
    "<font size=\"1\"> *This notebook is best viewed in jupyter lab/notebook. You may also choose to use Google Colab but some parts of the images/colouring will not be rendered properly.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DjMf6aRE1AA_"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "# <p style=\"text-align: center;\">Lab 4 (Weeks 8,9): Convolutional Neural Networks (CNNs)</p>\n",
    "## <p style=\"text-align: center;\">Notebook I: Basics</p>\n",
    "\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:592/1*V6Y8FF2qfw_ztNbs1AHXNg.png\" width=\"800\" height=\"400\" />\n",
    "\n",
    "<!-- ![linear-vs-logistic-regression--medium](https://miro.medium.com/max/1400/1*dm6ZaX5fuSmuVvM4Ds-vcg.jpeg) -->\n",
    "\n",
    "Welcome to the fourth and <b>final</b> lab! As usual, this lab will span over two weeks.\n",
    "            \n",
    "In this Lab, you will find four tasks distributed across two Jupyter Notebooks: *Lab4_Basics_student.ipynb* and *Lab4_Applications_student.ipynb*. The first two tasks guide you to train a basic CNN using pytorch lightning. In the last task, you will apply the knowledge you gained to solve a practical problem.\n",
    "    \n",
    "- <b>Task 1:</b> Design a CNN and train to classify STL-10 images\n",
    "- <b>Task 2:</b> Analyse the results on the STL-10 test images\n",
    "- <b>Task 3:</b> Design and train a CNN by yourself on the FER-4 dataset and analyse results\n",
    "      \n",
    "Each task will contain code to complete, and a worded question, so ensure you complete both before submitting. Feel free to add your own additional comments.\n",
    "    \n",
    "After completion, You need to submit both Jupyter Notebooks (.ipynb files) to Moodle. Make sure all the outputs are visible before submitting.\n",
    "    \n",
    "Good luck with the final Lab! Submit it before the <b>deadline</b> to enjoy full marks.\n",
    "\n",
    "__Submission details:__\n",
    "- __Make sure you have run all your cells from top to bottom (you can click _Kernel_ and _Restart Kernel and Run All Cells_).__ </br>\n",
    "- __Submit the Jupyter Notebooks (Lab4_Basics.ipynb_) and (Lab4_Applications.ipynb_).__\n",
    "- __Outputs must be visible upon submission. We will also be re-running your code__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F3OKXhPs1ABA"
   },
   "source": [
    "<img src=\"https://miro.medium.com/v2/resize:fit:592/1*V6Y8FF2qfw_ztNbs1AHXNg.png\" width=\"1000\" height=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wtB7sNp51ABB"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Welcome to the fourth and <b>final</b> lab! As usual, this lab will span over two weeks.\n",
    "            \n",
    "In this Lab, you will find four tasks distributed across two Jupyter Notebooks: *Lab4_Basics_student.ipynb* and *Lab4_Applications_student.ipynb*. The first two tasks guide you to train a basic CNN using pytorch lightning. In the last task, you will apply the knowledge you gained to solve a practical problem.\n",
    "    \n",
    "- <b>Task 1:</b> Design a CNN and train to classify STL-10 images\n",
    "- <b>Task 2:</b> Analyse the results on the STL-10 test images\n",
    "- <b>Task 3:</b> Design and train a CNN by yourself on the FER-4 dataset and analyse results\n",
    "      \n",
    "Each task will contain code to complete, and a worded question, so ensure you complete both before submitting. Feel free to add your own additional comments.\n",
    "    \n",
    "After completion, You need to submit both Jupyter Notebooks (.ipynb files) to Moodle. Make sure all the outputs are visible before submitting.\n",
    "    \n",
    "Good luck with the final Lab! Submit it before the <b>deadline</b> to enjoy full marks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "px9mfXUr1ABC"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "## Learning Objective\n",
    "    \n",
    "In the previous lab, you focused on classification tasks using MLPs. In this lab, you will focus on image classification using CNNs. There are differences in how MLPs and CNNs operates. You will also notice that many of the steps are identical, except for some small differences such as your class defining your network, and the way you pre-process and post-process the data. This is the beauty of PyTorch and more generally, modular differentiable programming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lYya21UN1ABD"
   },
   "source": [
    "<b>Enter you student details below</b>\n",
    "\n",
    "- <b>Student Name:</b> Firstname Lastname\n",
    "- <b>Student ID:</b> 123456789   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kv4wBVkD1ABD"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "## Table of Contents\n",
    "    \n",
    "* [Libraries](#Libraries)\n",
    "\n",
    "    \n",
    "* [Task 1: Design a CNN and train to classify STL-10 images](#task_1)  \n",
    "    * [1.1 Create Pytorch datasets and dataloaders for STL-10](#t1_1)\n",
    "    * [1.2 Visualize the dataset](#t1_2)\n",
    "    * [1.3 Design the CNN and Lightning Modules](#t1_3)  \n",
    "    * [1.4 Train and Evaluate the CNN](#t1_4)\n",
    "    * [1.5 Visualize training and validation loss/accuracy](#t1_5)   \n",
    "    * [Discussion Questions](#t1_6)\n",
    "    \n",
    "    \n",
    "* [Task 2: Analyse the results on the STL-10 test images](#task_2)  \n",
    "    * [2.1 Visualize predictions](#t2_1)\n",
    "    * [2.2 Visualize Top Classified/Misclassified](#t2_2)\n",
    "    * [2.3 Confusion Matrix](#t2_3)\n",
    "    * [2.4 Visualize Feature maps](#t2_4)\n",
    "    * [2.5 Visualize Saliency maps](#t2_5)\n",
    "    * [Discussion Questions](#t2_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PlBGi6Bt1ABE"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "# Libraries\n",
    "\n",
    "In this lab, you will use several pytorch and lightning libraries along with several other basic python libraries. All the libraries that you need are given below.\n",
    "\n",
    "## Note for lab 4!\n",
    "We have migrated lab 4 to \"lightning\" instead of \"pytorch_lightning\". The only difference is the way you import the lightning modules. All functionalities are identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YLdtI4-T1ABF",
    "outputId": "0700ba0a-b5fe-4547-b087-9ff9bc26342e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.nn.functional import log_softmax, softmax\n",
    "import torchvision\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision import transforms\n",
    "import torchmetrics\n",
    "from torchmetrics import ConfusionMatrix\n",
    "from torchsummary import summary\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch lightning is a wrapper for pytorch that makes it easier to train models.\n",
    "# We will use lightning instead of pytorch-lightning. The functionality is the same.\n",
    "# But the import structure for lightning is slightly different\n",
    "# This is because lightning contains other lightning-based packages and pytorch-lightning is one of them.\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.callbacks.progress.tqdm_progress import TQDMProgressBar\n",
    "from pytorch_lightning.callbacks.progress.rich_progress import (\n",
    "    RichProgressBarTheme,\n",
    "    RichProgressBar,\n",
    ")\n",
    "\n",
    "# Setting seeds for reproducibility\n",
    "pl.seed_everything(4179)\n",
    "random.seed(4179)\n",
    "np.random.seed(4179)\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"True\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fpsbP-lf1ABJ"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "# STL-10 dataset <a class=\"anchor\" id=\"python-basics\"></a>\n",
    "    \n",
    "For all the work in this notebook, you will be using the [STL-10 dataset](https://cs.stanford.edu/~acoates/stl10/).\n",
    "    \n",
    "<img src=\"https://cs.stanford.edu/~acoates/stl10/images.png\" width=\"500\" height=\"500\" />\n",
    "    \n",
    "The STL-10 dataset is a widely used benchmark in machine learning, containing images of 10 distinct object classes: airplane, bird, car, cat, deer, dog, horse, monkey, ship', truck. Each image is of size 96x96 pixels, enabling efficient processing. The train set contains 5000 images whereas the validation and test set contains 8000 images in total. STL-10 serves as a challenging testbed for image classification algorithms, encouraging the development of robust and accurate models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gyBy0b1x1ABK"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "# Task 1 - Design a CNN and train to classify STL-10 images <a class=\"anchor\" id=\"task_1\"></a>\n",
    "        \n",
    "In Task 1, you will design a CNN for image classification task and train it on the STL-10 dataset. You will use PyTorch's inbuilt datasets class, and Lightning's module class to construct a CNN in order to perform training on the STL-10 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bJ9sD3u61ABK"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## 1.1 Create Pytorch datasets and dataloaders for STL-10 <a class=\"anchor\" id=\"t1_1\"></a>\n",
    "     \n",
    "The original STL-10 dataset contains 5000 train images and 8000 test images. In this task, we will take out 1000 images from the test set as validation images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kE75Nao61ABL",
    "outputId": "873e8af2-11ca-46cb-c763-c969d306d8c9"
   },
   "outputs": [],
   "source": [
    "class_names = [\n",
    "    \"airplane\",\n",
    "    \"bird\",\n",
    "    \"car\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"horse\",\n",
    "    \"monkey\",\n",
    "    \"ship\",\n",
    "    \"truck\",\n",
    "]\n",
    "num_classes = len(class_names)\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHC0r4N61ABL"
   },
   "source": [
    "### Transforms & Data Augmentation\n",
    "\n",
    "In this lab, we are also introducing transforms to the image datasets. This concept is covered in the workshops, and [here](https://pytorch.org/vision/stable/transforms.html) is the pyTorch documentaion about transforms.\n",
    "\n",
    "We will give you some guidance on how to implement the transforms in this lab  but keep in mind that this concept and implementation will be critical for the assignment later on. The transforms we will use are mainly (\"ToTensor()\" and \"Normalize()\") which we have used in the workshop. Now in addition to the usual transforms, we will be adding __data augmentation__ as well. The ones we have included are:\n",
    "\n",
    "- RandomHorizontalFlip()\n",
    "- RandomRotation()\n",
    "\n",
    "Now, what is **data augmentation**? Data augmentation in deep learning refers to the process of artificially diversifying a dataset by applying various transformations and modifications to the existing training examples. The objective of data augmentation is to enhance the model's generalization capability and robustness by exposing it to a wider range of variations and scenarios that might be encountered during real-world inference. These transformations can include rotations, flips, translations, changes in lighting and contrast, cropping, and more, depending on the nature of the data. By introducing this augmented data during training, the model becomes better equipped to handle novel and previously unseen examples, effectively reducing overfitting and improving its ability to extract meaningful features from noisy or imperfect inputs. Data augmentation has become a fundamental technique in deep learning, contributing significantly to the improved performance of neural networks across various tasks, from image and speech recognition to natural language processing.\n",
    "\n",
    "Now, why do we only apply augmentation to the training set? Data augmentation is specifically applied to the training set and not the test set because the purpose of data augmentation is to introduce diversity and variation into the training examples, helping the model to generalize better to real-world scenarios. The test set serves as an unbiased evaluation of the model's performance on unseen data. If data augmentation were applied to the test set, it could lead to overly optimistic performance estimates, as the model would have already seen similar augmented examples during training, potentially inflating its apparent accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zv9EjyST1ABL"
   },
   "outputs": [],
   "source": [
    "# Define a series of transformations to be applied to training data\n",
    "\n",
    "# hint: Use Compose function that allows you to combine multiple image transformations into a sequential pipeline. \n",
    "#       Each transformation will be applied in the order they appear within the list.\n",
    "train_transforms = transforms.Compose([\n",
    "    ???? # Randomly flip images horizontally (left to right)\n",
    "    ???? # Apply a random rotation up to 10 degrees to each image\n",
    "    ???? # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize(0.5, 0.5),     # Normalize pixel values to have a mean and standard deviation of 0.5\n",
    "])\n",
    "\n",
    "# Define a series of image transformations to be applied to validation and test data\n",
    "val_test_transforms = transforms.Compose([\n",
    "    ????  # Convert the image to a PyTorch tensor\n",
    "    ???? # Normalize pixel values to have a mean and standard deviation of 0.5\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXsoj9OM1ABM"
   },
   "source": [
    "### Datasets\n",
    "\n",
    "Torchvision provides many built-in datasets in the `torchvision.datasets` module, as well as utility classes for building your own datasets.\n",
    "\n",
    "All datasets are subclasses of `torch.utils.data.Dataset` i.e, they have **\\__getitem\\__** and **\\__len\\__** methods implemented. Hence, they can all be passed to a `torch.utils.data.DataLoader` which can load multiple samples in parallel using torch.multiprocessing workers. \n",
    "\n",
    "Here's an example of creating a training dataset using the built-in datasets class from torchvision:\n",
    "\n",
    "`train_imagenet_data = torchvision.datasets.ImageNet(root='path/to/imagenet_root/', download=True, split='train')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kEZWPZA61ABM",
    "outputId": "2cd7fe30-594f-44d7-e385-c32e3fff63a3"
   },
   "outputs": [],
   "source": [
    "# Create your datasets\n",
    "\n",
    "# Define the transformations to be applied to the training and test datasets\n",
    "# Torchvision datasets contains the STL10 database already, so we can download it directly. \n",
    "# Once downloaded, the datasets will load from your local device.\n",
    "trainset = ????\n",
    "\n",
    "# Create the test dataset with specified transformations\n",
    "testset = ????\n",
    "\n",
    "# Randomly split the testset into validation and test subsets of 1000 and 7000 respectively\n",
    "# You can achieve this by using build in functions in torch.utils.data\n",
    "valset, testset = ????\n",
    "\n",
    "# Print the number of images in each dataset\n",
    "print('Image count for each set\\n------------------------')\n",
    "print(f'trainset\\t: {len(trainset)}')  # Number of images in the training set\n",
    "print(f'valset  \\t: {len(valset)}')    # Number of images in the validation set\n",
    "print(f'testset \\t: {len(testset)}')   # Number of images in the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZDVFOLz1ABN"
   },
   "source": [
    "### Dataloaders\n",
    "\n",
    "Now that you have created the datasets, the next step is to create data loaders. This process is straightforward, and you are already familiar with it from Lab 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AKj_lyen1ABN",
    "outputId": "ac76fe73-5e02-4cf4-b0ca-f473c90544cb"
   },
   "outputs": [],
   "source": [
    "# Define the batch size of 128 for data loaders\n",
    "BATCH_SIZE = ???\n",
    "\n",
    "# Create data loaders for the training, validation, and test sets\n",
    "trainloader = ???  # Training data loader with shuffling\n",
    "valloader = ???     # Validation data loader without shuffling\n",
    "testloader = ???   # Test data loader without shuffling\n",
    "\n",
    "# Print the shape of batches for each data loader\n",
    "images, labels = next(iter(trainloader))\n",
    "print(f'trainloader\\t: {images.shape}')\n",
    "images, labels = next(iter(valloader))\n",
    "print(f'valloader  \\t: {images.shape}')\n",
    "images, labels = next(iter(testloader))\n",
    "print(f'testloader \\t: {images.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "soi5U44_1ABN"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## 1.2 Visualize the dataset <a class=\"anchor\" id=\"t1_2\"></a>\n",
    "\n",
    "In this task, you will write a fucntion which takes a data loader and the list of class names as input and visualize 5 images in the batch. Then use it to visualize train, val and test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c_34v2og1ABO"
   },
   "outputs": [],
   "source": [
    "# Write visualize_dataloader function to visualize images from a given data loader\n",
    "# Note: Use titles to denote class label in words.\n",
    "\n",
    "# Define a function to visualize images from a data loader\n",
    "# Input parameters: dataloader and class_names\n",
    "def visualize_dataloader(dataloader, class_names):\n",
    "\n",
    "    # Create a figure with subplots to display images\n",
    "    ???\n",
    "\n",
    "    # Get a batch of images and labels from the data loader\n",
    "    ???\n",
    "\n",
    "    # Loop through the first 5 images in the batch\n",
    "    for i in range(5):\n",
    "        image = images[i].permute(1, 2, 0) * 0.5 + 0.5  # Un-normalize the image\n",
    "        label = labels[i]  # Get the label for the image\n",
    "\n",
    "        # Display the image in the subplot\n",
    "        ???\n",
    "\n",
    "    # Show the plot with the images and labels\n",
    "    ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YwwHL6631ABO",
    "outputId": "088143c7-6712-4cd2-f29a-77335252e2c8"
   },
   "outputs": [],
   "source": [
    "# visulaize images from the trainloader\n",
    "visualize_dataloader(trainloader, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TA0BUPho1ABR",
    "outputId": "ae112ac6-2f65-44b9-d464-9d30a9157fa6"
   },
   "outputs": [],
   "source": [
    "# visulaize images from the valloader\n",
    "visualize_dataloader(valloader, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xtEoyxq61ABS",
    "outputId": "cfcf2eba-812f-4716-9176-f26aed380027"
   },
   "outputs": [],
   "source": [
    "# visulaize images from the testloader\n",
    "visualize_dataloader(testloader, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x9ia8d_R1ABS"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## 1.3 Design the CNN and Lightning Modules <a class=\"anchor\" id=\"t1_3\"></a>\n",
    "\n",
    "You will design your CNN architecture using torch.nn module inside the *init* function. Then you will use the *forward* function to set the forward pass logic. The model architecture needs to contain the following:\n",
    "    \n",
    "| Layer Name |    Layer Type     | channels | kernel size | stride | padding |  Input size  |  Output size |\n",
    "|:----------:|:---------------:  |----------|-------------|--------|---------|:------------:|:------------:|\n",
    "|    conv1   |      Conv2D       | 16       | (3,3)       | 1      | 1       |  (3, 96, 96) | (16, 96, 96) |\n",
    "| relu       |       ReLU        |          |             |        |         | (16, 96, 96) | (16, 96, 96) |\n",
    "|    pool    |     MaxPool2d     |          | (2,2)       | 2      | 0       | (16, 96, 96) | (16, 48, 48) |            \n",
    "|    conv2   |      Conv2D       | 32       | (3,3)       | 1      | 1       | (16, 48, 48) | (32, 48, 48) |    \n",
    "| relu       |       ReLU        |          |             |        |         | (32, 48, 48) | (32, 48, 48) |\n",
    "|    pool    |     MaxPool2d     |          | (2,2)       | 2      | 0       | (32, 48, 48) | (32, 24, 24) |              \n",
    "|    conv3   |      Conv2D       | 64       | (3,3)       | 1      | 1       | (32, 24, 24) | (64, 24, 24) |   \n",
    "| relu       |       ReLU        |          |             |        |         | (64, 24, 24) | (64, 24, 24) |\n",
    "|    pool    |     MaxPool2d     |          | (2,2)       | 2      | 0       | (64, 24, 24) | (64, 12, 12) |             \n",
    "| gap        | AdaptiveAvgPool2d |          |             |        |         | (64, 12, 12) | (64, 12, 12) |\n",
    "| relu       |       ReLU        |          |             |        |         | (64, 12, 12) | (64, 12, 12) |\n",
    "| flatten    | Flatten           |          |             |        |         | (64, 12, 12) |    (9216)    |\n",
    "| fc         | Linear            | 10       |             |        |         |  (9216)      |     (10)     |\n",
    "   \n",
    "Apart from the model design we will construct the following pytorch lightning modules.\n",
    "\n",
    "- *training_step*: Define logic for training step\n",
    "- *validation_step*: Define logic for validation step\n",
    "- *test_step*: Define logic for test step\n",
    "- *predict_step*: Define logic for inference/prediction step\n",
    "- *configure_optimizers*: Configure optimizers and schedulers\n",
    "\n",
    "Other hyperparameters:\n",
    "- Use cross-entropy loss\n",
    "- Use the ADAM optimizer with lr = 0.001\n",
    "- Epochs = 50\n",
    "\n",
    "_Notes:_ \n",
    "\n",
    "- `AdaptiveAvgPool2d` will automatically calculate the required parameters to achieve the output dimension so you only need to specify an output size (number of channels will be reflected automatically). More information can be found here: https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CigHxjT01ABT"
   },
   "source": [
    "### Design the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iILF4Nkz1ABT"
   },
   "outputs": [],
   "source": [
    "class MyCNN(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # You may or may not want to save the hyperparameters\n",
    "\n",
    "        # Define Neural Network layers\n",
    "        # Create the 1st Convolutional Layer:\n",
    "        # Input channels: 3, Output channels: 16, Kernel size: 3x3, Stride: 1, Padding: 1\n",
    "        ???\n",
    "\n",
    "        # Create the 2nd Convolutional Layer:\n",
    "        # Input channels: 16, Output channels: 32, Kernel size: 3x3, Stride: 1, Padding: 1\n",
    "        ???\n",
    "\n",
    "        # Create the 3rd Convolutional Layer:\n",
    "        # Input channels: 32, Output channels: 64, Kernel size: 3x3, Stride: 1, Padding: 1\n",
    "        ???\n",
    "\n",
    "        # Create ReLU Activation Layer\n",
    "        ???\n",
    "\n",
    "        # Create a MaxPool layer with the required kernel size and stride\n",
    "        ???\n",
    "        \n",
    "        # Create Global Adaptive Average Pooling Layers:\n",
    "        # Use nn.AdaptiveAvgPool2d\n",
    "        # GAP - Layer : Output size 12x12\n",
    "        ???\n",
    "        \n",
    "        # Create Fully Connected Layers:\n",
    "        # FC - Layer : Input size 64*12*12, Output size num_classes=10\n",
    "        ???\n",
    "\n",
    "        # Define loss function\n",
    "        ???\n",
    "\n",
    "        # Define metrics for Train, Val and Test\n",
    "        self.train_accuracy = ????\n",
    "        self.val_accuracy = ????\n",
    "        self.test_accuracy = ????\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Define Forward pass logic\n",
    "        # Conv1 -> ReLU -> MaxPool2d\n",
    "        ???\n",
    "\n",
    "        # Conv2 -> ReLU -> MaxPool2d\n",
    "        ???\n",
    "\n",
    "        # Conv3 -> ReLU -> MaxPool2d\n",
    "        ???\n",
    "\n",
    "        # AdaptiveAvgPool2d(12x12) -> ReLU \n",
    "        ???\n",
    "        \n",
    "        # Reshape the Tensor to 1 dimension so (Batch_size, everything else):\n",
    "        ???\n",
    "\n",
    "        # Apply the Fully Connected Layer (Output Layer):\n",
    "        # The final output x represents the predictions of the neural network.\n",
    "        ???\n",
    "        \n",
    "        return ??? # Return the output\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Define logic for training step\n",
    "        ???\n",
    "\n",
    "        return ???\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Define logic for validation step\n",
    "        ???\n",
    "\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Define logic for test step\n",
    "        ???\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        ???\n",
    "\n",
    "        return ??? # Return predicted class, actual class, and input \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Configure optimizers and schedulers\n",
    "        optimizer = ???\n",
    "        \n",
    "        return optimizer\n",
    "\n",
    "    ####################\n",
    "    # DATA RELATED HOOKS\n",
    "    ####################\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return trainloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return valloader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return testloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6v5etBCu1ABV"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## 1.4 Train and Evaluate the CNN <a class=\"anchor\" id=\"t1_4\"></a>\n",
    "    \n",
    " Here you will train the model that you have already constructed. To start with, you may visualzue the CNN to make sure that everything is in place. Then you will define a checkpoint callback function in order to save the weights of model at the best performing epoch. Finally you will train your model for a predefined number of epochs. Once trained, you may load the saved weights of the model and verify your model performance on the test set. Let's see whether you can achieve a test accuracy over 80%.\n",
    "\n",
    " _Note_: You can load the model checkpoint similar to how we loaded the CNN from lab 3. You need to define an object with the same class definition, then load in the weights via the dictionary. This way, once you have finalised your model in task 1, you do not need to re-train it in task 2 (as you will use the same model to perform analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwlYiE_a1ABV"
   },
   "source": [
    "### Initialize the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KPY17R3p1ABV",
    "outputId": "41554bce-e1e0-4b76-e1e1-574104e83bae"
   },
   "outputs": [],
   "source": [
    "# Initialize the model by creating instance from Model class\n",
    "task1_model = ???\n",
    "\n",
    "# Display a summary of the model's architecture\n",
    "summary(task1_model.to('cuda'), (3, 96,96)) # delete .to('cuda') if not using cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKMBACpo1ABW"
   },
   "source": [
    "### Define progress bar and checkpoint callback functions to save the model at the best epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gk4Uu4Vo1ABW"
   },
   "outputs": [],
   "source": [
    "# Create a checkpoint callback function to save the best model based on validation accuracy\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_acc\",  # Monitor the validation accuracy for saving checkpoints\n",
    "    save_top_k=1,  # Save the top 1 best model based on validation accuracy\n",
    "    mode=\"max\",  # Choose the mode 'max' to maximize the monitored metric (accuracy)\n",
    "    every_n_epochs=1,  # Save a checkpoint every epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JSrTv8t1ABX"
   },
   "source": [
    "### Train the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WQsNadKv1ABX",
    "outputId": "51be2cb8-5099-4523-bb94-2183864265a9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define the trainer with specified configurations\n",
    "task1_trainer = ??? # Call pl.Trainer and put in the relevant arguments\n",
    "\n",
    "# Fit the model using the defined trainer\n",
    "# You may comment this line if you do not wish to train the model but use a previous checkpoint\n",
    "task1_trainer.fit(task1_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "# Checkpoints and model's state_dict\n",
    "\n",
    "In PyTorch, a model's state_dict is a Python dictionary object that contains the learnable parameters (model weights) and their associated values for a neural network model. It serves as a convenient way to save and load model weights, enabling model persistence and transferability across different sessions and tasks.\n",
    "\n",
    "Once you have trained your CNN by running the above cell, you will notice a `/logs` folder appearing in your working directory. Within this folder you will find model checkpoints that contains the model's state_dict. Also it will contain a csv file which contains train/val loss/accuracy information. Explore this folder on your own and learn how pytorch lightning saves this information. \n",
    "    \n",
    "Also note how different versions of your implementations are saved. This feature in pytorch lightning enables you to access information of previously trained models easily. You can easily know the version of the current training run by observing the progress bar during training as below:\n",
    "\n",
    "<img src=\"./images/progress-bar.png\"/>\n",
    "    \n",
    "You might have to access that specific version in your file directory to obtain the name of the checkpoint in order to access the relevant state_dict for your CNN as the examples given below:\n",
    "    \n",
    "`checkpoint_task1 = torch.load('logs/task1/lightning_logs/version_18/checkpoints/epoch=41-step=1680.ckpt')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model's state_dict\n",
    "\n",
    "When you load the model's state dictionary, you have to adjust the version of the model that you want to load. The one below is just an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_task1 = torch.load(\n",
    "    \"logs/task1/lightning_logs/version_18/checkpoints/epoch=41-step=1680.ckpt\"\n",
    ")\n",
    "print(\n",
    "    checkpoint_task1.keys()\n",
    ")  # print out the keys of the checkpoint to see what information it contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict_task1 = checkpoint_task1[\"state_dict\"]\n",
    "task1_model.load_state_dict(state_dict_task1)\n",
    "\n",
    "# If your state_dic is loaded successfully, you will get a message like below:\n",
    "# \"<All keys matched successfully>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EaTTlaRn1ABX"
   },
   "source": [
    "### Test the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now call test from trainer\n",
    "task1_trainer.test(task1_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your implementation is correct, you should obtain a printed output similar to this:\n",
    "\n",
    "Note: You may not get exactly the same numbers\n",
    "\n",
    "[{'test_loss': 1.5672231912612915, 'test_acc': 0.6288571357727051}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Uaohhjc1ABY"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## 1.5 Visualize training and validation loss/accuracy <a class=\"anchor\" id=\"t1_5\"></a>\n",
    "\n",
    "The PyTorch-Lightning module has its own built-in methods to log values. These log files can be then used to visualize experimental results. You can use python plotting libraries such as matplotlib. You can read the logs by using the pandas library (pd.read_csv) as the example given below: (Make sure to access the correct version number)\n",
    "\n",
    "`metrics_task_1 = pd.read_csv(\"logs/task1/lightning_logs/version_18/metrics.csv\")`\n",
    "\n",
    "Using those logs, generate two plots.\n",
    "    \n",
    "1. train and validation losses against epoch.\n",
    "2. train and validation accuracies against epoch.\n",
    "    \n",
    "Note: Visualizing the loss and accuracy curves will give you an understanding of whether your model has been trained/tested properly. If there are any strange behaviours in your loss and accuracy curves, it means either there may have been a mistake in your code or your implementation could be further improved. Based on your observations, you may want to go back and rectify all the above steps until you obtain well-behaved loss and accuracy curves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjlqnXWS1ABY"
   },
   "source": [
    "### Read logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EDffu7BS1ABZ"
   },
   "outputs": [],
   "source": [
    "# Read the metrics from the CSV file from the relevant version path\n",
    "metrics_task_1 = ????\n",
    "\n",
    "# Set the \"epoch\" column as the index for easier data manipulation\n",
    "metrics_task_1.set_index(\"epoch\", inplace=True)\n",
    "\n",
    "# Group the metrics by epoch and sum them while dropping the \"step\" column\n",
    "metrics_task_1 = metrics_task_1.groupby(level=0).sum().drop(\"step\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PU5YVMqK1ABl"
   },
   "source": [
    "### Plot train and validation losses against epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sACzWLFc1ABm",
    "outputId": "2c7a97f7-e381-4e14-c356-24d17c7ae9fb"
   },
   "outputs": [],
   "source": [
    "# Create a figure with all the appropriate labels/titles etc.\n",
    "\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6EIzpQy61ABn"
   },
   "source": [
    "### Plot train and validation accuracies against epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TotHBAh41ABn",
    "outputId": "b55a3b47-d604-4330-e9bf-e08571fdde8c"
   },
   "outputs": [],
   "source": [
    "# Create a figure with all the appropriate labels/titles etc.\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xkTyl-f1ABo"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "## Discussion Questions <a class=\"anchor\" id=\"t1_6\"></a>\n",
    "    \n",
    "#### What is the purpose of saving the model checkpoint based on validation accuracy instead of training accuracy?\n",
    "    \n",
    "    \n",
    "Answer: \n",
    "\n",
    "    \n",
    "#### Comment on your train / val loss plots and train / val accuracy plots\n",
    "   \n",
    "Answer: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmqXGzvY1ABo"
   },
   "source": [
    "$\\;$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qrc4HdbJ1ABp"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "# Task 2 - Visualization and Analysis of Results <a class=\"anchor\" id=\"task_2\"></a>\n",
    "\n",
    "Now that you have successfully trained a CNN to classify STL-10 data, you will visualize the results in order to see how well your trained model performs in class predictions. You will:\n",
    "\n",
    "- Visualize some predictions\n",
    "- Investigate which images the model shows high and low confidence.  \n",
    "- Generate a confusion matrix and interpret the results\n",
    "- Visualize some intermediate feature maps from your model and see how the convolution filters capture the information within the model.\n",
    "- Understand which regions of the image are more important for the classification by applying saliency maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "477807961f1649b18eb8789ecfcc6b03"
     ]
    },
    "id": "ZnOVa-2k1ABp",
    "outputId": "7ae35a67-efc8-44fe-d5ef-dcd4aa4dfe4c"
   },
   "outputs": [],
   "source": [
    "# Generate predictions using the trained model and the test data loader\n",
    "# Note: Do not do anything to the variable 'predictions' you will reuse it\n",
    "predictions = task1_trainer.predict(task1_model, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IK_NF7nO1ABq"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## 2.1 Visualizing predictions <a class=\"anchor\" id=\"t2_1\"></a>\n",
    "\n",
    "In this task, we want you to plot out 5 input images of the test set along with their respective predicted and ground-truth labels so that we can compare the predictions with the actual labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ylQWNPIg1ABr",
    "outputId": "19da92e8-f292-4f62-cf22-b0fda636f0f0"
   },
   "outputs": [],
   "source": [
    "# Extract predictions, labels, and inputs from the predictions result\n",
    "\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mOwN7GO01ABs"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## 2.2 Top Classified/Misclassified Images <a class=\"anchor\" id=\"t2_2\"></a>\n",
    "\n",
    "\n",
    "In this task, we find the top 5 correctly classified and top 5 misclassified images for each of the classes from the test set and visualize them.\n",
    "\n",
    "- Top 5 correctly  classified - for e.g. take the 'airplane' class. First, find the test images that that were correctly predicted as 'airplane'. Out of those images, find out the predctions that got highest softmax scores and visualize them. Do this for all classes (you might want to use a for loop for this).\n",
    "- Top 5 Misclassified - for e.g. take the 'airplane' class. First, find the test images which were airplanes but were wrongly predicted as a different class. Out of those images, find out the 5 predictions that got highest softmax scores for the wrong class and visualize them. Do this for all classes (you might want to use a for loop for this as well).    \n",
    "    \n",
    "In total, you have 10 classes, and 5 top correctly classified and 5 top misclassified images for each class, hence you will have $10* (5+5)= 100$ images altogether. On top of each image, display its label, prediction, as well the the top softmax score as a percentage.\n",
    "    \n",
    "An example of an expected output for the Top Classified and Top Missclassifed images for the 'car' and 'dog' class are shown below:  \n",
    "    \n",
    "<img src=\"./images/top-classified-demo.PNG\" />\n",
    "\n",
    "<img src=\"./images/top-mis-classified-demo.PNG\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppONmXjJ1ABs"
   },
   "source": [
    "### Get predictions for the test set for later use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g_JYQnDj1ABs"
   },
   "outputs": [],
   "source": [
    "## These are the predictions, actual labels, and original images from the test set. You will use this for the tasks below during the analysis process.\n",
    "\n",
    "\n",
    "# Concatenate the predictions from multiple batches\n",
    "test_outputs = torch.cat([prediction[0] for prediction in predictions], dim=0)  # Concatenate model outputs\n",
    "test_labels = torch.cat([prediction[1] for prediction in predictions], dim=0)  # Concatenate true labels\n",
    "test_inputs = torch.cat([prediction[2] for prediction in predictions], dim=0)  # Concatenate input images\n",
    "\n",
    "# Determine the predicted labels by selecting the class with the highest probability\n",
    "test_preds = ????  # Find the index of the maximum probability along the classes (predicted labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mflr_21b1ABt"
   },
   "source": [
    "### Top Classified\n",
    "\n",
    "Complete your plots here for the top 5 correctly classified images from each class. \n",
    "Make sure you put in the predicted and actual labels for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_zPDiEPk1ABt",
    "outputId": "576f0af4-51d0-49ee-963b-0232f9bedba3",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Loop through each class (category)\n",
    "for c in range(num_classes):\n",
    "    \n",
    "    ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VS1H_gVn1ABu"
   },
   "source": [
    "### Top Misclassified\n",
    "\n",
    "Complete your plots here for the top 5 misclassified images from each class. \n",
    "Make sure you put in the predicted and actual labels for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "giS_kqf81ABu",
    "outputId": "736df512-f608-42e6-f5ef-69b078847c1c",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Loop through each class (category)\n",
    "for c in range(num_classes):\n",
    "    \n",
    "    ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHsum2Bv1ABv"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## 2.3 Confusion Matrix <a class=\"anchor\" id=\"t2_3\"></a>\n",
    "\n",
    "### Introduction to Confusion matrix\n",
    "\n",
    "Now let's do something new!\n",
    "\n",
    "You will be generating a confusion matrix. For a two-class classification, the valued in a confusion matrix is defined as recall, precision values based on confusion matrix outcomes. See the following table for a two-class confusion matrix (in our case, we will have 10 classes):\n",
    "\n",
    "<center><img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*uR09zTlPgIj5PvMYJZScVg.png\" width=\"600\" height=\"300\" /></center>\n",
    "\n",
    "\n",
    "Now, what is a confusion matrix? A confusion matrix is a fundamental tool in the field of machine learning used to assess the performance of a classification model. It provides a comprehensive tabular representation of the model's predictions by categorizing them into four key outcomes: true positives, true negatives, false positives, and false negatives. \n",
    "    \n",
    "True positives indicate the instances where the model correctly predicted a positive class, while true negatives represent correct predictions of the negative class. False positives occur when the model wrongly predicts a positive class as negative, and false negatives signify incorrect predictions of the negative class as positive. \n",
    "    \n",
    "By visually summarizing these prediction results, the confusion matrix enables you to derive various performance metrics, such as accuracy, precision, recall, and F1-score, which offer deeper insights into the model's strengths and weaknesses across different classes, aiding in the evaluation and refinement of its predictive capabilities.\n",
    "\n",
    "In this task, we will visualize the ability of the network to classify the images in the test set by using a slightly more complex confusion matrix, ie, a confusino matrix for multiclass classification. You may choose to construct the matrix from scratch by yourself, or use the existing [ConfusionMatrix](https://lightning.ai/docs/torchmetrics/stable/classification/confusion_matrix.html) class in pytorch lightning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to correctly label the x and y-axes to display the class names. Apply a **proportional** output for the confusion matrix (ie. all the cells should be standardised between 0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zjAOyg831ABy",
    "outputId": "e9552bbe-0aa5-4422-9e87-c7559f65f8ab"
   },
   "outputs": [],
   "source": [
    "# Create a ConfusionMatrix instance for multiclass classification with 'num_classes'\n",
    "#  You may use the \"confusion_matrix\" from the sklearn library  \n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOQudUDK1ABy"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "## Feature Maps\n",
    "\n",
    "Intermediate feature maps in CNN represent the hierarchical transformations of input data as it passes through the network's layers. These feature maps capture increasingly abstract and complex features, gradually transitioning from low-level features like edges and textures in the early layers to high-level concepts and object representations in deeper layers. By visualizing these intermediate feature maps, we can gain insights into how the network is learning and extracting relevant information from the input data, helping us in the understanding and improvement of CNN architectures."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "gX86B-9N1ABz"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## 2.4 Visualize Feature Maps <a class=\"anchor\" id=\"t2_4\"></a>\n",
    "\n",
    "In this task we will visualize the feature maps of an image at different stages of the trained CNN. Select a __correctly__ predicted image from the test set and pass it through the CNN.\n",
    "    \n",
    "1. Observe the output after 1st relu layer (16 feature maps of size 96x96)\n",
    "2. Observe the output after 2nd relu layer (32 feature maps of size 48x48)\n",
    "3. Observe the output after 3rd relu layer (64 feature maps of size 24x24)\n",
    "\n",
    "Observe the differences between the feature maps at different levels.\n",
    "    \n",
    "\n",
    "We pass through an original image here\n",
    "\n",
    "<img src=\"images/feature_map_sample_original.PNG\" />\n",
    "\n",
    "An example of an expected output for the feature maps generated from the 1st relu layer are shown below (based on the image above):  \n",
    "\n",
    "<img src=\"images/feature_map_sample.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9MTbbyBb1ABz",
    "outputId": "21a284d8-1a6b-4e00-d0b2-fb1d62f0808b"
   },
   "outputs": [],
   "source": [
    "# Find indices of correctly classified test samples\n",
    "???\n",
    "\n",
    "# Select a random index from the correctly classified samples\n",
    "???\n",
    "\n",
    "# Retrieve the image, true label, model output, predicted label for the selected index\n",
    "???\n",
    "\n",
    "# Create a plot for displaying the selected image\n",
    "???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WDN3IXPG1ABz"
   },
   "outputs": [],
   "source": [
    "# You will be defining the visualize_features function here\n",
    "# Where you will visualize the features at different convolutional layers\n",
    "# To see what \"features\" is as an input, we recommend looking at the comments in the next code cell\n",
    "# so that you can understand what the shape of the feature map is after passing the image through the convolutional layers and activation functions\n",
    "\n",
    "def visualize_features(features):\n",
    "    # Determine the number of feature maps\n",
    "    # Determine the maximum and minimum values within each feature map so you can normalize the plot of the features according to these values\n",
    "    num_fmaps = features.shape[0] # Number of feature maps in each layer\n",
    "    num_cols = 8 # We always have divisible powers of 8 for the number of channels in the conv layers\n",
    "    num_rows = ???\n",
    "    fmax = ??? # Max values of features\n",
    "    fmin = ??? # Min values of features\n",
    "\n",
    "    # Create the figures of the feature maps\n",
    "    ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "chgPexPI1AB0",
    "outputId": "491f53e5-438b-487e-de55-c07ab1fe22ab",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compute the feature maps for the input image using the first relu layer\n",
    "# You can do this by only passing your image through the first conv1 and then relu layer manually\n",
    "# For example, if the conv1 in my first layer of the class was called my_cnn.conv1(), then I can pass\n",
    "# the image through the conv1 layer by my_cnn.conv1(image) then passing that through a relu layer\n",
    "# We can then plot the output from that feature map\n",
    "???\n",
    "\n",
    "# Visualize the feature maps using the 'visualize_features' function\n",
    "???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PPK-gNi_1AB0",
    "outputId": "63adbbbf-7020-49c0-cf6d-c3eb2756d9cd",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compute the feature maps for the input image using the second conv and relu layer (after passing it through earlier layers)\n",
    "# You can reuse the feature map from the previous code cell to do this\n",
    "\n",
    "???\n",
    "\n",
    "# Visualize the feature maps using the 'visualize_features' function\n",
    "???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ozpPb92u1AB1",
    "outputId": "8aaa540c-2821-4c7c-cbc8-898d3b72c605"
   },
   "outputs": [],
   "source": [
    "# Compute the feature maps for the input image using the third conv and relu layer (after passing it through earlier layers)\n",
    "# You can reuse the feature map from the previous code cell to do this\n",
    "???\n",
    "\n",
    "# Visualize the feature maps using the 'visualize_features' function\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2PK5W791AB1"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "## Saliency Maps\n",
    "\n",
    "A saliency map is a visual representation that highlights the most important regions within an input image that significantly influence the decision of a Convolutional Neural Network (CNN). It provides insights into the features and areas of an image that contribute the most to the network's output, helping in understanding model decisions and feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zylaEinI1AB1"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## 2.5 Visualize Saliency Maps  <a class=\"anchor\" id=\"t2_5\"></a>\n",
    "    \n",
    "One of the easiest methods to generate a saliency map of a CNN predictions is to find the the derivative of the maximum output with respect to the input image. This will tell us which pixels of the image are most sensitive towards the prediction. Such a derivative map can be obtained via following steps.\n",
    "    \n",
    "1. Select a correctly predicted image from the test set\n",
    "    \n",
    "2. Enable the `grad_fun` of this image tensor using `.requires_grad = True` (This is will enable us to compute gradients of the image)\n",
    "    \n",
    "3. Pass the image through the CNN.\n",
    "\n",
    "4. At the output, find the maximum score (out of the 10 values).\n",
    "    \n",
    "5. Perform a backpropagation with respect to the maximum output score using `.backward()`.\n",
    "\n",
    "6. Find the absolute gradients of the input image tensor using `.grad.data.abs()`.\n",
    "    \n",
    "7. This will give you a map with three channels. Let's reduce is to a single channel by finding the maximum value across all channels for each pixel.\n",
    "\n",
    "Voila! Your Saliency Map! Visualize it alongside with the input image.\n",
    "    \n",
    "An example of an expected output for the saliency map is shown below:  \n",
    "    \n",
    "<img src=\"images/saliency_map_sample.PNG\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s9Y3pMdE1AB2",
    "outputId": "6612bb2c-543a-459b-9bf4-1ac5c03dd191"
   },
   "outputs": [],
   "source": [
    "# Find indices of correctly classified test samples\n",
    "???\n",
    "\n",
    "# Select a random index from the correctly classified samples\n",
    "???\n",
    "\n",
    "# Retrieve the input image, true label, model output, predicted label for the selected index\n",
    "???\n",
    "\n",
    "# Add a batch dimension to the input image (via .unsqueeze()) and set requires_grad to True for saliency analysis\n",
    "image = image.unsqueeze(0)\n",
    "image.requires_grad = True\n",
    "\n",
    "# Compute the scores and gradients for the input image\n",
    "# To compute the scores, do a forward pass of the image and then take the argmax\n",
    "# Use this index to extract the score_max value from \"scores\"\n",
    "# Then perform a backward step so that it backpropagates the gradient\n",
    "scores = ???\n",
    "score_max_index = ???\n",
    "score_max = scores[0, score_max_index]\n",
    "??? # Do the backward step here for the gradient calculation\n",
    "\n",
    "# Calculate the saliency map by finding the maximum absolute gradient values across channels\n",
    "# You can use .abs() and torch.max()\n",
    "???\n",
    "\n",
    "# Create a subplot to display the original image and saliency map side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n",
    "\n",
    "# Display the original image with proper unnormalization\n",
    "# Do this in the first subplot\n",
    "???\n",
    "\n",
    "# Display the saliency map\n",
    "# Do this in the second subplot\n",
    "???\n",
    "\n",
    "# Show the original image and saliency map side by side\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTJ925Yi1AB2"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "## Discussion <a class=\"anchor\" id=\"t2_6\"></a>\n",
    "    \n",
    "#### What do you notice about the top correctly classified and misclassified images? Explain your reasoning.\n",
    "    \n",
    "Answer: \n",
    "    \n",
    "  \n",
    "#### Based on the confusion matrix, what can you say about this model's ability to predict certain classes?\n",
    "    \n",
    "Answer: \n",
    "    \n",
    "\n",
    "#### What are the implications behind stacking CNN layers? Discuss with respect to the generated feature maps.\n",
    "    \n",
    "Answer: \n",
    "    \n",
    "     \n",
    "#### Test your code for saliency maps on several correctly classified images of the test set. Discuss and interpret the resulting saliency maps.\n",
    "    \n",
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0G2L3hdw1AB3"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "#### This is the end of Notebook I. Now move on to the Notebook II in order to apply the knowledge you have gained on CNNs to solve a practical problem."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "dcb4be09c6369887b255ec43f7f14d3e527fd1b58c5c0441738eb1ab0a956e78"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
