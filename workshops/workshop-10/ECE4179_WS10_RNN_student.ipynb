{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://www.dropbox.com/s/vold2f3fm57qp7g/ECE4179_5179_6179_banner.png?dl=1\" alt=\"ECE4179/5179/6179 Banner\" style=\"max-width: 60%;\"/>\n",
    "</div>\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "# Recurrent Neural Networks (RNNs)\n",
    "\n",
    "</div>\n",
    "\n",
    "Welcome to week 10 of ECE4179/5179/6179! \n",
    "\n",
    "*Dun dun dun dun-da-da dun-da-da, dun-da-da!* ðŸŽ¶\n",
    "\n",
    "In an island far, far away you find yourself on a distant jungle, with nothing but your trusty laptop, equipped with the latest release of PyTorch and other beloved Python packages. \n",
    "\n",
    "What would you do? Of course you start coding and learning about **Recurrent Neural Networks (RNNs)**!\n",
    "Letâ€™s get started, May the gradients be with you! âœ¨\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"data/Mysterious-Island.jpg\" alt=\"mysterious_island\" style=\"max-width: 60%;\"/>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA GPU detected. Using CUDA.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "RND_SEED = 42\n",
    "np.random.seed(RND_SEED)\n",
    "random.seed(RND_SEED)\n",
    "torch.manual_seed(RND_SEED)\n",
    "\n",
    "\n",
    "# Check if CUDA is available (for NVIDIA GPUs)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA GPU detected. Using CUDA.\")\n",
    "\n",
    "# Check if MPS (Metal Performance Shaders) is available (for Apple Silicon M1/M2)\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Apple Silicon GPU detected. Using MPS.\")\n",
    "\n",
    "# Otherwise, default to CPU\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #2b0080; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "### <span style=\"color: pink;\">Task #1. So you claim to be a Jedi in neural networks?</span>\n",
    "\n",
    "Over the past 10 weeks, you've achieved amazing things with neural networks. Now, I challenge you to design an MLP for a simple task: adding two 3-digit numbers. For example, given 4179 and 5179, your MLP should return their sum, 9358. To simplify things further, if the sum exceeds 9999, just return the last 4 digits (e.g., 899 + 101 = 000).\n",
    "\n",
    "Consider the input to your model as 2x3 symbols (digits), where each symbol can take 10 possible values (0 to 9), and the output will be 3 symbols. Discuss how you would approach this as a supervised learning problem with an MLP. Think about how to design the output, handle classification (if needed), process the input, and other aspects of your solution.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #2b0080; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "### <span style=\"color: pink;\">Task #2. Have a go at your MLP!</span>\n",
    "\n",
    "Great! Hopefully, you have a solid plan for Task #1. Now, let's implement your MLP in PyTorch. Below, Iâ€™ve provided a couple of functions to help prepare the data and evaluate the performance of your model. Your task is to design the model and the training loop. \n",
    "\n",
    "If you're unsure how to approach this problem, here's a suggestion to get started:\n",
    "\n",
    "The input to your MLP should be an 8D vectorâ€”where the first 4 elements represent the digits of the first number, and the last 4 represent the second number. Instead of formulating this as a regression problem (which would be inefficient since the output space isn't continuous), treat it as a classification problem. Each digit is a class, meaning you'll need 4x10 output units. Use the cross-entropy loss function, applying it separately to each 10D output vector to predict each digit. PyTorchâ€™s `CrossEntropyLoss` handles this directly.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitAdditionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset class to generate pairs of random n-digit numbers and their sums,\n",
    "    where the sum is truncated or padded to match the length of the input sequences.\n",
    "\n",
    "    Args:\n",
    "        size (int): The total number of samples in the dataset. Default is 1000.\n",
    "        seq_len (int): Length of the input sequences (i.e., number of digits in each number). Default is 4.\n",
    "\n",
    "    Returns:\n",
    "        A tuple of three elements:\n",
    "            - seq1 (torch.Tensor): The first number represented as a sequence of digits.\n",
    "            - seq2 (torch.Tensor): The second number represented as a sequence of digits.\n",
    "            - sum_seq (torch.Tensor): The sum of the two numbers, truncated/padded to seq_len.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size=1000, seq_len=3):\n",
    "        self.seq_len = (\n",
    "            seq_len  # Length of the sequence (number of digits in the numbers)\n",
    "        )\n",
    "        self.size = size  # Total number of data samples to generate\n",
    "        self.data = []  # Initialize an empty list to store the generated data\n",
    "\n",
    "        # Define the range for random numbers based on the sequence length\n",
    "        min_val = 10 ** (\n",
    "            seq_len - 1\n",
    "        )  # Minimum value for a number with seq_len digits (e.g., 100 for 3 digits)\n",
    "        max_val = (\n",
    "            10**seq_len\n",
    "        ) - 1  # Maximum value for a number with seq_len digits (e.g., 999 for 3 digits)\n",
    "\n",
    "        # Generate 'size' number of data samples\n",
    "        for _ in range(size):\n",
    "            num1 = random.randint(\n",
    "                min_val, max_val\n",
    "            )  # Randomly generate the first number\n",
    "            num2 = random.randint(\n",
    "                min_val, max_val\n",
    "            )  # Randomly generate the second number\n",
    "            sum_result = num1 + num2  # Calculate the sum of the two numbers\n",
    "\n",
    "            # Convert the numbers into lists of digits\n",
    "            seq1 = [\n",
    "                int(digit) for digit in str(num1)\n",
    "            ]  # Convert num1 to a list of digits\n",
    "            seq2 = [\n",
    "                int(digit) for digit in str(num2)\n",
    "            ]  # Convert num2 to a list of digits\n",
    "            sum_seq = [\n",
    "                int(digit) for digit in str(sum_result)\n",
    "            ]  # Convert sum_result to a list of digits\n",
    "\n",
    "            # Ensure the sequences are all the same length (seq_len) by truncating or padding the sum\n",
    "            if len(sum_seq) > seq_len:\n",
    "                sum_seq = sum_seq[\n",
    "                    -seq_len:\n",
    "                ]  # Truncate the sum if it has more digits than seq_len\n",
    "            else:\n",
    "                sum_seq = [0] * (\n",
    "                    seq_len - len(sum_seq)\n",
    "                ) + sum_seq  # Pad the sum with leading zeros if it has fewer digits\n",
    "\n",
    "            # Append the generated sequence (input1, input2, and the sum) to the data list\n",
    "            self.data.append((seq1, seq2, sum_seq))\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns the input-output pair (two input sequences and their sum) for a given index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            A tuple of torch.Tensors:\n",
    "                - seq1: The first number represented as a sequence of digits.\n",
    "                - seq2: The second number represented as a sequence of digits.\n",
    "                - sum_seq: The sum of the two numbers, represented as a sequence of digits.\n",
    "        \"\"\"\n",
    "        seq1, seq2, sum_seq = self.data[idx]\n",
    "        return torch.tensor(seq1), torch.tensor(seq2), torch.tensor(sum_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "train_size = 500  # Number of training examples\n",
    "train_dataset = DigitAdditionDataset(size=train_size)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Test dataset\n",
    "test_size = 1000  # Number of test examples\n",
    "test_dataset = DigitAdditionDataset(size=test_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3]) torch.Size([32, 3]) torch.Size([32, 3])\n",
      "torch.Size([3]) torch.Size([3])\n",
      "Sequence 1: [4, 4, 4]\n",
      "Sequence 2: [2, 0, 4]\n",
      "Sum       : [6, 4, 8]\n",
      "------------------------------\n",
      "torch.Size([3]) torch.Size([3])\n",
      "Sequence 1: [6, 7, 8]\n",
      "Sequence 2: [6, 3, 5]\n",
      "Sum       : [3, 1, 3]\n",
      "------------------------------\n",
      "torch.Size([3]) torch.Size([3])\n",
      "Sequence 1: [4, 0, 8]\n",
      "Sequence 2: [9, 7, 0]\n",
      "Sum       : [3, 7, 8]\n",
      "------------------------------\n",
      "torch.Size([3]) torch.Size([3])\n",
      "Sequence 1: [3, 2, 0]\n",
      "Sequence 2: [8, 8, 1]\n",
      "Sum       : [2, 0, 1]\n",
      "------------------------------\n",
      "torch.Size([3]) torch.Size([3])\n",
      "Sequence 1: [7, 1, 7]\n",
      "Sequence 2: [3, 2, 5]\n",
      "Sum       : [0, 4, 2]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Get the first batch from train_loader\n",
    "seq1, seq2, sum_seq = next(iter(train_loader))\n",
    "print(seq1.shape, seq2.shape, sum_seq.shape)\n",
    "\n",
    "# Print the first 5 data samples from the batch\n",
    "for i in range(5):\n",
    "    print(seq1[i].shape, sum_seq[i].shape)\n",
    "    print(f\"Sequence 1: {seq1[i].tolist()}\")\n",
    "    print(f\"Sequence 2: {seq2[i].tolist()}\")\n",
    "    print(f\"Sum       : {sum_seq[i].tolist()}\")\n",
    "    print(\"-\" * 30)  # Separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Mapping\n",
    "import pytorch_lightning as pl\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class AddModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        lr=1e-3,\n",
    "        hidden_size=128,\n",
    "        num_layers=2,\n",
    "        seq_len=3,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        input_size = 2\n",
    "        output_size = 10\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x1, x2) -> Any:\n",
    "        hidden = torch.zeros(self.num_layers, x1.size(0), self.hidden_size).to(\n",
    "            self.device\n",
    "        )\n",
    "        cell = hidden.clone()\n",
    "        x = torch.stack((x1, x2), dim=2).float().flip([1])\n",
    "        rnn_out, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "        out = rnn_out.flip([1])\n",
    "        out = self.fc(out)\n",
    "        out = out.permute(0, 2, 1)\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch) -> Tensor | Mapping[str, Any] | None:\n",
    "        x1, x2, y = batch\n",
    "        logits = self(x1, x2)\n",
    "        loss = self.loss(logits, y)\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch) -> Tensor | Mapping[str, Any] | None:\n",
    "        x1, x2, y = batch\n",
    "        logits = self(x1, x2)\n",
    "        loss = self.loss(logits, y)\n",
    "\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch) -> Tensor | Mapping[str, Any] | None:\n",
    "        x1, x2, y = batch\n",
    "        logits = self(x1, x2)\n",
    "        loss = self.loss(logits, y)\n",
    "\n",
    "        self.log(\"test_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=self.hparams_initial[\"lr\"])\n",
    "\n",
    "\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints\",\n",
    "    filename=\"model-{epoch:02d}-{val_loss:.2f}\",\n",
    "    monitor=\"val_loss\",\n",
    ")\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "\n",
    "model = AddModel()\n",
    "trainer = pl.Trainer(max_epochs=200, callbacks=[checkpoint, early_stopping])\n",
    "trainer.fit(model, train_loader, test_loader)\n",
    "trainer.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.80\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.802"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AddModel.load_from_checkpoint(\"checkpoints/model-epoch=122-val_loss=0.23.ckpt\")\n",
    "\n",
    "\n",
    "def eval_model(model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for x1, x2, y in test_loader:\n",
    "            logits = model(x1.to(model.device), x2.to(model.device))\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == y.to(model.device)).all(dim=1).sum().item()\n",
    "            total += y.size(0)\n",
    "    accuracy = correct / total\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "eval_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNNs\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are a type of neural network designed for sequence data. Unlike standard feed-forward neural networks, RNNs have **recursion** in their architecture, which allows them to maintain a form of memory, known as **hidden state** across different time steps. Can you think why they might suit our addition problem?\n",
    "\n",
    "\n",
    "\n",
    "In a vanilla RNN, the recursion can be mathematically represented as follows:\n",
    "\n",
    "\\begin{align}\n",
    "\\boldsymbol{h}^{\\{t\\}} = f\\left(\\boldsymbol{W}_{hh} \\cdot \\boldsymbol{h}^{\\{t-1\\}}  + \\boldsymbol{W}_{hx} \\cdot \\boldsymbol{x}^{\\{t\\}}  + \\boldsymbol{b}_h \\right)\n",
    "\\end{align}\n",
    "\n",
    "Where:\n",
    "- $\\boldsymbol{h}^{\\{t\\}}$ is the hidden state at time step $t$.\n",
    "- $\\boldsymbol{h}^{\\{t-1\\}}$ is the hidden state from the previous time step.\n",
    "- $\\boldsymbol{x}^{\\{t\\}}$ is the input at time step $t$.\n",
    "- $\\boldsymbol{W}_{hh}$ and $\\boldsymbol{W}_{hx}$ are weights of the model, and $\\boldsymbol{b}_h$ is the bias term.\n",
    "- $f$ is the activation function (commonly $ \\tanh $ or $ \\text{ReLU} $).\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"data/ece4179_ws10_rnn1.png\" alt=\"RNN Example1\" style=\"width: 50%;\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "To understand how an RNN process sequences, you can think of it as unrolling the recursion over time. The hidden state at each time step is updated based on the input at that time step and the hidden state from the previous time step. This allows the RNN to maintain a form of memory across different time steps.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"data/ece4179_ws10_rnn2.png\" alt=\"RNN Example2\" style=\"width: 50%;\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM in PyTorch\n",
    "\n",
    "\n",
    "\n",
    "A **Long Short-Term Memory (LSTM)** network is a type of recurrent neural network (RNN) designed to handle sequence data and learn long-term dependencies. Unlike a vanilla RNN, an LSTM contains **cell states** and **gates** (e.g., forget) to control the flow of information and prevent issues like vanishing gradients.\n",
    "\n",
    "In PyTorch, an LSTM layer processes an input sequence and returns two outputs: \n",
    "1. The output for each time step.\n",
    "2. The hidden and cell states.\n",
    "\n",
    "The input to the LSTM layer in PyTorch has the following shape:\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "    \\text{input} = (\\text{B}, \\tau, \\text{input\\_size})\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "- $\\text{B}$ is the **batch size**, the number of sequences processed simultaneously.\n",
    "- $\\tau$ is the **sequence length**, which is the number of time steps in each sequence.\n",
    "- $\\text{input\\_size}$ refers to the size (number of features) of each element in the sequence.\n",
    "\n",
    "For example, in a problem where each element in the sequence is a 2D vector (two features), and we process a batch of 4 sequences of length 5 in one batch, the input will have shape `(4, 5, 2)`.\n",
    "\n",
    "LSTMs maintain a **hidden state** and a **cell state** for each layer. The hidden and cell states have the following shape:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\text{hidden}, \\text{cell} = (\\text{num\\_layers}, \\text{B}, \\text{hidden\\_size})\n",
    "\\end{align*}\n",
    "\n",
    "Where:\n",
    "- $\\text{num\\_layers}$ is the number of LSTM layers.\n",
    "- $ \\text{hidden\\_size} $ is the size of the hidden state (number of features in the hidden state).\n",
    "\n",
    "If your LSTM has 2 layers, a batch size of 4, and a hidden size of 128, the hidden and cell states will have shape `(2, 4, 128)`.\n",
    "\n",
    "\n",
    "\n",
    "The output of the LSTM consists of:\n",
    "1. **Output for each time step**: This is the hidden state for each time step in the sequence.\n",
    "   \\begin{align*}\n",
    "        \\text{output} = (\\text{B}, \\tau, \\text{hidden\\_size})\\;.\n",
    "   \\end{align*}\n",
    "   \n",
    "   If you process 4 sequences of length 5, and the hidden size is 128, the output will have the shape `(4, 5, 128)`.\n",
    "\n",
    "2. **Final hidden and cell states**:\n",
    "   \\begin{align*}\n",
    "   \\text{hidden}, \\text{cell} = (\\text{num\\_layers}, \\text{B}, \\text{hidden\\_size})\n",
    "   \\end{align*}\n",
    "   These tensors contain the final hidden and cell states for each layer and for each sequence in the batch. If the LSTM has 2 layers, a batch size of 4, and a hidden size of 128, both the hidden and cell states will have shape `(2, 4, 128)`.\n",
    "\n",
    "\n",
    "### Did You Know? \n",
    "\n",
    "Very recently, advancements in LSTM architecture have led to the development of **xLSTM: Extended Long Short-Term Memory**. This new variation introduces modifications to the standard LSTM, enhancing its ability to **retain memory** over longer sequences and improve its ability to learn **long-range dependencies**. \n",
    "\n",
    "While weâ€™ll be working with the traditional LSTM in this notebook, it's exciting to know that LSTMs are continuously evolving, and innovations like xLSTM are pushing the boundaries of what these models can achieve. \n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"data/xlstm.jpg\" alt=\"xLSTM\" style=\"width: 50%;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #2b0080; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "### <span style=\"color: pink;\">Task #3. Learn to add via RNNs</span>\n",
    "\n",
    "Time to level up your skills! Design an LSTM model to solve the addition problem you tackled with an MLP.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Models\n",
    "\n",
    "A **language model (LM)** is a type of model that learns to predict the next token in a sequence, given a preceding context. An example of an LM, duh, ChatGPT! So, let's design one "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Mysterious Island\n",
    "\n",
    "**Jules Verne** (1828â€“1905) was a French novelist, and poet, widely regarded as one of the pioneers of science fiction. His adventure novels, filled with detailed scientific explanations. Some of his most famous works include **\"Twenty Thousand Leagues Under the Sea\"**, **\"Around the World in Eighty Days\"**, and **\"Journey to the Center of the Earth\"**.\n",
    "\n",
    "**\"The Mysterious Island\"** is the story of five castaways who find themselves stranded on a mysterious, uninhabited island. Using their ingenuity and resourcefulness, they attempt to survive, all while uncovering the secrets of the island. In this task, weâ€™ll use **\"The Mysterious Island\"** as the training data to build an **LM**, maybe we can mimic Verneâ€™s unique writing style.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"data/Jules_Verne.jpg\" alt=\"Jules Verne\" style=\"width: 25%;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading and processing text\n",
    "with open(\"data/The_Mysterious_Island.txt\", \"r\", encoding=\"utf8\") as jv_file:\n",
    "    jv_book_text = jv_file.read()\n",
    "\n",
    "# Create a set of unique characters\n",
    "jv_char_set = set(jv_book_text)\n",
    "\n",
    "# Print results\n",
    "print(\"Total Length:\", len(jv_book_text))\n",
    "print(\"Unique Characters:\", len(jv_char_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "A tokenizer processes text and converts it to a sequence of symbols or tokens that can be fed into a model. In this task, weâ€™ll use a simple tokenizer that converts the text to lowercase and splits it into words. The **Tokenizer** class is responsible for:\n",
    "1. **Creating a Vocabulary**: The tokenizer identifies all unique characters in the input text and builds a vocabulary, which is a sorted list of these unique characters.\n",
    "2. **Character-to-Integer Mapping**: Once the vocabulary is established, the tokenizer creates a dictionary that maps each character to a unique integer index (`char2int`). This enables the model to work with numbers rather than raw text.\n",
    "3. **Integer-to-Character Mapping**: The tokenizer also builds the reverse mapping (`int2char`), which allows the model to convert predicted integers back into characters for generating readable text.\n",
    "\n",
    "\n",
    "- `encode(text)`: This method converts input text into a list of integer values based on the character-to-integer mapping. The output is a NumPy array of integers, where each integer represents a character in the text.\n",
    "  \n",
    "  Example:\n",
    "  ```python\n",
    "  tokenizer.encode('Hello') \n",
    "  # Output: [8, 4, 11, 11, 14]  (based on the vocabulary and char2int mapping)\n",
    "  ```\n",
    "\n",
    "- `decode(encoded_text)`: This method converts a list of integers (representing characters) back into the original text using the integer-to-character mapping.\n",
    "  \n",
    "  Example:\n",
    "  ```python\n",
    "  tokenizer.decode([8, 4, 11, 11, 14]) \n",
    "  # Output: 'Hello'  (converts integers back to the corresponding characters)\n",
    "  ```\n",
    "\n",
    "\n",
    "BTW, check this out [OpenAI Tokenizer](https://platform.openai.com/tokenizer)! ðŸ¤–\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, text):\n",
    "        # Create the vocabulary (sorted set of unique characters)\n",
    "        self.vocab = sorted(set(text))\n",
    "        self.char2int = {ch: i for i, ch in enumerate(self.vocab)}\n",
    "        self.int2char = np.array(self.vocab)\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Converts text into an array of integers.\"\"\"\n",
    "        return np.array([self.char2int[ch] for ch in text], dtype=np.int32)\n",
    "\n",
    "    def decode(self, encoded_text):\n",
    "        \"\"\"Converts an array of integers back into text.\"\"\"\n",
    "        return \"\".join(self.int2char[encoded_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #2b0080; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "### <span style=\"color: pink;\">Task #4. Study the character-level tokenizer</span>\n",
    "\n",
    "To have a short and fun break, study the `Tokenizer` class provided below. Understand how it works and how it can be used to encode and decode text. Feel free to experiment with the tokenizer by encoding and decoding different texts.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Tokenizer with the text\n",
    "jv_tokenizer = Tokenizer(jv_book_text)\n",
    "\n",
    "# Provide a custom text for encoding and decoding\n",
    "prompt = \"hello ece4179\"\n",
    "\n",
    "# Encode the custom text\n",
    "enc_tokens = jv_tokenizer.encode(prompt)\n",
    "\n",
    "# Print the encoded custom text\n",
    "print(\"Prompt:\", prompt)\n",
    "print(\"Encoded tokens:\", enc_tokens)\n",
    "\n",
    "# Decode the encoded custom text back to the original text\n",
    "dec_prompt = jv_tokenizer.decode(enc_tokens)\n",
    "\n",
    "# Print the decoded text\n",
    "print(\"Decoded prompt from tokens:\", dec_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data\n",
    "\n",
    "The **TextDataset** class is responsible for preparing the training data for our LM. It encodes the input text into integer sequences and creates pairs of input-target data that the model uses to learn how to predict the next character in a sequence.\n",
    "\n",
    "\n",
    "When training a character-level LM, the goal is to feed sequences of characters (input) into the model and have it predict the next character (target). The **TextDataset** class constructs the training data by creating sliding windows of sequences from the encoded text.\n",
    "\n",
    "The main idea is to slide a window of `seq_length` characters across the encoded text. For each index idx, the dataset generates an input sequence and its corresponding target sequence. The input sequence is a chunk of seq_length characters starting at position idx in the encoded text. The target sequence is created by shifting the input sequence by one character. This teaches the model to predict the next character for each position in the input sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, book_text, tokenizer, seq_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_length = seq_length\n",
    "        # Encode the entire book text using the tokenizer\n",
    "        self.text_encoded = self.tokenizer.encode(book_text)\n",
    "\n",
    "    def __len__(self):\n",
    "        # The number of available chunks in the dataset\n",
    "        return len(self.text_encoded) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Generate input sequence and target dynamically\n",
    "        input_seq = self.text_encoded[idx : idx + self.seq_length]\n",
    "        target_char = self.text_encoded[idx + 1 : idx + self.seq_length + 1]\n",
    "        return torch.tensor(input_seq).long(), torch.tensor(target_char).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 40  # Sequence length for training\n",
    "jv_dataset = TextDataset(jv_book_text, jv_tokenizer, seq_length=seq_length)\n",
    "\n",
    "batch_size = 64\n",
    "jv_dataloader = DataLoader(\n",
    "    jv_dataset, batch_size=batch_size, shuffle=True, drop_last=True\n",
    ")\n",
    "\n",
    "# let's print the first sample from the dataset\n",
    "trn_x_sample, trn_y_sample = jv_dataset[0]\n",
    "print(f\"Input sequence: {trn_x_sample}\")\n",
    "print(f\"Target character: {trn_y_sample}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #2b0080; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "### <span style=\"color: pink;\">Task #5. Create an RNN for Character-Level LM</span>\n",
    "\n",
    "Your RNN should consist of the following components:\n",
    "\n",
    "1. **Embedding Layer**: This layer will map each character (represented as an integer) into a dense vector of fixed size (`embed_dim`). The embedding allows the model to learn meaningful representations of each character.\n",
    "\n",
    "2. **LSTM Layer**: The core of your RNN model will be an **LSTM** (Long Short-Term Memory) layer. This layer will process the input sequences and capture patterns over time.\n",
    "\n",
    "3. **Fully Connected (Linear) Layer**: After processing the sequence with the LSTM, a fully connected layer will map the hidden states back to the vocabulary space, allowing the model to predict the next character.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(jv_tokenizer.vocab)\n",
    "embed_dim = 256\n",
    "rnn_hidden_size = 512\n",
    "\n",
    "# instantiate the model\n",
    "jv_model = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop for LM\n",
    "\n",
    "The training loop for the LM is similar to the training loop for the previous task to a great degree. The loss function used in the training loop is the **CrossEntropyLoss**. We will train the model for fixed number of iterations in the workshop, feel free to improve the model by training it for more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(jv_model.parameters(), lr=0.001)\n",
    "\n",
    "# Number of iterations\n",
    "num_iterations = 10000\n",
    "print_every_k = 100  # Print and update the progress bar every `k` iterations\n",
    "batch_size = 64  # Example batch size\n",
    "\n",
    "\n",
    "# Initialize the progress bar for iterations\n",
    "pbar = tqdm(total=num_iterations, desc=\"Training Progress\", position=0, leave=True)\n",
    "\n",
    "\n",
    "total_loss = 0\n",
    "\n",
    "for iteration in range(num_iterations):\n",
    "\n",
    "    seq_batch, target_batch = next(iter(jv_dataloader))\n",
    "\n",
    "    # Initialize hidden and cell states\n",
    "    hidden, cell = jv_model.init_hidden(batch_size)\n",
    "\n",
    "    # Move batch to the correct device (CPU/GPU)\n",
    "    seq_batch, target_batch = seq_batch.to(device), target_batch.to(device)\n",
    "\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass through the model with the entire sequence\n",
    "    pred, hidden, cell = jv_model(seq_batch, hidden, cell)\n",
    "    pred = pred.view(-1, vocab_size)  # Reshape to [batch_size,  seq_length, vocab_size]\n",
    "    # Compute loss for the last time step prediction\n",
    "    loss = loss_fn(pred, target_batch.view(-1).long())\n",
    "\n",
    "    # Backpropagation and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Accumulate total loss for this iteration\n",
    "    total_loss += loss.item()\n",
    "\n",
    "    # Update the progress bar and print every `k` iterations\n",
    "    if iteration % print_every_k == 0:\n",
    "        # Calculate the average loss over the last `k` iterations\n",
    "        avg_loss = total_loss / print_every_k\n",
    "\n",
    "        # Update the progress bar with the average loss\n",
    "        pbar.set_postfix(loss=avg_loss)\n",
    "        pbar.update(print_every_k)  # Update the progress bar by 'k' steps\n",
    "\n",
    "        # Reset total_loss for the next `k` iterations\n",
    "        total_loss = 0\n",
    "\n",
    "# Close the progress bar when training is done\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #2b0080; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "### <span style=\"color: pink;\">Task #6. Write with your model</span>\n",
    "\n",
    "Too tired to write? Let your model do the work! Use your trained LM to generate text based on a given prompt. You can start with a simple prompt like \"The island\" and see what your model comes up with. Feel free to experiment with different prompts and see how your model performs.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Categorical\n",
    "\n",
    "\n",
    "def sample(model, starting_str, len_generated_text=500, temperature=1.0):\n",
    "    # Encode the starting string\n",
    "    encoded_input = jv_tokenizer.encode(starting_str)\n",
    "    encoded_input = torch.tensor(encoded_input).unsqueeze(0)  # Shape [1, seq_length]\n",
    "\n",
    "    generated_str = starting_str\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    hidden, cell = model.init_hidden(1)  # For batch size of 1\n",
    "    hidden, cell = hidden.to(\"cpu\"), cell.to(\"cpu\")\n",
    "\n",
    "    # Pass the entire starting string through the model to prime the hidden state\n",
    "    _, hidden, cell = model(encoded_input, hidden, cell)\n",
    "\n",
    "    # Start sampling from the last character of the input\n",
    "    last_char = encoded_input[:, -1]  # Get the last character of the input\n",
    "\n",
    "    # Generate the next characters\n",
    "    for i in range(len_generated_text):\n",
    "        # Forward pass for the last character\n",
    "        logits, hidden, cell = model(last_char.view(1, 1), hidden, cell)  # Shape [1, 1]\n",
    "        logits = torch.squeeze(logits, 0)  # Remove batch dimension\n",
    "\n",
    "        # Scale logits by temperature (divide by temperature)\n",
    "        scaled_logits = logits / temperature\n",
    "\n",
    "        # Sample from the scaled distribution\n",
    "        m = Categorical(logits=scaled_logits)\n",
    "        last_char = m.sample()\n",
    "\n",
    "        # Decode and append the generated character to the string\n",
    "        generated_str += jv_tokenizer.decode([last_char.item()])\n",
    "\n",
    "    return generated_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jv_model.to(\"cpu\")\n",
    "print(\n",
    "    sample(\n",
    "        jv_model, starting_str=\"The island\", len_generated_text=500, temperature=0.50\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ECE4179",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
