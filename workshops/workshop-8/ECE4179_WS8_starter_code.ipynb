{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://www.dropbox.com/s/vold2f3fm57qp7g/ECE4179_5179_6179_banner.png?dl=1\" alt=\"ECE4179/5179/6179 Banner\" style=\"max-width: 60%;\"/>\n",
    "</div>\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "# Transfer Learning\n",
    "\n",
    "</div>\n",
    "\n",
    "Welcome to the eight workshop of ECE4179/5179/6179! In this notebook, we learn about Transfer Learning (TL), a technique that allows us to reuse pre-trained models to solve new problems. In TL, we leverage pre-trained models, trained on large datasets, to tackle new, often smaller, tasks. Instead of starting the learning process from scratch, TL gives us a head start, making it possible to achieve high accuracy with less data and training time. Let's get started!\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "from lightning import seed_everything\n",
    "\n",
    "\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "\n",
    "\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "\n",
    "RND_SEED = 42\n",
    "np.random.seed(RND_SEED)\n",
    "seed_everything(RND_SEED)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\") # if CUDA is not working for you\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #2b0080; color: white; padding: 10px; border-radius: 5px; width: 90%; margin: 0 auto; margin-top: 10px;\">\n",
    "\n",
    "### <span style=\"color: pink;\">Task #1. Understanding a PyTorch model</span>  \n",
    "\n",
    "When conducting transfer learning or fine-tuning specific layers of a neural network, it's crucial to understand how layers are named and structured within the model. Accessing and modifying particular layers often relies on correctly referencing their names.\n",
    "\n",
    "As your first task, simply compare the naming conventions of the two models below. \n",
    "\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary style=\"color: yellow; font-weight: bold;\">PyTorch</summary>\n",
    "\n",
    "In PyTorch, every `nn.Module` contains layers or components that hold learnable parameters, typically weights and biases. The method `named_parameters()` is useful when you want to retrieve both the names and the actual parameters of your model.\n",
    "\n",
    "Both **linear** and **convolutional** layers have two main types of parameters:\n",
    "1. **Weights**: These are the core learnable parameters in a layer, responsible for transforming the input.\n",
    "2. **Biases**: Additional learnable parameters, added to the layer output after applying the weights.\n",
    "\n",
    "When calling `named_parameters()`, the returned iterable yields:\n",
    "- `name`: A string that represents the name of the parameter. PyTorch automatically names the parameters following the structure in the model definition.\n",
    "- `param`: The actual parameter object, which contains the weight values and gradients.\n",
    "\n",
    "\n",
    "</details>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define layers\n",
    "        self.input_layer = nn.Linear(input_size, hidden_size1)\n",
    "        self.hidden_layer = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.output_layer = nn.Linear(hidden_size2, output_size)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.input_layer(x))\n",
    "        x = self.relu(self.hidden_layer(x))\n",
    "        return self.output_layer(x)\n",
    "\n",
    "# Create an instance of the MLP\n",
    "mlp1 = MLP(10, 50, 20, 5)\n",
    "\n",
    "# Print layer names and their corresponding sizes (weights and biases)\n",
    "for name, param in mlp1.named_parameters():\n",
    "    print(f\"Parameter Name: {name}, Shape: {param.shape}\")\n",
    "\n",
    "#====================================================================================================\n",
    "mlp2 = nn.Sequential(\n",
    "    nn.Linear(10, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 5)\n",
    ")\n",
    "print(f\"*\"*50)\n",
    "for name, param in mlp2.named_parameters():\n",
    "    print(f\"Parameter Name: {name}, Shape: {param.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "We will be utilizing the [Butterfly and Moth](https://www.kaggle.com/datasets/gpiosenka/butterfly-images40-species) dataset available on Kaggle. This dataset comprises images of 100 distinct species of butterflies and moths. Each image is a color JPG file with dimensions $224 \\times 224$.\n",
    "\n",
    "The dataset is divided as follows:\n",
    "- Training: 12,594 images\n",
    "- Testing: 500 images (5 per class, ensuring balanced representation)\n",
    "- Validation: 500 images (similarly balanced with 5 images per species)\n",
    "\n",
    "It's worth noting that while the test and validation sets maintain class balance, the training set exhibits slight imbalances among species. Given time constraints in the workshop, our exercises will focus on a reduced subset of the training data, with a random selection of 50 images from each species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the line below with your own path\n",
    "dataset_path = \"data/\"\n",
    "\n",
    "# Get a list of directories (classes) in the train folder\n",
    "classes = os.listdir(os.path.join(dataset_path, \"train\"))\n",
    "classes.sort()  # Optional: sort the class names alphabetically\n",
    "num_classes = len(classes)\n",
    "\n",
    "\n",
    "print(f\"Number of classes: {num_classes}\\n\")\n",
    "print(\"Classes:\")\n",
    "for i, class_name in enumerate(classes, 1):\n",
    "    print(f\"{i:3}: {class_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #2b0080; color: white; padding: 10px; border-radius: 5px; width: 90%; margin: 0 auto; margin-top: 10px;\">\n",
    "\n",
    "### <span style=\"color: pink;\">Task #2: Dataset Exploration and Visualization</span>  \n",
    "\n",
    "Familiarize yourself with the `ButterflyDataset` class and use it to create training, validation, and test datasets. \n",
    "Note that we need to use the imagenet transformations for the pre-trained models per what we discussed in the previous workshop. \n",
    "\n",
    "To instantiate from the `ButterflyDataset` class, you need to provide the root directory of the data, the transformation to be applied, and the list of classes. For example, if your train folder is located at `dataset_path/train`, you can create the training dataset as follows:\n",
    "\n",
    "```python\n",
    "\n",
    "train_dataset = ButterflyDataset(\"dataset_path/train\", \n",
    "                                 transform=data_transforms[\"train\"],\n",
    "                                 classes=classes)\n",
    "```\n",
    "\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. **Dataset Creation**:\n",
    "   - Instantiate training, validation, and test datasets using the `ButterflyDataset` class. Make sure to apply the respective transformations provided in the `data_transforms` dictionary.\n",
    "   \n",
    "   \n",
    "2. **Data Loaders**:\n",
    "   - Construct data loaders for the datasets you've just created. Consider using `torch.utils.data.DataLoader`. Choose a reasonable batch size that aligns with the computational resources available to you.\n",
    "\n",
    "3. **Sanity Check**:\n",
    "   - Print out the length of each dataset (training, validation, test) to ensure your data has been loaded correctly.\n",
    "\n",
    "4. **Visualization**:\n",
    "   - Use the `imshow` function provided to visualize some images from each set. This will give you a feel for the data's appearance after transformations and also validate the correctness of your data loaders.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "\n",
    "# image dataset class\n",
    "class ButterflyDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, classes=None):\n",
    "        super().__init__()  \n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = classes\n",
    "        self.data = []\n",
    "        \n",
    "        # Gather all image paths and corresponding labels\n",
    "        for label, label_id in zip(self.classes, range(len(self.classes))):\n",
    "            label_dir = os.path.join(self.root_dir, label)\n",
    "            for filename in os.listdir(label_dir):\n",
    "                if filename.endswith('.jpg'):\n",
    "                    image_path = os.path.join(label_dir, filename)\n",
    "                    self.data.append((image_path, label_id))\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path, label = self.data[idx]\n",
    "        image = Image.open(image_path).convert('RGB') \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ButterflyDataset(os.path.join(dataset_path, \"train\"), \n",
    "                                 transform=data_transforms[\"train\"],\n",
    "                                 classes=classes)\n",
    "test_dataset = ButterflyDataset(os.path.join(dataset_path, \"test\"), \n",
    "                                transform=data_transforms[\"test\"],\n",
    "                                classes=classes)\n",
    "val_dataset = ButterflyDataset(os.path.join(dataset_path, \"valid\"), \n",
    "                               transform=data_transforms[\"test\"],\n",
    "                               classes=classes)\n",
    "    \n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders\n",
    "batch_size = 64\n",
    "\n",
    "trn_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "tst_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Display an image from a Tensor.\n",
    "\n",
    "    Args:\n",
    "        inp (Tensor): The input image tensor in the format (C, H, W), \n",
    "                      typically obtained from the dataloader.\n",
    "        title (str, optional): Optional title for the image plot.\n",
    "    \"\"\"\n",
    "    # Convert the input tensor to a NumPy array and transpose it to (H, W, C) for display\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    \n",
    "    # Mean and standard deviation used during the normalization of ImageNet pre-trained models\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    \n",
    "    # the input is a normalized tensor, we need to reverse the normalization process (multiply by std and add mean)\n",
    "    inp = std * inp + mean\n",
    "    \n",
    "    # Clip values to ensure they are within the range [0, 1], suitable for displaying images\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    \n",
    "    # Display the image using matplotlib\n",
    "    plt.imshow(inp)\n",
    "    \n",
    "    # If a title is provided, display it\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    \n",
    "    # Remove axis labels and ticks for a cleaner image display\n",
    "    plt.axis('off')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "imgs, y = next(iter(trn_loader))  # Get a batch of images and labels from the train dataloader\n",
    "\n",
    "# Make a grid of images from the batch using torchvision's make_grid function\n",
    "# `nrow=16` means there will be 8 images per row, and `padding=3` adds space between images\n",
    "out = make_grid(imgs, nrow=16, padding=3)\n",
    "\n",
    "# Plot the images in a larger figure to improve visibility\n",
    "plt.figure(figsize=(16, 12))  # Set the size of the figure\n",
    "imshow(out)  # Display the grid of images using the imshow function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help you get started, we've provided two functions to evaluate and train a model. Familiarize yourself with these functions as you will be using them throughout the workshop. Note that we opt for pure PyTorch training loops (as Mehrtash is too lazy to use Lightning) to keep things simple and easy to follow. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(data_loader, model=None):\n",
    "    # Ensure that a model is provided for evaluation\n",
    "    if model is None:\n",
    "        raise ValueError(\"model is None\")\n",
    "    \n",
    "    # Initialize the Accuracy metric for multiclass classification\n",
    "    # `Accuracy` will compute the accuracy of the predictions with respect to labels\n",
    "    # `classes` should be a list of the unique class names\n",
    "    accuracy = Accuracy(task=\"multiclass\", num_classes=len(classes)).to(device)\n",
    "\n",
    "    # Define the criterion (loss function) as Cross Entropy Loss\n",
    "    # This is commonly used for multiclass classification tasks\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Set the model to evaluation mode. In this mode, certain layers like Dropout \n",
    "    # and BatchNorm will behave differently than in training mode.\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize counters for total loss and accuracy\n",
    "    total_loss = 0.0\n",
    "    total_accuracy = 0.0\n",
    "\n",
    "    # `torch.inference_mode()` disables gradient computation during the loop, \n",
    "    # which reduces memory usage and speeds up the computations\n",
    "    with torch.inference_mode():\n",
    "        # Iterate over the batches of data in the data loader\n",
    "        for images, labels in data_loader:\n",
    "            # Move the images and labels to the GPU (or specified device)\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(dtype=torch.long).to(device)\n",
    "            \n",
    "            # Forward pass: compute predicted outputs by passing inputs to the model\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            # Accumulate the batch loss into the total loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Convert model outputs to predicted class indices\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            # Update the accuracy metric with predictions and actual labels\n",
    "            accuracy.update(preds, labels)\n",
    "\n",
    "    # Calculate the average loss over all batches\n",
    "    total_loss /= len(data_loader)\n",
    "    # Compute the overall accuracy\n",
    "    total_accuracy = accuracy.compute().item()\n",
    "\n",
    "    return total_loss, total_accuracy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model=None, lr=0.001, num_epochs=5):\n",
    "    # Ensure that a model is provided for training\n",
    "    if model is None:\n",
    "        raise ValueError(\"model is None\")\n",
    "\n",
    "    # Define the loss function as Cross Entropy Loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # Define the optimizer as Adam with the provided learning rate `lr`\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Initialize lists to store loss and accuracy values for each epoch\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    # Training loop for the specified number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        # Set the model to training mode\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        # Iterate over the batches of data in the training data loader\n",
    "        for images, labels in trn_loader:\n",
    "            # Move images and labels to the GPU (or specified device)\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(dtype=torch.long).to(device)\n",
    "            \n",
    "            # Forward pass: compute predicted outputs by passing inputs to the model\n",
    "            outputs = model(images)\n",
    "            # Compute the loss between predicted and actual labels\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Zero the gradients to ensure that they aren't accumulated across epochs\n",
    "            optimizer.zero_grad()\n",
    "            # Perform the backward pass to compute gradients\n",
    "            loss.backward()\n",
    "            # Update the model's weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumulate the batch loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Compute the average training loss for the epoch\n",
    "        avg_loss = total_loss / len(trn_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "\n",
    "        # Evaluate the model on validation data\n",
    "        val_loss, val_acc = evaluate_model(val_loader, model)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        # Evaluate the model on test data\n",
    "        tst_loss, tst_acc = evaluate_model(tst_loader, model)\n",
    "        test_losses.append(tst_loss)\n",
    "        test_accuracies.append(tst_acc)\n",
    "        \n",
    "        # Print training progress\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
    "        print(f\"Training Loss: {avg_loss:.4f}\")\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "        print(f\"Test Loss: {tst_loss:.4f}, Test Accuracy: {tst_acc:.4f}\")\n",
    "        print(\"======================================\")\n",
    "\n",
    "\n",
    "    # Create a dictionary to hold results\n",
    "    results_dict = {\n",
    "        'train_loss': avg_loss,\n",
    "        'val_loss': val_loss,\n",
    "        'val_accuracy': val_acc,\n",
    "        'test_loss': tst_loss,\n",
    "        'test_accuracy': tst_acc\n",
    "    }\n",
    "\n",
    "    return results_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #2b0080; color: white; padding: 10px; border-radius: 5px; width: 90%; margin: 0 auto; margin-top: 10px;\">\n",
    "\n",
    "### <span style=\"color: pink;\">Task #3: ResNet18 as pre-trained model</span>\n",
    "\n",
    "We will employ the ResNet18 model, pre-trained on the ImageNet dataset, in this workshop. Use the PyTorch Hub to load the model and print out its layers and parameters. This will give you a sense of the model's architecture and the dimensionality of its weights.\n",
    "\n",
    "\n",
    "There are a couple of things to note here:\n",
    "- the model has four main convolutional blocks. The number of filters increases from 64 to 512 as we go deeper into the network.\n",
    "\n",
    "- at the start of each convolutional block, we have a downsampling layer that reduces the spatial dimensions of the input by a factor of 2. This is achieved by a convolutional layer with stride 2.\n",
    "\n",
    "- the last layer of the model is a fully-connected layer with 512 input features and 1000 output features. This layer is responsible for mapping the features extracted by the convolutional layers to the 1000 ImageNet classes.\n",
    "\n",
    "- Inside each block, the input of the block is added similar to the residual connections to the output of middle convolutional layer. Since the number of channels of the input and output of the block are different, the input is first passed through a 1x1 convolutional layer to match the number of channels of the output. This is called the projection shortcut. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will keep this for later\n",
    "model0 = # <--- Your code here\n",
    "\n",
    "\n",
    "for name, param in model0.named_parameters():\n",
    "    print(f\"Name: {name}, Shape: {param.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #2b0080; color: white; padding: 10px; border-radius: 5px; width: 90%; margin: 0 auto; margin-top: 10px;\">\n",
    "\n",
    "\n",
    "### <span style=\"color: pink;\">Task #4. Linear Probing</span>  \n",
    "\n",
    "Linear probing is a technique in transfer learning where we leverage the features learned by a pre-trained model by just replacing the classifier of the pre-trained model with a single linear layer and learning only the parameters of the linear layer. The idea is to benefit directly from the representational power of the fixed, pre-trained model. A good performance by linear probing indicates that the extracted features by the pre-trained model are rich and discriminative for the given task. To implement linear probing, these steps are typically followed:\n",
    "\n",
    "1. Use a pre-trained model and remove its classification layer. This truncated model will serve as our feature extractor.\n",
    "\n",
    "2. On top of this, add a new linear layer that corresponds to the number of classes in your task.\n",
    "\n",
    "3. Train only this linear layer using your dataset while keeping the rest of the model frozen.\n",
    "\n",
    "Here is some Python/PyTorch hints to get you going:\n",
    "\n",
    "```python\n",
    "\n",
    "# use deepcopy from the copy module to avoid modifying the original model\n",
    "model = copy.deepcopy(pretrained_model)\n",
    "\n",
    "# find the name of the last layer of the model\n",
    "# you can use print(model) to see the structure of the model\n",
    "\n",
    "\n",
    "# imagine the name of the last layer is \"fc\"\n",
    "# you can replace the last layer with a new layer as follows\n",
    "model.fc = nn.Linear(in_features=..., out_features=...)\n",
    "\n",
    "# now you need to freeze the rest of the model from getting updated\n",
    "# you can do this by setting the requires_grad attribute of the parameters to False\n",
    "# you can use the following code snippet to do this\n",
    "for name, param in model.parameters():\n",
    "    if 'fc' in name: # unfreeze the last fully-connected\n",
    "        param.requires_grad = True\n",
    "    else: # freeze all other parameters\n",
    "        param.requires_grad = False\n",
    "```\n",
    "Note that ResNets use batchnorm layers, helping them to generalize better. Batchnorm layers have learnable parameters and also running mean and variance. In transfer learning, we usually freeze the batchnorm layers to avoid updating their running mean and variance. This can be done via the following code snippet:\n",
    "\n",
    "```python   \n",
    "# Set BN layers to eval mode to freeze running mean/variance\n",
    "for module in model.modules():\n",
    "    if isinstance(module, nn.BatchNorm2d) or isinstance(module, nn.BatchNorm1d):\n",
    "        module.eval()  # This will freeze the running statistics of the BN layers\n",
    "\n",
    "```\n",
    "\n",
    "I will let you explore whether freezing the batchnorm layers is beneficial or not in this task.\n",
    "\n",
    "<details>\n",
    "<summary style=\"color: yellow; font-weight: bold;\">Show me some results</summary>\n",
    "After training this model for 20 epochs, you can expect a validation accuracy of around <span style=\"background-color: #ADD8E6\">85.8%</span>. Correspondingly, the test accuracy for that model would be <span style=\"background-color: #ADD8E6\">84.0%</span>.\n",
    "</details>\n",
    "\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code below. Define the number of epochs to train for based on your laptop's performance\n",
    "# on a CPU, my machine takes about 2 minutes per epoch\n",
    "num_epochs=10\n",
    "\n",
    "# Create a new model from the pretrained one\n",
    "model1 = deepcopy(model0)  # <--- study the code here\n",
    "\n",
    "# Change the last layer to output 100 classes\n",
    "model1.fc =         # <--- add your code here\n",
    "\n",
    "# Freeze all layers except the last fully-connected\n",
    "for name, param in model1.named_parameters():\n",
    "    if 'fc' in name: # unfreeze the last fully-connected\n",
    "        param.requires_grad = # <--- add your code here\n",
    "    else: # freeze all other parameters\n",
    "        param.requires_grad = # <--- add your code here\n",
    "\n",
    "model1 = model1.to(device)\n",
    "\n",
    "# Train the model\n",
    "res1 = train_model(model=model1, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning the last convolutional block of a pre-trained model\n",
    "\n",
    "In linear probing, we merely adjusted the classifier (last fully-connected layer) of the model. Fine-tuning the convolutional blocks allows us to harness the power of pre-trained models even further.\n",
    "\n",
    "A pre-trained model has already learned a significant amount of information from data, especially in the initial layers. The initial layers of a model often capture generic features.For example, a CNN trained on ImageNet will learn to detect features like edges, textures, and primitive shapes in its initial convolutional layers. As we move deeper into the model, the layers tend to capture more dataset-specific or task-specific features. By fine-tuning the last convolutional layer(s), we aim to adjust these deeper features to better suit our specific task.\n",
    "\n",
    "**Why Fine-tune the Last Convolutional Layer?**\n",
    "\n",
    "1. **Better Feature Representation**: The final layers usually represent high-level features. Adjusting them can lead to better feature representations for our specific task.\n",
    "\n",
    "2. **Faster Training**: Since we are adjusting only the last few layers, the training process is faster than training from scratch.\n",
    "  \n",
    "3. **Avoid Overfitting**: Fine-tuning a few layers (instead of the entire model) reduces the number of trainable parameters, potentially reducing the risk of overfitting, especially when our dataset is not very large.\n",
    "\n",
    "**How to Do It?**\n",
    "1. First, you'll want to identify the last convolutional layer or block in your model. This might vary depending on the architecture.\n",
    "2. Once identified, set `requires_grad=True` for its parameters, allowing them to be updated during training.\n",
    "3. Train the model for a few epochs and observe the results. If the model is overfitting, you can try reducing the learning rate.\n",
    "\n",
    "Remember, the idea is not to start from scratch but to leverage the knowledge already present in the model and adapt it to our specific task.\n",
    "\n",
    "\n",
    "<div style=\"background-color: #2b0080; color: white; padding: 10px; border-radius: 5px; width: 90%; margin: 0 auto; margin-top: 10px;\">\n",
    "\n",
    "\n",
    "### <span style=\"color: pink;\">Task #5. Fine-tune convolutional block 4 and the classifier</span>\n",
    "\n",
    "ResNet18 has 4 convolutional blocks. Each convoltional block has two convolutional layers. In this task, you will fine-tune the last convolutional block and the classifier. \n",
    "\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary style=\"color: yellow; font-weight: bold;\">Show me some results</summary>\n",
    "After training this model for 20 epochs, you can expect a validation accuracy of around <span style=\"background-color: #ADD8E6\">90.2%</span>. Correspondingly, the test accuracy for that model would be <span style=\"background-color: #ADD8E6\">91.2%</span>.\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = deepcopy() # <--- add your code here\n",
    "\n",
    "\n",
    "# freeze all parameters except the last fc layer (classifier)\n",
    "# and the conv layers in the last block (layer4)\n",
    "# add your code below\n",
    "for name, param in model2.named_parameters():\n",
    "    # <--- add your code here\n",
    "        \n",
    "\n",
    "model2 = model2.to(device)\n",
    "\n",
    "# Train the model\n",
    "res2 = train_model(model=model2, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #2b0080; color: white; padding: 10px; border-radius: 5px; width: 90%; margin: 0 auto; margin-top: 10px;\">\n",
    "\n",
    "\n",
    "### <span style=\"color: pink;\">Task #6. Question everything! </span>\n",
    "Just to gain a better understanding, let's try to fine-tune the first convolutional block of ResNet18. What do you expect to happen? Will the model perform better or worse? Why? In terms of training time, do you expect it to be faster or slower? Why?\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary style=\"color: yellow; font-weight: bold;\">Show me some results</summary>\n",
    "After training this model for 20 epochs, you can expect a validation accuracy of around <span style=\"background-color: #ADD8E6\">83.6%</span>. Correspondingly, the test accuracy for that model would be <span style=\"background-color: #ADD8E6\">83.8%</span>.\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = deepcopy() # <--- add your code here\n",
    "\n",
    "\n",
    "# freeze all parameters except the last fc layer (classifier)\n",
    "# and the conv layers in the first block (layer1)\n",
    "# add your code below\n",
    "for name, param in model3.named_parameters():\n",
    "    # <--- add your code here\n",
    "        \n",
    "\n",
    "model3 = model3.to(device)\n",
    "\n",
    "# Train the model\n",
    "res3 = train_model(model=model3, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Progressive Fine-Tuning\n",
    "\n",
    "Progressive fine-tuning is an intuitive and gradual adaptation strategy for fine-tuning pre-trained models. Instead of immediately adjusting all the layers or a specific subset, we progressively unfreeze and fine-tune the layers starting from the top (final layers) and moving towards the bottom (initial layers).\n",
    "\n",
    "**Why Use Progressive Fine-Tuning?**\n",
    "There are two main reasons to use progressive fine-tuning:\n",
    "\n",
    "- **Reduce Overfitting**: Gradually training layers reduces the risk of overfitting, especially when the target dataset is small.\n",
    "  \n",
    "- **Stable Convergence**: It often results in more stable convergence during training. Immediate unfreezing of all layers can sometimes lead to catastrophic forgetting of useful features.\n",
    "\n",
    "**Steps for Progressive Fine-Tuning**:\n",
    "1. **Start Frozen**: Begin with all layers frozen except the final classification layer. Train until convergence or for a few epochs.\n",
    "2. **Unfreeze Gradually**: Start unfreezing the last layers progressively. For instance, you can unfreeze the last convolutional block and fine-tune.\n",
    "3. **Iterate**: Continue this process, moving deeper into the network, unfreezing more blocks or layers as you go. After each unfreezing step, train the model for a few epochs.\n",
    "\n",
    "There are various strategies for progressive fine-tuning, and one approach is as follows:\n",
    "\n",
    "1. **Initial Training**: Start by training only the final classification layer, keeping all other layers frozen.\n",
    "   \n",
    "2. **Fine-Tune Convolutional Blocks**: Once the classification layer has been trained, freeze it. Proceed to unfreeze and fine-tune the last convolutional block(s).\n",
    "   \n",
    "3. **Re-Train Classification Layer**: After fine-tuning the convolutional block(s), unfreeze the classification layer. Train both the classification layer and the last convolutional block(s) simultaneously.\n",
    "   \n",
    "\n",
    "\n",
    "<div style=\"background-color: #2b0080; color: white; padding: 10px; border-radius: 5px;\">\n",
    " \n",
    "### <span style=\"color: pink;\">Task #7. Progressive Fine-Tuning</span>\n",
    "\n",
    "We will take a simple approach to progressive fine-tuning. We will start with the linear probing model and then fine-tune the last convolutional block along with the classifier.\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary style=\"color: yellow; font-weight: bold;\">Show me some results</summary>\n",
    "After training this model for 20 epochs, you can expect a validation accuracy of around <span style=\"background-color: #ADD8E6\">88.6%</span>. Correspondingly, the test accuracy for that model would be <span style=\"background-color: #ADD8E6\">91.4%</span>.\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code below\n",
    "model4 = deepcopy() # model1 is the one with the last fc layer unfrozen\n",
    "\n",
    "\n",
    "# freeze all parameters except the last fc layer (classifier)\n",
    "# and the conv layers in the last block (layer4)\n",
    "# add your code below\n",
    "for name, param in model4.named_parameters():\n",
    "    # <--- add your code here\n",
    "\n",
    "model4 = model4.to(device)\n",
    "\n",
    "# Train the model\n",
    "res4 = train_model(model=model4, num_epochs=num_epochs, lr=# <--- add your code here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #2b0080; color: white; padding: 10px; border-radius: 5px; width: 90%; margin: 0 auto; margin-top: 10px;\">\n",
    "\n",
    "\n",
    "### <span style=\"color: pink;\">Task #8. MLP as Classifier in Transfer Learning</span>  \n",
    "\n",
    "\n",
    "When employing transfer learning, one common practice is to replace the final fully connected layer (or linear probing layer) of the pre-trained model with a new classifier tailored to the new task. While a single linear layer is frequently used for this purpose, a more expressive MLP can be beneficial for complex tasks that require higher capacity.\n",
    "\n",
    "Your Task:\n",
    "\n",
    "1. Remove the last fully connected layer from the pre-trained model.\n",
    "2. Append a two-layer MLP with 256 hidden units to the model.\n",
    "3. Fine-tune this new structure on the dataset.\n",
    "\n",
    "To implement the MLP, you can use the following code snippet:\n",
    "\n",
    "```python\n",
    "mlp = nn.Sequential(\n",
    "    nn.Linear(in_features=..., out_features=...),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=..., out_features=...),\n",
    ")\n",
    "\n",
    "# Then you can append the MLP to the model as follows\n",
    "model.fc = mlp\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "You may want to use a dropout layer to prevent overfitting. \n",
    "\n",
    "\n",
    "<details>\n",
    "<summary style=\"color: yellow; font-weight: bold;\">Show me some results</summary>\n",
    "After training this model for 20 epochs, you can expect a validation accuracy of around <span style=\"background-color: #ADD8E6\">85.0%</span>. Correspondingly, the test accuracy for that model would be <span style=\"background-color: #ADD8E6\">87.2%</span>.\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we start with the pretrained model\n",
    "model5 = deepcopy(model0) \n",
    "\n",
    "# replace the last fc layer with an MLP\n",
    "# add your code below\n",
    "model5.fc = nn.Sequential(\n",
    "    # <--- add your code here\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "model5 = model5.to(device)\n",
    "res5 = train_model(model=model5, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding-top: 10cm;\"></div>\n",
    "\n",
    "<div style=\"background-color: #3352FF; color: white; padding: 10px; border-radius: 5px; width: 90%; margin: 0 auto; margin-top: 10px;\">\n",
    "\n",
    "\n",
    "### Fine-tuning a Self-Supervised Model\n",
    "\n",
    "</div>\n",
    "\n",
    "Self-supervised learning (SSL) is to train models without the need for labeled data. Unlike conventional supervised models that rely on large amounts of labeled data, SSL models learn powerful representations by predicting certain aspects of the data itself.\n",
    "\n",
    "Key Differences:\n",
    "- **Learning Approach**: SSL models predict parts of the input, like masked portions or augmentations, rather than explicit class labels.\n",
    "- **Robust Representations**: SSL often leads to more generalized and robust feature representations, beneficial in scarce labeled data scenarios.\n",
    "\n",
    "[**DINO (DIstillation with NO Labels)**](https://dinov2.metademolab.com/) is a prominent SSL technique. Instead of using class labels, DINO optimizes the model to produce consistent embeddings for different augmentations of the same image. These embeddings capture rich semantic information. Below, we will use a pre-trained DINO model and fine-tune it for our butterfly classification task. We will use DINO-ResNet50 model but you can use any other DINO-V2 models (all are transformer-based) from HuggingFace for the fun of it.\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary style=\"color: yellow; font-weight: bold;\">Show me some results</summary>\n",
    "After training this model for 20 epochs, you can expect a validation accuracy of around <span style=\"background-color: #ADD8E6\">88.6%</span>. Correspondingly, the test accuracy for that model would be <span style=\"background-color: #ADD8E6\">89.8%</span>.\n",
    "\n",
    "</details>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The DINO model is available in the PyTorch Hub\n",
    "ssl_model = torch.hub.load('facebookresearch/dino:main', 'dino_resnet50')\n",
    "\n",
    "print(ssl_model)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssl_model.fc = nn.Linear(2048, num_classes)\n",
    "print(ssl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in ssl_model.named_parameters():\n",
    "    if 'fc' in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "ssl_model = ssl_model.to(device)\n",
    "res_ssl = train_model(model=ssl_model, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
