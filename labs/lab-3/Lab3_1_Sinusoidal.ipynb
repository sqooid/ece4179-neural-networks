{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "# <p style=\"text-align: center;\">Lab 3 (Weeks 5,6): Multi-Layer Perceptrons (MLP) </p>\n",
    "\n",
    "<img src=\"https://viso.ai/wp-content/uploads/2021/04/multilayer-perceptrons-MLP-concept-1.jpg\" width=\"400\" height=\"200\" />\n",
    "\n",
    "<!-- ![linear-vs-logistic-regression--medium](https://miro.medium.com/max/1400/1*dm6ZaX5fuSmuVvM4Ds-vcg.jpeg) -->\n",
    "\n",
    "Welcome to your third lab of ECE4179! Labs in this unit will run as a help desk and they are not mandatory to attend. As a reminder, this lab can be completed in pairs but you and your partner need to register via the google forms on Moodle under week 5.\n",
    "\n",
    "The two notebooks provided contain all the code and comments that you need to submit. Feel free to add in your own markdown for additional comments. After completion, You need to submit both Jupyter Notebooks (.ipynb file) to Moodle. Make sure you run the Notebooks before submitting and all outputs are visible.\n",
    "\n",
    "In this Lab, you will find three tasks distributed across two notebooks: Sinusoidal and CoverType. These tasks will guide you through to learn using a deep learning framework (Pytorch lightning) for MLP based problems. These knowledge and skills will be essential for lab 4 and assignment, and in general, critical to get you prepared to enter the deep learning world.\n",
    "\n",
    "- <b>Task 1:</b> Shallow MLP to fit MLP into a sinusoidal function\n",
    "- <b>Task 2:</b> Shallow MLP for multiclass classification of the Covertype dataset\n",
    "- <b>Task 3:</b> Deep MLP for multiclass classification of the Covertype dataset\n",
    "\n",
    "Each task will contain code to complete, and worded questions, so ensure you complete both before submitting.\n",
    "\n",
    "Good luck with the Lab!\n",
    "\n",
    "__Submission details:__\n",
    "- __Make sure you have run all your cells from top to bottom (you can click _Kernel_ and _Restart Kernel and Run All Cells_).__ </br>\n",
    "- __Submit the Jupyter Notebooks (Lab3\\_1\\_Sinusoidal.ipynb) and (Lab3\\_2\\_CoverType.ipynb).__\n",
    "- __Outputs must be visible upon submission. We will also be re-running your code__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Enter your student details below</b>\n",
    "\n",
    "- <b>Student Name:</b> Firstname Lastname\n",
    "- <b>Student ID:</b> 123456789\n",
    "\n",
    "If you have a partner:\n",
    "- <b>Second Student Name:</b> Firstname Lastname\n",
    "- <b>Second Student ID:</b> 123456789"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "* [Task 1 - Approximate the sine function](#prox-sine)\n",
    "    * [1.1 Create custom dataset and dataloaders](#1_1)\n",
    "    * [1.2 Design a Neural Network](#1_2)\n",
    "    * [1.3 Train and evaluate the network](#1_3)\n",
    "    * [1.4 Visualise and Analyse the Experimental Results](#1_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "# Before you begin\n",
    "\n",
    "We have provided some numerical answers for you to aim for. To replicate these results, do not change any of the codes that are labelled \"Do not change\".\n",
    "\n",
    "Throughout this lab, there will be code and written answers that you need to fill in / complete. Please read the instructions carefully. The comments in the code snippet and markdown text will guide you on what you need to do.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "## Pytorch & Pytorch-Lightning Installation <a class=\"anchor\" id=\"installization\"></a>\n",
    "\n",
    "What are the differences between PyTorch and PyTorch-Lightning?\n",
    "\n",
    "PyTorch is based on the Torch library, adapted for Python. PyTorch is a deep learning library that allows you to have greater control of your neural network architecture, and have a lot more customisable hyper parameters / functions (such as the loss function etc.) compared to other deep learning frameworks such as TensorFlow / Keras.\n",
    "\n",
    "PyTorch-Lightning is a higher level of PyTorch, meaning that it is easier to use and run models on compared to the original PyTorch. There are methods that are already built into the classes for PyTorch-Lightning which you don't have to worry about as much, and takes away a lot of the additional considerations such as passing your dataset via CUDA, hence PyTorch-Lightning code will look simpler than PyTorch!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have not installed pytorch-lightning already, then please follow the instructions below\n",
    "\n",
    "# # If you run on Jupyter Lab uncomment below comment\n",
    "# ! pip install --quiet \"matplotlib\" \"pytorch-lightning\" \"pandas\" \"torchmetrics\" \"torchvision\"\n",
    "\n",
    "# # If you run on google colab uncomment below comment\n",
    "# ! pip install pytorch-lightning -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Libraries, you do not need to import any additional libraries for this lab\n",
    "\n",
    "import numpy as np ## Numpy is the fundamental building block of understanding tensor (matrices) within Python\n",
    "import matplotlib.pyplot as plt ## Matplotlib.pyplot is the graphing library that we will be using throughout the semester\n",
    "import random ## Useful for sampling\n",
    "# import sys ## Useful to retrieve some system information\n",
    "\n",
    "import os ## Useful for running command line within python\n",
    "import pandas as pd ## Useful for data manipulation\n",
    "from IPython.display import Image ## For markdown purposes\n",
    "from IPython.display import clear_output\n",
    "import PIL\n",
    "\n",
    "import torch ## Pytorch is the deep learning library that we will be using\n",
    "import torch.nn as nn ## Neural network module\n",
    "import torch.nn.functional as F ## Functional module\n",
    "from torch import optim ## Optimizer module\n",
    "from torch.utils.data import DataLoader, random_split ## Under torchvision datasets you can find popular datasets that are frequently used for machine learning/deep learning tasks (eg., MNIST, SVHN, CIFAR10, CIFAR100 etc).\n",
    "from torch.utils.data.dataset import Dataset ## You can create your own custom dataset using torchvision dataset in-built functionalities.\n",
    "from torchmetrics import Accuracy ## Torchmetrics is a library that contains metrics for evaluating models\n",
    "import torchvision ## Torchvision is a library that contains popular datasets, model architectures, and image transformations for computer vision tasks\n",
    "from torchvision import transforms\n",
    "\n",
    "import pytorch_lightning as pl ## Pytorch lightning is a wrapper for pytorch that makes it easier to train models\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import Callback, ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
    "\n",
    "\n",
    "\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "## Setting seeds for reproducibility. Do NOT change these!\n",
    "seed_everything(4179)\n",
    "random.seed(4179)\n",
    "np.random.seed(4179)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "# Task 1 - Approximate the sine function <a class=\"anchor\" id=\"prox-sine\"></a>\n",
    "\n",
    "In this section, we will approximate the sine function with a neural network to get a sense of how architecture and hyperparameters affect neural network performance. \n",
    "\n",
    "As the first step you should have PyTorch and PyTorch-Lightning installed in your local machine. We have given you the set of necessary libraries and details on PyTorch functionalities that you need to use when implementing algorithm for Task 1.\n",
    "\n",
    "#### In this task, you will work on the following points:\n",
    "    \n",
    " 1. Define the custom dataset class (i.e., Train and Test datasets) and visualize the train dataset you've created.\n",
    "    \n",
    " 2. Design the Shallow Linear MLP model using PyTorch Lightning Module.\n",
    "    \n",
    " 3. Train and evaluate the MLP model on defined train dataset and test dataset.\n",
    "    \n",
    " 4. Visualize experimental results using Matplotlib.\n",
    "\n",
    "\n",
    "Before approaching, we will be introducing Pytorch Lightning Neural Network structure, Pytorch datasets, dataloaders and optimizers.\n",
    "\n",
    "<img src=\"./figures/sine_wave.gif\" width=\"1000\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## 1.1 Create custom dataset and dataloaders <a class=\"anchor\" id=\"1_1\"></a>\n",
    "\n",
    "Pytorch has a huge number of functionalities that make training our neural networks very easy! One of those functionalities is the Pytorch dataset and dataloader (they are real life-savers!). In depth review on PyTorch Datasets and Dataloaders are covered in Lectures!\n",
    "\n",
    "\n",
    "In PyTorch, there are two Classes that can make the training process much easier - **Dataset** ([```torch.utils.data.Dataset```](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset)) and **Dataloader** ([```torch.utils.data.DataLoader```](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)). Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset to enable easy access to the samples. Take a look that [this page](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) that explain further about data handling in PyTorch.\n",
    "\n",
    "Simply speaking, a **Dataset** object loads the raw data and provides easy access to each sample (sample point + their label). Then a **Dataloader** object takes the Dataset object and creates batches of the data for model training/testing.\n",
    "\n",
    "In the following section, let's create our Dataset and Dataloader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Creating a dataset\n",
    "\n",
    "The dataset you are going to be creating will be points from a \"noisy\" sine wave. To create the custom dataset, you can use PyTorch dataset inbuilt functionalities. <br>\n",
    "\n",
    "\n",
    "The Pytorch dataset class has three essential parts:<br>\n",
    "1. The \\__init__ function (as most Python classes do)<br>\n",
    "2. The \\__getitem__ function (this is called during every iteration)<br>\n",
    "3. The \\__len__ function (this must return the length of the dataset)\n",
    "\n",
    "**Remember! The \"self\" within classes will become attributes of that class that you can use within other methods that are defined for that class. If you defined an attribute without self.<\\name>, then that attribute cannot be used for other methods.**\n",
    "\n",
    "Make sure to follow all inline comments specified for each code fragment that you need to work on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a \"SineDataset\" class by importing the Pytorch Dataset class\n",
    "class SineDataset(Dataset):\n",
    "    \"\"\" Data noisy sinewave dataset\n",
    "        num_datapoints - the number of datapoints you want\n",
    "    \"\"\"\n",
    "    def __init__(self, num_datapoints):\n",
    "        # Lets generate the noisy sinewave points\n",
    "        \n",
    "        # Create \"num_datapoints\" worth of random x points using a uniform distribution (0-1) using torch.rand\n",
    "        # Then scale and shift the points to be between -9 and 9\n",
    "        self.x_data = ???? \n",
    "        \n",
    "        # Calculate the sin of all data points in the x vector and the scale amplitude\n",
    "        self.y_data = ????\n",
    "        \n",
    "        # Add some gaussein noise to each datapoint using torch.randn_like\n",
    "        # Note:torch.randn_like will generate a tensor of gaussein noise the same size and type as the provided tensor\n",
    "        self.y_data += ????\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # This function is called by the dataLOADER class whenever it wants a new mini-batch\n",
    "        # The dataLOADER class will pass the dataSET and number of datapoint indexes (mini-batch of indexes)\n",
    "        # It is up to the dataSET's __getitem__ function to output the corresponding input datapoints \n",
    "        # AND the corresponding labels\n",
    "        return ????, ????\n",
    "        # Note:Pytorch will actually pass the __getitem__ function one index at a time\n",
    "        # If you use multiple dataLOADER \"workers\" multiple __getitem__ calls will be made in parallel\n",
    "        # (Pytorch will spawn multiple threads)\n",
    "\n",
    "    def __len__(self):\n",
    "        # We also need to specify a \"length\" function, Python will use this fuction whenever\n",
    "        # You use the Python len(function)\n",
    "        # We need to define it so the dataLOADER knows how big the dataSET is!\n",
    "        return ????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Create Dataset instance from the defined Class\n",
    "\n",
    "Now that you've defined your dataset Class, lets create an instance of it for training and testing and then create dataloaders to make it easy to iterate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x_train = 30000   # the number of training datapoints\n",
    "n_x_test = 8000     # the number of testing datapoints\n",
    "BATCH_SIZE = 16     # the batch size for training task 1\n",
    "BATCH_SIZE_TEST = 1 # the batch size for testing task 1\n",
    "\n",
    "# Create an instance of the SineDataset for both the training and test set \n",
    "# (Here we have only training and test set. Therefore consider validation set also equals to test set)\n",
    "dataset_train = ???? \n",
    "dataset_test  = ????\n",
    "\n",
    "# Now we need to pass the dataset to the Pytorch dataloader class along with some other arguments\n",
    "# batch_size - the size of our mini-batches\n",
    "# shuffle - whether or not we want to shuffle the dataset \n",
    "#         - For training shuffle is set to True and for Testing/Validation shuffle is set to False\n",
    "data_loader_train = ????\n",
    "data_loader_test = ????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Visualise the dataset you've created!\n",
    "\n",
    "Note: see here how we can just directly access the data from the dataset class. Make sure your generated sinewave matches the described sinewave within the dataset class!\n",
    "\n",
    "**Hint**: Scatter plot is a good way to visualise these points. You can also change the size of the scattered points for a better view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the scatter plot\n",
    "????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## 1.2 Design a Neural Network <a class=\"anchor\" id=\"1_2\"></a>\n",
    "\n",
    "In the workshops, we have had a taste of how to train a Gradient Descent algorithm. In this task, we will build a *Shallow MLP* that can be trained to approximate a sine wave.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "#### Activation functions\n",
    "\n",
    "Starting from here you will start to create multi-layered networks with one or many \"hidden\" layers separated by \"**activation functions**\" that give our networks \"**non-linearities**\". If we didn't have these activation functions and simple stacked layers together, our network would be no better than a single linear layer! Why? Because multiple sequential \"linear transformations\" can be modeled with just a single linear transformation.\n",
    "\n",
    "Neural networks are associated with a Directed Acyclic Graphs (DAG) describing how the functions are composed together. For example, we might have three functions $f^{(1)}, f^{(2)}$, and $f^{(3)}$, connected in a chain, to form in a chain, to form  $f(x) = f^{(3)}(f^{(2)}(f^{(1)}(x))).$ In this case, $f^{(1)}$ is called the first layer of the network, $f^{(2)}$ is called the second layer, and so on.  The ﬁnal layer of a feedforward network is called the output layer.\n",
    "\n",
    "Except the output layer, the behavior of the other layers is not directly speciﬁed by the training data. The learning algorithm must decide how to use those layers to produce the desired output, but the training data do not say what each individual layer should do. Instead, the learning algorithm must decide how to use these layers to best implement an approximation of $f^*$. Because the training data does not show the desired output for each of these layers, and they\n",
    "are called hidden layers. These hidden layers, receive input from many other units and computes its own **activation value**. This requires us to choose the activation functions that will be used to compute the hidden layer values.\n",
    "\n",
    "So what are these nonlinear activation functions that turn our simple linear models into a power \"nonlinear function approximator\"? Some common examples are:<br>\n",
    "1. relu\n",
    "2. sigmoid\n",
    "3. tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's design a two layer Shallow MLP model with **one hidden layer** and **tanh** activation function using the PyTorch Lightning module. The model structure is given below:\n",
    "\n",
    "fc1 : Linear(1 $\\times$ n) $\\rightarrow$ tanh $\\rightarrow$ fc2 : Linear(n $\\times$ 1)\n",
    "\n",
    "Here since both the input data *x* and target value *y* are just one number, the input and output of the network are 1. And **n** is the dimension we would like to define for the hidden layer.\n",
    "\n",
    "We are going to use the **mean squeare error loss** for our model training - [```nn.MSELoss()```](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html).\n",
    "\n",
    "All models created in PyTorch Lightning need to inherit the parent class **pytorch_lightning.LightningModule**, check [this link](https://lightning.ai/docs/pytorch/stable/common/lightning_module.html) for a starter example for creating custom models using PyTorch Lightning. There are two basic functions: \\_\\_init__() and forward().\n",
    "\n",
    "- In \\_\\_init__(), we can define the hyperparameters, the shape of each layer, etc. Same as in all python classess, this function will be called when a new instance is made.\n",
    "  \n",
    "- In forward(), we define the forward propagation for the model. This method is called by the object to do forward pass.\n",
    "\n",
    "Other core functions include:\n",
    "\n",
    "- training_step() - one complete training step that takes one batch as input, and returns a loss value at the end.\n",
    "\n",
    "- validation_step() - one complete validation step that takes one batch as input.\n",
    "\n",
    "- test_step() - one complete test step that takes one batch as input.\n",
    "\n",
    "- predict_step() - one complete prediction step that takes one batch as the input, and returns the predicted labels, the true lables and the input samples.\n",
    "\n",
    "- configure_optimizers() - define optimizers (and learning rate schedulers if needed).\n",
    "\n",
    "In the following task, we will define all the functions mentioned above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytorch Lightning Module\n",
    "\n",
    "Now enough reading! Let's get to work and start implementing a MLP. The hyperparameters that you will be using are as follows:\n",
    "\n",
    "- Learning rate of 1e-2 (remember, \"1e-2 is identical to the scientific notation of 10^-2)\n",
    "- Loss - MSELoss. This will be used to calculate loss for a classification task.\n",
    "- MLP - One hidden layer for the MLP. You will need to figure out how to find the size of the MLP. You will also use tanh as your non-linear activiation function after the hidden layer. \n",
    "- MLP - One output layer to estimate the y value. \n",
    "- Optimizer - SGD or Adam (you can use the torch.optim library that we have already imported)\n",
    "\n",
    "To define a Pytorch Lightning model to be trained, here you need to use the Pytorch LightningModule class as the base for defining your network. Just like the dataset class, this class has a number of important functions.\n",
    "\n",
    "A LightningModule is a PyTorch nn.Module and it has a few more helpful methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowLinear(LightningModule):\n",
    "    def __init__(self, hidden_size=[1, 64, 1], learning_rate=1e-2, optimizer=\"SGD\"):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Set our init args as class attributes\n",
    "        self.learning_rate = learning_rate # Learning rate\n",
    "        self.loss_fn = ???? # Use MSE loss as cost function\n",
    "        self.optimizer = optimizer # Optimizer\n",
    "        \n",
    "        D_in, H, D_out = hidden_size\n",
    "        \n",
    "        # define the two layers\n",
    "        self.linear1 = ????\n",
    "        self.linear2 = ????\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # This function is an important one and we must create it or pytorch will give us an error!\n",
    "        # This function defines the \"forward pass\" of our neural network\n",
    "        \n",
    "        #Lets define the sqeuence of events for our forward pass!\n",
    "        x = ???? # hidden layer\n",
    "        x = ????   # activation function\n",
    "\n",
    "        #No activation function on the output!!\n",
    "        x = ???? # output layer\n",
    "        \n",
    "        #Note we re-use the variable x as we don't care about overwriting it \n",
    "        #though in later labs we will want to use earlier hidden layers\n",
    "        #later in our network!\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # get the x and y values of the data point\n",
    "        x, y = batch\n",
    "        \n",
    "        # pass through the network\n",
    "        logits = ????\n",
    "        \n",
    "        # Get the loss\n",
    "        loss = ????\n",
    "        \n",
    "        # Calling self.log will surface up scalars for you in TensorBoard\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # get the x and y values of the data point\n",
    "        x, y = ????\n",
    "        # pass through the network\n",
    "        logits = ????\n",
    "        # Get the loss\n",
    "        loss = ????\n",
    "\n",
    "        # Calling self.log will surface up scalars for you in TensorBoard\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # get the x and y values of the data point\n",
    "        ????\n",
    "        # pass through the network\n",
    "        ????\n",
    "        # Get the loss\n",
    "        ????\n",
    "        \n",
    "\n",
    "        # Calling self.log will surface up scalars for you in TensorBoard\n",
    "        self.log(\"test_loss\", loss, prog_bar=True)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        # get the x and y values of the data point\n",
    "        ????\n",
    "        # pass through the network\n",
    "        pred = ????\n",
    "        return pred\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        if self.optimizer == \"Adam\":\n",
    "            # Define the optimizer with the learning rate if we choose the Adam optimizer\n",
    "            optimizer = ????\n",
    "        elif self.optimizer == \"SGD\":\n",
    "            # Define the optimizer with the learning rate if we choose the SGD optimizer\n",
    "            optimizer = ????\n",
    "                \n",
    "        return optimizer\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return data_loader_train\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return data_loader_test\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return data_loader_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## 1.3 Train and evaluate the network  <a class=\"anchor\" id=\"1_3\"></a>\n",
    "\n",
    "Now let's do some training for the network we just created!\n",
    "\n",
    "By using the Trainer Constructor you can train and test the model. Also Trainer will automatically enables: \n",
    "1. Tensorboard logging \n",
    "2. Model checkpointing \n",
    "3. Training and validation loop \n",
    "4. early-stopping\n",
    "\n",
    "In this task, we will test the model performance for both optimizers:\n",
    "- SGD\n",
    "- ADAM\n",
    "\n",
    "And let's train both models with **20 epoches**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Train with SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Model with SGD optimizer\n",
    "model_sgd = ????\n",
    "\n",
    "# Train model for 20 epochs\n",
    "trainer_task1_SGD = Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    devices=1  if not torch.cuda.is_available() else torch.cuda.device_count(),\n",
    "    max_epochs=????,\n",
    "    callbacks=[TQDMProgressBar(refresh_rate=20)],\n",
    "    logger=????,\n",
    ")\n",
    "\n",
    "# Start the training loop\n",
    "????\n",
    "\n",
    "# Evaluate Model\n",
    "????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your implementation is correct, you should obtain a printed output similar to this:\n",
    "\n",
    "[{'test_loss': 0.011488508433103561}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Train with Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Model with Adam optimizer\n",
    "model_adam = ????\n",
    "\n",
    "# Train model for 20 epochs\n",
    "trainer_task1_ADAM = Trainer(\n",
    "    ????\n",
    ")\n",
    "\n",
    "# Start the training loop\n",
    "???\n",
    "\n",
    "# Evaluate Model\n",
    "????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your implementation is correct, you should obtain a printed output similar to this:\n",
    "\n",
    "[{'test_loss': 0.005056165624409914}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## 1.4 Visualise and Analyse the Experimental Results <a class=\"anchor\" id=\"1_4\"></a>\n",
    "\n",
    "Now the training is done. Let's check how well our model has performed.\n",
    "\n",
    "There are a few ways we can evaluate the model performance:\n",
    "- Evaluate accuracy\n",
    "- Inspect the loss\n",
    "- Precision, recall and F1-Score\n",
    "- Confusion Matrix\n",
    "- And more\n",
    "\n",
    "Let's try some of them here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Visual check on the approximation of the model\n",
    "\n",
    "Visualise by generate scatter plot for the above two trained models to compare noisy datapoints and denoised (approximated) predictions.\n",
    "\n",
    "For each model, you should plot the datapoints and the predictions (a predicted line) on the same plot.\n",
    "\n",
    "Hint: You may need to manipulate the outputs from the model prediction, so the variable is in a proper type for the plot. You may find ```np.stack``` useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do prediction using the model with SGD\n",
    "predictions_sgd = ????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot for model trained using SGD\n",
    "????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do prediction using the model with Adam\n",
    "predictions_adam = ????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot for model trained using Adam\n",
    "????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Analyse the training process\n",
    "\n",
    "Pytorch-lightning module has its own built in methods to log values. These log files can be then used to visualize experimental results. You can use python plotting libraries such as matplotlib.\n",
    "\n",
    "Now, let's do the following:\n",
    "1. Read the logs with pandas library ([pd.read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html))\n",
    "2. Using those logs, plot train losses for models trained on SGD and Adam.\n",
    "\n",
    "Note: Make sure to drop NaN entries from dataframes before you plot using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read logs for model trained with SGD\n",
    "metrics_task_1_SGD = ???? # read the csv files\n",
    "metrics_task_1_SGD.set_index(\"epoch\", inplace=True) # set index\n",
    "metrics_task_1_SGD = metrics_task_1_SGD.groupby(level=0).sum().drop(\"step\", axis=1) # group the values properly\n",
    "\n",
    "# read logs for model trained with Adam\n",
    "metrics_task_1_Adam = ????\n",
    "metrics_task_1_Adam.set_index(\"epoch\", inplace=True)\n",
    "metrics_task_1_Adam = ????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of the results\n",
    "????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "### Discussion\n",
    "\n",
    "Please enter your answers below\n",
    "\n",
    "**Question 1: Analysing above results, what optimizer you will choose? Explain why?**\n",
    "\n",
    "Answer:\n",
    "\n",
    "**Question 2: Discuss why this simple shallow network is able to denoise noisy data and approximate sine function ?**\n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ECE4179",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
