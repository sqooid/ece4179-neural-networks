{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "# Assignment\n",
    "\n",
    "Welcome to your task 1 of the assignment! This notebook contains all the code and comments that you need to submit. The places where you need to edit are highlighted in red. Feel free to add in your own markdown for additional comments.\n",
    "\n",
    "__Submission details: make sure you all your outputs have been generated. Submit this Jupyter Notebook.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Enter you student details below</b>\n",
    "\n",
    "- <b>Student Name:</b> Lucas Liu\n",
    "- <b>Student ID:</b> 31445179   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "# Table of Contents\n",
    "\n",
    "* [Task 1.1: Iterative weighted linear regression](#t1_1)\n",
    "    \n",
    "* [Task 1.2: Using non-linear features and learning rate decay for better classification](#t1_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "# Libraries \n",
    "\n",
    "Libraries are important as it saves us time from writing our own functions all the time such as graphing, or creating matrices. Brief library descriptions have been added for every library that we import. You may also check the official documentations for more details.\n",
    "\n",
    "The required libraries are specified in this notebook for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "\n",
    "# Before you begin\n",
    "\n",
    "Ensure that you use __numpy__ or __torch__ efficiently instead of nested for loops to perform tensor multiplications. For example, if you are doing a 4x4 tensor multiplied with a 4x4 tensor using numpy, you should use np.matmul instead of 2 for loops. We highly recommend using __numpy__ for the entire task 1, as you will resuse some of the numpy code from the labs.\n",
    "\n",
    "\n",
    "# Linear and logistic regression\n",
    "\n",
    "In this task, you will be furthering your implementations with linear and logistic regression. This builds off your previous lab and aims to explore further possibilities with these models in both numerical estimation and classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "# Task 1.1 - Iterative weighted linear regression <a class=\"anchor\" id=\"t1_1\"></a>\n",
    "\n",
    "In some problems, every data point might not have the same importance. Having weights associated to samples will provide us with a principal way to model such problems. Consider a data set in which each data point $(x_i, y_i)$, $x_i \\in \\mathbb{R}^n$, $y_i \\in \\mathbb{R}$ is associated with a weighting factor $0 < \\alpha_i ≤ 1$. Define the loss of a linear model with parameters $w \\in \\mathbb{R}^n$ as the weighted sum-of-squares error:\n",
    "\n",
    "$$ \\mathcal{L}_{\\mathrm{wSE}}(\\boldsymbol{w})=\\frac{1}{m}\\sum_{i=1}^{m}\\alpha_i(w^\\top x_i - y_i)^2 $$\n",
    "\n",
    "It can be shown that the optimal weights can be obtained as: \n",
    "\n",
    "$$ w^* = (X^\\top A X)^{-1}X^\\top A Y $$\n",
    "\n",
    "Here, $X$ is a matrix of size $m × n$ where every row is one input sample (i.e., row $i$ in $X$ is $x_i$). Similarly, $Y$ is an m dimensional vector storing $y_i$ and $A$ is a diagonal matrix of size $m × m$ with $A[i, i] = \\alpha_i$.\n",
    "\n",
    "The purpose of this task is to train an iterative linear regression model that will weight the samples as the model is being trained. You need to develop the mechanism to weigh each sample as the model is being iteratively trained. Here, the diagonal matrix $A$ is updated by using the following equation:\n",
    "\n",
    "$$ \\hat{\\alpha_i} = exp(-\\sigma||y_i^{noisy} - \\hat{y_i}||^2) $$\n",
    "\n",
    ",where $\\sigma$ is a hyperparemeter you can tune."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Developing a generic function for (weighted) linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This weighted least squares function can be used for both weighted least squares \n",
    "# or a vanilla linear regression model.\n",
    "\n",
    "# \"X,Y\" inputs are required by the user\n",
    "# The input \"A\" can be defaulted to an identity matrix which would result in a vanilla linear regression model\n",
    "# The output will be the least square's calculated coefficients\n",
    "# It is recommended you use the least squares formula which has been derived in the lectures when calculating your linear regression coefficients\n",
    "\n",
    "def weighted_LS(X, Y, A=np.empty(0)):\n",
    "\n",
    "\n",
    "    ???\n",
    "\n",
    "\n",
    "    return ??? # Return optimal weight\n",
    "\n",
    "# Function to calculate the error\n",
    "# We are using least squares to calculate error (i.e., MSE)\n",
    "# This is basically the euclidean distance for the error measure\n",
    "\n",
    "def error_linear_model(X, w, Y):\n",
    "\n",
    "    # Define MSE loss\n",
    "\n",
    "    \n",
    "    return ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Iterative weighted linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data: _Task1_1.npz_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data was saved in a dictionary-liked form, where the keys are ('arr_0','arr_1')\n",
    "# Load the values into correct variables according to the following mapping.\n",
    "# X_trn <-- X_trn\n",
    "# Y_trn <-- Y_trn\n",
    "# X_val <-- X_val\n",
    "# Y_val <-- Y_val\n",
    "\n",
    "data = np.load(\"data/Task1_1.npz\")\n",
    "\n",
    "X_trn = data[\"X_trn\"]\n",
    "Y_trn = data[\"Y_trn\"]\n",
    "X_val = data[\"X_val\"]\n",
    "Y_val = data[\"Y_val\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise the train and validation data. Using a scatter plot, plot train data with red dots, validation data with blue dots. Remember to put title and x, y labels, legend for your plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the data. \n",
    "\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data is non-linear, let's create non-linear features with the mapping: $ x_{aug} = [1, x, x^2, 1/x] $ for both train and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create non-linear features, do it for both train and validation data\n",
    "\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement iterative weighted linear regression. You have to tune $\\sigma$ and $max\\_iter$ yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteratively fit the model\n",
    "\n",
    "max_iter = ??? # Set the max. iteration\n",
    "for num_iter in range(max_iter):\n",
    "    # Fit a weighted linear regression model\n",
    "    ???\n",
    "\n",
    "    # Calculate MSE of the model on validation samples\n",
    "    err_val = ???\n",
    "    print(f\"Error at iteration {num_iter}: {err_val:.4f}\")\n",
    "    \n",
    "    # Obtain the weight of the train samples\n",
    "    ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualise the estimated **train data after being weighted**. Plot the original train samples in blue and the weighted samples after passing them into the model in red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results.\n",
    "\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "# Task 1.2 - Using non-linear features and learning rate decay for better classification <a class=\"anchor\" id=\"t1_2\"></a>\n",
    "\n",
    "You have been given a set of samples $x = (x_1,x_2)^\\top \\in \\mathbb{R}^2$. In this task, you will apply the knowledge you learned from lab 2 to train a logistic model via Gradient Descent and try to improve the performance by implementing the decaying learning rate.\n",
    "\n",
    "__Instructions:__\n",
    "\n",
    "a. Copy any functions from Lab 2. The 3 functions you will need are: sigmoid(), predict(), compute_loss_and_grad()\n",
    "\n",
    "b. Load and visualise data\n",
    "\n",
    "c. Apply nonlinear transformation\n",
    "\n",
    "d. Train the model using GD\n",
    "\n",
    "e. Visualise the decision boundary\n",
    "\n",
    "f. Write a decaying learning rate function\n",
    "\n",
    "g. Train the model with decaying learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Copy the required functions from lab 2 (if they were functional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the sigmoid function\n",
    "def sigmoid(x):\n",
    "\n",
    "    ???\n",
    "\n",
    "    return ???\n",
    "\n",
    "# Write a prediction function -> We predict the output class probability, NOT the class label (no 0,1 rounding)\n",
    "def predict(X, w):\n",
    "    \n",
    "    ???\n",
    "\n",
    "    return ???\n",
    "\n",
    "def compute_loss_and_grad(X, y, y_hat):\n",
    "\n",
    "\n",
    "\n",
    "    ???\n",
    "\n",
    "\n",
    "\n",
    "    return ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Load and visualise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load numpy dataset from Task1_2.npz\n",
    "loaded_data_task4 = np.load(\"data/Task1_2.npz\")\n",
    "\n",
    "# Create train and test datasets\n",
    "# (X_train--> arr_0, X_test--> arr1_, Y_train_circle--> arr_2, Y_test_circle--> arr_3)\n",
    "X_train = loaded_data_task4[\"arr_0\"]\n",
    "X_test = loaded_data_task4[\"arr_1\"]\n",
    "Y_train_circle = loaded_data_task4[\"arr_2\"]\n",
    "Y_test_circle = loaded_data_task4[\"arr_3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize non-linear features for both train and test data\n",
    "# Create two plots side-by-side (Use subplots). Use scatter plots and show samples in different classes with different colours. The samples are 2-dimensional.\n",
    "\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Apply nonlinear transformation\n",
    "\n",
    "Let's map the samples with $x_{trans}=[x_1, x_2, x_1^2, x_2^2, x_1x_2, 1]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply nonlinear mapping\n",
    "\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) Train the model using GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed (Do not change!) ==============\n",
    "np.random.seed(0)\n",
    "# =================================================\n",
    "\n",
    "lr=1.0 # Set the learning to be 1.0\n",
    "num_epochs = 10000 # Train for 10000 epochs\n",
    "\n",
    "loss = np.zeros(num_epochs)\n",
    "theta = ??? # Initialise theta\n",
    "\n",
    "for ep in range(num_epochs):\n",
    "\n",
    "    # Apply gradient descent here and record loss\n",
    "\n",
    "    ???\n",
    "\n",
    "\n",
    "# Evaluate the classification accuracy for training and test data\n",
    "\n",
    "???\n",
    "\n",
    "\n",
    "print(\"Training accuracy: {:.3f}\".format(???))\n",
    "print(\"Test accuracy: {:.3f}\".format(???))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## (e) Visualise the Decision Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use meshgrid and linearly spaced points to plot the decision boundary\n",
    "# You may choose other ways to plot the decision boundary\n",
    "\n",
    "xx=np.linspace(-2,2,1000)\n",
    "yy=np.linspace(-2,2,1000)\n",
    "[X2,Y2]=np.meshgrid(xx,yy)\n",
    "\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (f) Write a decaying learning rate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a simple function to decrease current learning rate by 0.1% in each epoch\n",
    "def decaying_lr(lr):\n",
    "    \n",
    "    \n",
    "    return ??? # return the new learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (g) Train the model with decaying learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed (Do not change!) ==============\n",
    "np.random.seed(0)\n",
    "# =================================================\n",
    "\n",
    "lr=1.0 # Set initial learning rate to 1.0\n",
    "num_epochs = 10000 # Train for 10000 epochs\n",
    "\n",
    "loss = np.zeros(num_epochs)\n",
    "theta = ??? # Initialise theta\n",
    "    \n",
    "for ep in range(num_epochs):\n",
    "    \n",
    "    # Apply gradient descent here and record loss\n",
    "\n",
    "    # Apply learning rate decay here\n",
    "\n",
    "    \n",
    "\n",
    "# Evaluate the classification accuracy for training and test data\n",
    "\n",
    "???\n",
    "\n",
    "\n",
    "print(\"Training accuracy: {:.3f}\".format(???))\n",
    "print(\"Test accuracy: {:.3f}\".format(???))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "## 1.3 Discussion  <a class=\"anchor\" id=\"t1_3\"></a>\n",
    "\n",
    "1. What do you notice about the outliers for the implementations weighted linear regression?\n",
    "    \n",
    "2. In what situations, you may want to use higher dimensional features? (Answer in terms of decision boundary)\n",
    "\n",
    "3. Why is it important to augment the bias term for this non-linear transformation? What does it actually do to the decision boundary?\n",
    "\n",
    "4. What is the aim of applying decay in learning rate? \n",
    "    \n",
    "## Answer\n",
    "    \n",
    "1. \n",
    "\n",
    "2. \n",
    "\n",
    "3. \n",
    "\n",
    "4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
